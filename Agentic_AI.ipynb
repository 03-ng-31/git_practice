{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhJU3jN1fJNf"
      },
      "outputs": [],
      "source": [
        "# agents/planner_agent.py\n",
        "class PlannerAgent:\n",
        "    def plan(self, filters: dict, fields: list) -> list:\n",
        "        return [\n",
        "            \"Parse the document\",\n",
        "            \"Generate optimized prompt\",\n",
        "            \"Extract fields using prompt\",\n",
        "            \"Validate result\"\n",
        "        ]\n",
        "\n",
        "\n",
        "# agents/prompt_builder_agent.py\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "class PromptBuilderAgent:\n",
        "    def __init__(self):\n",
        "        self.chain = LLMChain(\n",
        "            llm=OpenAI(model=\"gpt-4\"),\n",
        "            prompt=PromptTemplate.from_template(\n",
        "                \"\"\"\n",
        "                You are a prompt engineer for legal documents. Based on filters: {filters} and fields: {fields},\n",
        "                create a JSON-only prompt for extracting structured data from OCR'd or digital PDF lease documents.\n",
        "                \"\"\"\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def run(self, filters: dict, fields: list) -> str:\n",
        "        return self.chain.run({\"filters\": str(filters), \"fields\": str(fields)})\n",
        "\n",
        "\n",
        "# agents/document_parser_agent.py\n",
        "from tools.ocr import extract_text_from_pdf\n",
        "\n",
        "class DocumentParserAgent:\n",
        "    def parse(self, pdf_path: str) -> dict:\n",
        "        return extract_text_from_pdf(pdf_path)\n",
        "\n",
        "\n",
        "# agents/extraction_agent.py\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "class ExtractionAgent:\n",
        "    def __init__(self):\n",
        "        self.llm = OpenAI(model=\"gpt-4\")\n",
        "\n",
        "    def run(self, prompt: str, doc_text: str) -> str:\n",
        "        full_input = f\"{prompt}\\n\\n### DOCUMENT CONTENT\\n{doc_text}\"\n",
        "        return self.llm.invoke(full_input)\n",
        "\n",
        "\n",
        "# agents/validation_agent.py\n",
        "from tools.json_validator import validate_json_output\n",
        "\n",
        "class ValidationAgent:\n",
        "    def validate(self, raw_output: str) -> tuple:\n",
        "        try:\n",
        "            result = json.loads(raw_output)\n",
        "        except Exception as e:\n",
        "            return None, f\"JSON parsing error: {e}\"\n",
        "        return validate_json_output(result)\n",
        "\n",
        "\n",
        "# agents/replay_panel.py\n",
        "import streamlit as st\n",
        "import os\n",
        "import json\n",
        "\n",
        "def render_replay_panel(log_dir=\"audit_logs\"):\n",
        "    st.sidebar.subheader(\"ğŸ•¹ï¸ Agent Replay Panel\")\n",
        "    files = [f for f in os.listdir(log_dir) if f.startswith(\"prompt\")]\n",
        "    choice = st.sidebar.selectbox(\"Select Prompt Log\", files)\n",
        "    if choice:\n",
        "        with open(os.path.join(log_dir, choice), \"r\") as f:\n",
        "            prompt = f.read()\n",
        "        st.code(prompt, language=\"markdown\")\n",
        "        resp_file = choice.replace(\"prompt\", \"response\").replace(\".txt\", \".json\")\n",
        "        with open(os.path.join(log_dir, resp_file), \"r\") as f:\n",
        "            st.json(json.load(f))\n",
        "\n",
        "\n",
        "# agents/claude_tool_agent.py\n",
        "# Optional Claude-like Tool Agent (mock behavior)\n",
        "class ClaudeToolAgent:\n",
        "    def run(self, tool: str, input_text: str) -> str:\n",
        "        if tool == \"summarize\":\n",
        "            return input_text[:300] + \"... (summary)\"\n",
        "        elif tool == \"extract_dates\":\n",
        "            import re\n",
        "            return str(re.findall(r\"\\b\\w+ \\d{1,2}, \\d{4}\\b\", input_text))\n",
        "        return \"Unknown tool\"\n",
        "\n",
        "\n",
        "# __main__.py to run the pipeline\n",
        "from agents.planner_agent import PlannerAgent\n",
        "from agents.prompt_builder_agent import PromptBuilderAgent\n",
        "from agents.document_parser_agent import DocumentParserAgent\n",
        "from agents.extraction_agent import ExtractionAgent\n",
        "from agents.validation_agent import ValidationAgent\n",
        "\n",
        "class AgentPipeline:\n",
        "    def __init__(self):\n",
        "        self.planner = PlannerAgent()\n",
        "        self.prompt_agent = PromptBuilderAgent()\n",
        "        self.parser = DocumentParserAgent()\n",
        "        self.extractor = ExtractionAgent()\n",
        "        self.validator = ValidationAgent()\n",
        "\n",
        "    def run(self, pdf_path, filters, fields):\n",
        "        plan = self.planner.plan(filters, fields)\n",
        "        parsed = self.parser.parse(pdf_path)\n",
        "        prompt = self.prompt_agent.run(filters, fields)\n",
        "        output = self.extractor.run(prompt, parsed['text'])\n",
        "        validated, err = self.validator.validate(output)\n",
        "        if err:\n",
        "            return {\"error\": err, \"raw_output\": output}\n",
        "        return validated.dict()\n",
        "\n",
        "# Optional UI hook\n",
        "# from agents.replay_panel import render_replay_panel\n",
        "# render_replay_panel()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tools/ocr.py:\n",
        "def extract_text_from_pdf(pdf_path: str) -> dict:\n",
        "    try:\n",
        "        import fitz\n",
        "        doc = fitz.open(pdf_path)\n",
        "        return {\"text\": \"\\n\".join([p.get_text() for p in doc]), \"is_ocr\": False}\n",
        "    except:\n",
        "        from pdf2image import convert_from_path\n",
        "        import pytesseract\n",
        "        images = convert_from_path(pdf_path)\n",
        "        text = \" \".join([pytesseract.image_to_string(img) for img in images])\n",
        "        return {\"text\": text, \"is_ocr\": True}\n",
        "\n",
        "\n",
        "tools/json_validator.py\n",
        "from pydantic import BaseModel, Field, ValidationError\n",
        "from typing import Optional, Dict\n",
        "\n",
        "class FieldExtraction(BaseModel):\n",
        "    value: Optional[str]\n",
        "    reasoning: str\n",
        "    confidence: float = Field(..., ge=0, le=1)\n",
        "\n",
        "class ExtractionSchema(BaseModel):\n",
        "    __root__: Dict[str, FieldExtraction]\n",
        "\n",
        "def validate_json_output(data: dict):\n",
        "    try:\n",
        "        return ExtractionSchema.parse_obj(data), None\n",
        "    except ValidationError as ve:\n",
        "        return None, str(ve)\n"
      ],
      "metadata": {
        "id": "TQkN2YmffYiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "class PlannerAgent:\n",
        "    def __init__(self):\n",
        "        prompt = PromptTemplate.from_template(\"\"\"\n",
        "You are a task planner for an AI system that extracts structured data from scanned or digital lease documents.\n",
        "\n",
        "Given:\n",
        "- Fields: {fields}\n",
        "- Filters: {filters}\n",
        "\n",
        "Generate a step-by-step plan using available agents:\n",
        "- DocumentParserAgent\n",
        "- PromptBuilderAgent\n",
        "- ExtractionAgent\n",
        "- ValidationAgent\n",
        "\n",
        "Respond with a numbered list of agent calls (one per step).\n",
        "\"\"\")\n",
        "        self.chain = LLMChain(prompt=prompt, llm=OpenAI(model=\"gpt-4\"))\n",
        "\n",
        "    def generate_plan(self, filters: dict, fields: list) -> list:\n",
        "        plan_text = self.chain.run(filters=filters, fields=fields)\n",
        "        return [line.strip().split()[0] for line in plan_text.strip().split(\"\\n\") if line.strip()]\n"
      ],
      "metadata": {
        "id": "8UPiowvhgWww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from agents.prompt_builder_agent import PromptBuilderAgent\n",
        "from agents.document_parser_agent import DocumentParserAgent\n",
        "from agents.extraction_agent import ExtractionAgent\n",
        "from agents.validation_agent import ValidationAgent\n",
        "\n",
        "AGENT_REGISTRY = {\n",
        "    \"DocumentParserAgent\": DocumentParserAgent(),\n",
        "    \"PromptBuilderAgent\": PromptBuilderAgent(),\n",
        "    \"ExtractionAgent\": ExtractionAgent(),\n",
        "    \"ValidationAgent\": ValidationAgent(),\n",
        "}\n"
      ],
      "metadata": {
        "id": "7ste7Z_Bgewa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from agents.planner_agent import PlannerAgent\n",
        "from agents.agent_registry import AGENT_REGISTRY\n",
        "\n",
        "class AgentOrchestrator:\n",
        "    def __init__(self):\n",
        "        self.planner = PlannerAgent()\n",
        "\n",
        "    def run(self, pdf_path, filters, fields):\n",
        "        context = {\n",
        "            \"pdf_path\": pdf_path,\n",
        "            \"filters\": filters,\n",
        "            \"fields\": fields,\n",
        "            \"intermediates\": {}  # stores data shared between agents\n",
        "        }\n",
        "\n",
        "        plan = self.planner.generate_plan(filters, fields)\n",
        "\n",
        "        for agent_name in plan:\n",
        "            agent = AGENT_REGISTRY.get(agent_name)\n",
        "            if not agent:\n",
        "                raise ValueError(f\"No agent registered for {agent_name}\")\n",
        "\n",
        "            if agent_name == \"DocumentParserAgent\":\n",
        "                context[\"intermediates\"][\"parsed\"] = agent.parse(pdf_path)\n",
        "\n",
        "            elif agent_name == \"PromptBuilderAgent\":\n",
        "                context[\"intermediates\"][\"prompt\"] = agent.run(filters, fields)\n",
        "\n",
        "            elif agent_name == \"ExtractionAgent\":\n",
        "                prompt = context[\"intermediates\"][\"prompt\"]\n",
        "                text = context[\"intermediates\"][\"parsed\"][\"text\"]\n",
        "                context[\"intermediates\"][\"extracted\"] = agent.run(prompt, text)\n",
        "\n",
        "            elif agent_name == \"ValidationAgent\":\n",
        "                result = agent.validate(context[\"intermediates\"][\"extracted\"])\n",
        "                validated, error = result\n",
        "                if error:\n",
        "                    return {\"error\": error, \"raw\": context[\"intermediates\"][\"extracted\"]}\n",
        "                return validated.dict()\n",
        "\n",
        "        return context[\"intermediates\"]\n"
      ],
      "metadata": {
        "id": "MH-aWqGLgnYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# /agents/memory_board.py\n",
        "import os\n",
        "import json\n",
        "\n",
        "class AgentMemoryBoard:\n",
        "    def __init__(self, doc_id: str, memory_dir: str = \"memory\"):\n",
        "        os.makedirs(memory_dir, exist_ok=True)\n",
        "        self.path = os.path.join(memory_dir, f\"{doc_id}.json\")\n",
        "        self._memory = self._load()\n",
        "\n",
        "    def _load(self):\n",
        "        if os.path.exists(self.path):\n",
        "            with open(self.path, \"r\") as f:\n",
        "                return json.load(f)\n",
        "        return {}\n",
        "\n",
        "    def save(self):\n",
        "        with open(self.path, \"w\") as f:\n",
        "            json.dump(self._memory, f, indent=2)\n",
        "\n",
        "    def update(self, key: str, value):\n",
        "        self._memory[key] = value\n",
        "        self.save()\n",
        "\n",
        "    def get(self, key: str):\n",
        "        return self._memory.get(key)\n",
        "\n",
        "    def get_all(self):\n",
        "        return self._memory\n",
        "\n",
        "\n",
        "# /agents/agent_registry.py\n",
        "from agents.prompt_builder_agent import PromptBuilderAgent\n",
        "from agents.document_parser_agent import DocumentParserAgent\n",
        "from agents.extraction_agent import ExtractionAgent\n",
        "from agents.validation_agent import ValidationAgent\n",
        "\n",
        "AGENT_REGISTRY = {\n",
        "    \"DocumentParserAgent\": DocumentParserAgent(),\n",
        "    \"PromptBuilderAgent\": PromptBuilderAgent(),\n",
        "    \"ExtractionAgent\": ExtractionAgent(),\n",
        "    \"ValidationAgent\": ValidationAgent(),\n",
        "}\n",
        "\n",
        "\n",
        "# /agents/planner_agent.py (dynamic planner)\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "class PlannerAgent:\n",
        "    def __init__(self):\n",
        "        prompt = PromptTemplate.from_template(\"\"\"\n",
        "You are a task planner for an AI system that extracts structured data from scanned or digital lease documents.\n",
        "\n",
        "Given:\n",
        "- Fields: {fields}\n",
        "- Filters: {filters}\n",
        "\n",
        "Generate a step-by-step plan using these agents:\n",
        "- DocumentParserAgent\n",
        "- PromptBuilderAgent\n",
        "- ExtractionAgent\n",
        "- ValidationAgent\n",
        "\n",
        "Respond with one agent name per line in the order they should be called.\n",
        "\"\"\")\n",
        "        self.chain = LLMChain(prompt=prompt, llm=OpenAI(model=\"gpt-4\"))\n",
        "\n",
        "    def generate_plan(self, filters: dict, fields: list) -> list:\n",
        "        plan_text = self.chain.run(filters=filters, fields=fields)\n",
        "        return [line.strip() for line in plan_text.strip().split(\"\\n\") if line.strip()]\n",
        "\n",
        "\n",
        "# /agents/orchestrator.py\n",
        "from agents.planner_agent import PlannerAgent\n",
        "from agents.agent_registry import AGENT_REGISTRY\n",
        "from agents.memory_board import AgentMemoryBoard\n",
        "\n",
        "class AgentOrchestrator:\n",
        "    def __init__(self, doc_id: str):\n",
        "        self.memory = AgentMemoryBoard(doc_id)\n",
        "        self.planner = PlannerAgent()\n",
        "\n",
        "    def run(self, pdf_path, filters, fields):\n",
        "        plan = self.planner.generate_plan(filters, fields)\n",
        "\n",
        "        for agent_name in plan:\n",
        "            agent = AGENT_REGISTRY.get(agent_name)\n",
        "            if not agent:\n",
        "                raise ValueError(f\"No agent registered for {agent_name}\")\n",
        "\n",
        "            if agent_name == \"DocumentParserAgent\":\n",
        "                parsed = agent.parse(pdf_path)\n",
        "                self.memory.update(\"parsed\", parsed)\n",
        "\n",
        "            elif agent_name == \"PromptBuilderAgent\":\n",
        "                prompt = agent.run(filters, fields)\n",
        "                self.memory.update(\"prompt\", prompt)\n",
        "\n",
        "            elif agent_name == \"ExtractionAgent\":\n",
        "                prompt = self.memory.get(\"prompt\")\n",
        "                text = self.memory.get(\"parsed\")[\"text\"]\n",
        "                result = agent.run(prompt, text)\n",
        "                self.memory.update(\"extracted\", result)\n",
        "\n",
        "            elif agent_name == \"ValidationAgent\":\n",
        "                raw = self.memory.get(\"extracted\")\n",
        "                validated, error = agent.validate(raw)\n",
        "                self.memory.update(\"validated\", validated.dict() if validated else None)\n",
        "                if error:\n",
        "                    self.memory.update(\"error\", error)\n",
        "                    return {\"error\": error, \"raw\": raw}\n",
        "                return validated.dict()\n",
        "\n",
        "        return self.memory.get_all()\n"
      ],
      "metadata": {
        "id": "fFucb99BhEfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# agent_dashboard.py\n",
        "import streamlit as st\n",
        "import os\n",
        "import json\n",
        "from agents.orchestrator import AgentOrchestrator\n",
        "from agents.memory_board import AgentMemoryBoard\n",
        "\n",
        "st.set_page_config(page_title=\"Agentic Document Intelligence\", layout=\"wide\")\n",
        "st.title(\"ğŸ§  Agentic Lease Document Analyzer\")\n",
        "\n",
        "# --- Sidebar Config ---\n",
        "st.sidebar.header(\"Document & Metadata\")\n",
        "filters = {\n",
        "    \"Module Name\": st.sidebar.text_input(\"Module Name\", \"Real Estate\"),\n",
        "    \"Territory\": st.sidebar.text_input(\"Territory\", \"South\"),\n",
        "    \"Owner\": st.sidebar.text_input(\"Owner\", \"Crown\"),\n",
        "    \"Market\": st.sidebar.text_input(\"Market\", \"South East\"),\n",
        "    \"Contract Type\": st.sidebar.text_input(\"Contract Type\", \"Lease\"),\n",
        "    \"SubMarket\": st.sidebar.text_input(\"SubMarket\", \"Florida\"),\n",
        "    \"Document Type\": st.sidebar.text_input(\"Document Type\", \"Lease\"),\n",
        "    \"Local Market\": st.sidebar.text_input(\"Local Market\", \"Tampa\"),\n",
        "    \"Facility Type\": st.sidebar.text_input(\"Facility Type\", \"Easement\")\n",
        "}\n",
        "\n",
        "st.sidebar.markdown(\"---\")\n",
        "\n",
        "# --- Field Input ---\n",
        "st.sidebar.header(\"Fields to Extract\")\n",
        "fields = []\n",
        "field_count = st.sidebar.slider(\"# Fields\", 1, 10, 3)\n",
        "for i in range(field_count):\n",
        "    name = st.sidebar.text_input(f\"Field {i+1} Name\", key=f\"fname_{i}\")\n",
        "    desc = st.sidebar.text_input(f\"Field {i+1} Description\", key=f\"fdesc_{i}\")\n",
        "    if name:\n",
        "        fields.append({\"name\": name, \"description\": desc})\n",
        "\n",
        "# --- File Upload ---\n",
        "st.header(\"Upload PDF Lease Document\")\n",
        "uploaded = st.file_uploader(\"Choose PDF\", type=\"pdf\")\n",
        "\n",
        "doc_id = None\n",
        "if uploaded:\n",
        "    os.makedirs(\"uploads\", exist_ok=True)\n",
        "    doc_id = uploaded.name.replace(\".pdf\", \"\")\n",
        "    doc_path = os.path.join(\"uploads\", uploaded.name)\n",
        "    with open(doc_path, \"wb\") as f:\n",
        "        f.write(uploaded.read())\n",
        "\n",
        "    # Run agentic pipeline\n",
        "    if st.button(\"Run Agent Pipeline\"):\n",
        "        st.subheader(\"ğŸ§  Agent Execution Log\")\n",
        "        with st.spinner(\"Running full agent workflow...\"):\n",
        "            pipeline = AgentOrchestrator(doc_id=doc_id)\n",
        "            results = pipeline.run(doc_path, filters, fields)\n",
        "            st.success(\"Pipeline execution complete âœ…\")\n",
        "            st.json(results)\n",
        "\n",
        "# --- Memory Visualizer ---\n",
        "if doc_id and os.path.exists(f\"memory/{doc_id}.json\"):\n",
        "    st.markdown(\"---\")\n",
        "    st.header(\"ğŸ—‚ï¸ Agent Memory Replay\")\n",
        "    mem = AgentMemoryBoard(doc_id)\n",
        "    mem_data = mem.get_all()\n",
        "    for k, v in mem_data.items():\n",
        "        with st.expander(f\"{k}\"):\n",
        "            if isinstance(v, str):\n",
        "                st.code(v)\n",
        "            else:\n",
        "                st.json(v)\n"
      ],
      "metadata": {
        "id": "WZhXW9drhg07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# /agents/prompt_builder_agent.py with retry logic\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "class PromptBuilderAgent:\n",
        "    def __init__(self, retries: int = 2):\n",
        "        self.retries = retries\n",
        "        self.chain = LLMChain(\n",
        "            llm=OpenAI(model=\"gpt-4\"),\n",
        "            prompt=PromptTemplate.from_template(\n",
        "                \"\"\"\n",
        "                You are a prompt engineer for legal documents. Based on filters: {filters} and fields: {fields},\n",
        "                create a JSON-only prompt for extracting structured data from OCR'd or digital PDF lease documents.\n",
        "                \"\"\"\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def run(self, filters: dict, fields: list) -> str:\n",
        "        for attempt in range(self.retries):\n",
        "            try:\n",
        "                return self.chain.run({\"filters\": str(filters), \"fields\": str(fields)})\n",
        "            except Exception as e:\n",
        "                if attempt == self.retries - 1:\n",
        "                    raise RuntimeError(f\"PromptBuilder failed after {self.retries} attempts: {e}\")\n",
        "\n",
        "\n",
        "# /agents/extraction_agent.py with retry logic\n",
        "from langchain.llms import OpenAI\n",
        "import time\n",
        "\n",
        "class ExtractionAgent:\n",
        "    def __init__(self, retries: int = 2):\n",
        "        self.retries = retries\n",
        "        self.llm = OpenAI(model=\"gpt-4\")\n",
        "\n",
        "    def run(self, prompt: str, doc_text: str) -> str:\n",
        "        full_input = f\"{prompt}\\n\\n### DOCUMENT CONTENT\\n{doc_text}\"\n",
        "        for attempt in range(self.retries):\n",
        "            try:\n",
        "                return self.llm.invoke(full_input)\n",
        "            except Exception as e:\n",
        "                time.sleep(1)\n",
        "                if attempt == self.retries - 1:\n",
        "                    raise RuntimeError(f\"ExtractionAgent failed: {e}\")\n",
        "\n",
        "\n",
        "# /agents/validation_agent.py with retry logic\n",
        "from tools.json_validator import validate_json_output\n",
        "import json\n",
        "\n",
        "class ValidationAgent:\n",
        "    def __init__(self, retries: int = 1):\n",
        "        self.retries = retries\n",
        "\n",
        "    def validate(self, raw_output: str) -> tuple:\n",
        "        for _ in range(self.retries):\n",
        "            try:\n",
        "                data = json.loads(raw_output)\n",
        "                return validate_json_output(data)\n",
        "            except Exception as e:\n",
        "                continue\n",
        "        return None, \"Validation failed after retries.\"\n",
        "\n",
        "\n",
        "# /agents/memory_flow_panel.py - trace visualizer\n",
        "import streamlit as st\n",
        "import os\n",
        "import json\n",
        "import datetime\n",
        "import hashlib\n",
        "\n",
        "def render_memory_flow(doc_id: str):\n",
        "    mem_path = os.path.join(\"memory\", f\"{doc_id}.json\")\n",
        "    if not os.path.exists(mem_path):\n",
        "        st.warning(\"No memory found for this document.\")\n",
        "        return\n",
        "\n",
        "    with open(mem_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    st.header(\"ğŸ§  Memory Trace Flow\")\n",
        "    keys = list(data.keys())\n",
        "    for i in range(len(keys) - 1):\n",
        "        st.markdown(f\"**{keys[i]} âœ {keys[i+1]}**\")\n",
        "        with st.expander(keys[i]):\n",
        "            st.json(data[keys[i]])\n",
        "    if keys:\n",
        "        with st.expander(keys[-1]):\n",
        "            st.json(data[keys[-1]])"
      ],
      "metadata": {
        "id": "-UMXS-Sohtyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# /agents/memory_board.py with metadata + checkpoint replay\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "class AgentMemoryBoard:\n",
        "    def __init__(self, doc_id: str, memory_dir: str = \"memory\"):\n",
        "        os.makedirs(memory_dir, exist_ok=True)\n",
        "        self.path = os.path.join(memory_dir, f\"{doc_id}.json\")\n",
        "        self._memory = self._load()\n",
        "        if \"_meta\" not in self._memory:\n",
        "            self._memory[\"_meta\"] = {\"created\": datetime.utcnow().isoformat(), \"log\": []}\n",
        "\n",
        "    def _load(self):\n",
        "        if os.path.exists(self.path):\n",
        "            with open(self.path, \"r\") as f:\n",
        "                return json.load(f)\n",
        "        return {}\n",
        "\n",
        "    def save(self):\n",
        "        with open(self.path, \"w\") as f:\n",
        "            json.dump(self._memory, f, indent=2)\n",
        "\n",
        "    def update(self, key: str, value, retries: int = 0, runtime: float = 0.0):\n",
        "        self._memory[key] = value\n",
        "        self._memory[\"_meta\"][\"log\"].append({\n",
        "            \"step\": key,\n",
        "            \"timestamp\": datetime.utcnow().isoformat(),\n",
        "            \"retries\": retries,\n",
        "            \"runtime\": runtime\n",
        "        })\n",
        "        self.save()\n",
        "\n",
        "    def get(self, key: str):\n",
        "        return self._memory.get(key)\n",
        "\n",
        "    def get_all(self):\n",
        "        return self._memory\n",
        "\n",
        "    def replay_from(self, agent_key: str):\n",
        "        return {k: self._memory[k] for k in self._memory if k == agent_key or k == \"_meta\"}\n",
        "\n",
        "\n",
        "# /agents/memory_flow_panel.py - with Mermaid diagram + replay\n",
        "import streamlit as st\n",
        "import os\n",
        "import json\n",
        "import datetime\n",
        "\n",
        "\n",
        "def render_memory_flow(doc_id: str):\n",
        "    mem_path = os.path.join(\"memory\", f\"{doc_id}.json\")\n",
        "    if not os.path.exists(mem_path):\n",
        "        st.warning(\"No memory found for this document.\")\n",
        "        return\n",
        "\n",
        "    with open(mem_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    st.header(\"ğŸ“ˆ Agent Execution Flow\")\n",
        "    if \"_meta\" not in data or \"log\" not in data[\"_meta\"]:\n",
        "        st.warning(\"No execution log found.\")\n",
        "        return\n",
        "\n",
        "    logs = data[\"_meta\"][\"log\"]\n",
        "    steps = [entry[\"step\"] for entry in logs]\n",
        "    mermaid = [\"flowchart TD\"]\n",
        "    for i in range(len(steps) - 1):\n",
        "        mermaid.append(f\"    {steps[i]} --> {steps[i+1]}\")\n",
        "    st.markdown(\"```mermaid\\n\" + \"\\n\".join(mermaid) + \"\\n```\")\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    st.subheader(\"ğŸ“‚ Step Detail + Replay\")\n",
        "    for entry in logs:\n",
        "        key = entry[\"step\"]\n",
        "        with st.expander(f\"{key} (retried {entry['retries']}x, runtime: {entry['runtime']}s)\"):\n",
        "            st.json(data.get(key))\n",
        "            if st.button(f\"ğŸ” Replay {key}\", key=f\"replay_{key}\"):\n",
        "                st.session_state[\"replay_step\"] = key\n",
        "                st.success(f\"Set to replay from {key}\")"
      ],
      "metadata": {
        "id": "ZKU-AN0Bh_R1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# memory_flow_panel.py with replay resume button\n",
        "import streamlit as st\n",
        "import os\n",
        "import json\n",
        "\n",
        "\n",
        "def render_memory_flow(doc_id: str):\n",
        "    mem_path = os.path.join(\"memory\", f\"{doc_id}.json\")\n",
        "    if not os.path.exists(mem_path):\n",
        "        st.warning(\"No memory found for this document.\")\n",
        "        return\n",
        "\n",
        "    with open(mem_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    st.header(\"ğŸ“ˆ Agent Execution Flow\")\n",
        "    if \"_meta\" not in data or \"log\" not in data[\"_meta\"]:\n",
        "        st.warning(\"No execution log found.\")\n",
        "        return\n",
        "\n",
        "    logs = data[\"_meta\"][\"log\"]\n",
        "    steps = [entry[\"step\"] for entry in logs]\n",
        "    mermaid = [\"flowchart TD\"]\n",
        "    for i in range(len(steps) - 1):\n",
        "        mermaid.append(f\"    {steps[i]} --> {steps[i+1]}\")\n",
        "    st.markdown(\"```mermaid\\n\" + \"\\n\".join(mermaid) + \"\\n```\")\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    st.subheader(\"ğŸ“‚ Step Detail + Replay\")\n",
        "    replay_step = st.session_state.get(\"replay_step\")\n",
        "    resume_button = None\n",
        "\n",
        "    for entry in logs:\n",
        "        key = entry[\"step\"]\n",
        "        with st.expander(f\"{key} (retried {entry['retries']}x, runtime: {entry['runtime']}s)\"):\n",
        "            st.json(data.get(key))\n",
        "            if st.button(f\"ğŸ” Replay {key}\", key=f\"replay_{key}\"):\n",
        "                st.session_state[\"replay_step\"] = key\n",
        "                st.success(f\"Set to replay from {key}\")\n",
        "\n",
        "    if replay_step:\n",
        "        st.markdown(f\"### â–¶ï¸ Ready to Resume from: `{replay_step}`\")\n",
        "        with st.form(\"resume_form\"):\n",
        "            updated_fields = st.text_area(\"Optional: Update Field JSON\", \"\")\n",
        "            run = st.form_submit_button(\"Resume Pipeline\")\n",
        "            if run:\n",
        "                if updated_fields:\n",
        "                    try:\n",
        "                        fields = json.loads(updated_fields)\n",
        "                        st.session_state[\"override_fields\"] = fields\n",
        "                    except json.JSONDecodeError:\n",
        "                        st.error(\"Invalid JSON - cannot resume\")\n",
        "                        return\n",
        "                st.session_state[\"resume\"] = True\n",
        "                st.success(f\"Resuming from {replay_step} with updated fields\")\n",
        "\n"
      ],
      "metadata": {
        "id": "fACd11YxiVO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# /agents/orchestrator.py with resume logic + memory filter override\n",
        "from agents.planner_agent import PlannerAgent\n",
        "from agents.agent_registry import AGENT_REGISTRY\n",
        "from agents.memory_board import AgentMemoryBoard\n",
        "import time\n",
        "\n",
        "class AgentOrchestrator:\n",
        "    def __init__(self, doc_id: str):\n",
        "        self.memory = AgentMemoryBoard(doc_id)\n",
        "        self.planner = PlannerAgent()\n",
        "\n",
        "    def run(self, pdf_path, filters, fields):\n",
        "        override_fields = None\n",
        "        replay_point = None\n",
        "\n",
        "        # Resume mode\n",
        "        if \"resume\" in globals() or \"resume\" in locals():\n",
        "            override_fields = globals().get(\"override_fields\") or fields\n",
        "            replay_point = globals().get(\"replay_step\")\n",
        "\n",
        "        plan = self.planner.generate_plan(filters, override_fields or fields)\n",
        "\n",
        "        execution_started = False\n",
        "        for agent_name in plan:\n",
        "            if replay_point and not execution_started:\n",
        "                if agent_name == replay_point:\n",
        "                    execution_started = True\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "            agent = AGENT_REGISTRY.get(agent_name)\n",
        "            if not agent:\n",
        "                raise ValueError(f\"No agent registered for {agent_name}\")\n",
        "\n",
        "            t0 = time.time()\n",
        "            if agent_name == \"DocumentParserAgent\":\n",
        "                parsed = agent.parse(pdf_path)\n",
        "                self.memory.update(\"parsed\", parsed, retries=0, runtime=time.time() - t0)\n",
        "\n",
        "            elif agent_name == \"PromptBuilderAgent\":\n",
        "                prompt = agent.run(filters, override_fields or fields)\n",
        "                self.memory.update(\"prompt\", prompt, retries=agent.retries, runtime=time.time() - t0)\n",
        "\n",
        "            elif agent_name == \"ExtractionAgent\":\n",
        "                prompt = self.memory.get(\"prompt\")\n",
        "                text = self.memory.get(\"parsed\")[\"text\"]\n",
        "                result = agent.run(prompt, text)\n",
        "                self.memory.update(\"extracted\", result, retries=agent.retries, runtime=time.time() - t0)\n",
        "\n",
        "            elif agent_name == \"ValidationAgent\":\n",
        "                raw = self.memory.get(\"extracted\")\n",
        "                validated, error = agent.validate(raw)\n",
        "                self.memory.update(\"validated\", validated.dict() if validated else None, retries=agent.retries, runtime=time.time() - t0)\n",
        "                if error:\n",
        "                    self.memory.update(\"error\", error)\n",
        "                    return {\"error\": error, \"raw\": raw}\n",
        "                return validated.dict()\n",
        "\n",
        "        return self.memory.get_all()\n",
        "\n",
        "\n",
        "# /agents/insight_utils.py for confidence trend + PDF export\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import os\n",
        "from fpdf import FPDF\n",
        "\n",
        "\n",
        "def plot_confidence_trends(doc_id):\n",
        "    path = f\"memory/{doc_id}.json\"\n",
        "    if not os.path.exists(path):\n",
        "        return None\n",
        "    with open(path, \"r\") as f:\n",
        "        mem = json.load(f)\n",
        "\n",
        "    validations = mem.get(\"validated\", {})\n",
        "    confidences = {k: v.get(\"confidence\") for k, v in validations.items() if isinstance(v, dict)}\n",
        "    if not confidences:\n",
        "        return None\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.bar(confidences.keys(), confidences.values(), color='green')\n",
        "    plt.title(\"Confidence Scores for Extracted Fields\")\n",
        "    plt.ylabel(\"Confidence\")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.xticks(rotation=45)\n",
        "    fig_path = f\"memory/{doc_id}_confidence.png\"\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(fig_path)\n",
        "    return fig_path\n",
        "\n",
        "\n",
        "def generate_audit_pdf(doc_id):\n",
        "    mem_path = f\"memory/{doc_id}.json\"\n",
        "    img_path = f\"memory/{doc_id}_confidence.png\"\n",
        "    if not os.path.exists(mem_path):\n",
        "        return None\n",
        "\n",
        "    with open(mem_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    pdf = FPDF()\n",
        "    pdf.set_auto_page_break(auto=True, margin=15)\n",
        "    pdf.add_page()\n",
        "    pdf.set_font(\"Arial\", size=12)\n",
        "    pdf.cell(200, 10, txt=\"Agent Flow Audit Report\", ln=True, align='C')\n",
        "    pdf.ln(10)\n",
        "    pdf.cell(200, 10, txt=f\"Document ID: {doc_id}\", ln=True)\n",
        "\n",
        "    for k in data:\n",
        "        if k == \"_meta\": continue\n",
        "        pdf.set_font(\"Arial\", 'B', 12)\n",
        "        pdf.cell(200, 10, txt=f\"Step: {k}\", ln=True)\n",
        "        pdf.set_font(\"Arial\", size=10)\n",
        "        txt = json.dumps(data[k], indent=2)\n",
        "        for line in txt.split(\"\\n\"):\n",
        "            pdf.multi_cell(0, 5, txt=line)\n",
        "        pdf.ln()\n",
        "\n",
        "    if os.path.exists(img_path):\n",
        "        pdf.add_page()\n",
        "        pdf.image(img_path, x=10, y=30, w=180)\n",
        "\n",
        "    final_path = f\"memory/{doc_id}_audit.pdf\"\n",
        "    pdf.output(final_path)\n",
        "    return final_path\n"
      ],
      "metadata": {
        "id": "Fy7BZpjHisb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# agent_dashboard.py â€” Add PDF export and confidence trend buttons\n",
        "import streamlit as st\n",
        "import os\n",
        "import json\n",
        "from agents.orchestrator import AgentOrchestrator\n",
        "from agents.memory_board import AgentMemoryBoard\n",
        "from agents.insight_utils import plot_confidence_trends, generate_audit_pdf\n",
        "\n",
        "st.set_page_config(page_title=\"Agentic Document Intelligence\", layout=\"wide\")\n",
        "st.title(\"ğŸ§  Agentic Lease Document Analyzer\")\n",
        "\n",
        "# --- Sidebar Config ---\n",
        "st.sidebar.header(\"Document & Metadata\")\n",
        "filters = {\n",
        "    \"Module Name\": st.sidebar.text_input(\"Module Name\", \"Real Estate\"),\n",
        "    \"Territory\": st.sidebar.text_input(\"Territory\", \"South\"),\n",
        "    \"Owner\": st.sidebar.text_input(\"Owner\", \"Crown\"),\n",
        "    \"Market\": st.sidebar.text_input(\"Market\", \"South East\"),\n",
        "    \"Contract Type\": st.sidebar.text_input(\"Contract Type\", \"Lease\"),\n",
        "    \"SubMarket\": st.sidebar.text_input(\"SubMarket\", \"Florida\"),\n",
        "    \"Document Type\": st.sidebar.text_input(\"Document Type\", \"Lease\"),\n",
        "    \"Local Market\": st.sidebar.text_input(\"Local Market\", \"Tampa\"),\n",
        "    \"Facility Type\": st.sidebar.text_input(\"Facility Type\", \"Easement\")\n",
        "}\n",
        "\n",
        "st.sidebar.markdown(\"---\")\n",
        "\n",
        "# --- Field Input ---\n",
        "st.sidebar.header(\"Fields to Extract\")\n",
        "fields = []\n",
        "field_count = st.sidebar.slider(\"# Fields\", 1, 10, 3)\n",
        "for i in range(field_count):\n",
        "    name = st.sidebar.text_input(f\"Field {i+1} Name\", key=f\"fname_{i}\")\n",
        "    desc = st.sidebar.text_input(f\"Field {i+1} Description\", key=f\"fdesc_{i}\")\n",
        "    if name:\n",
        "        fields.append({\"name\": name, \"description\": desc})\n",
        "\n",
        "# --- File Upload ---\n",
        "st.header(\"Upload PDF Lease Document\")\n",
        "uploaded = st.file_uploader(\"Choose PDF\", type=\"pdf\")\n",
        "\n",
        "doc_id = None\n",
        "if uploaded:\n",
        "    os.makedirs(\"uploads\", exist_ok=True)\n",
        "    doc_id = uploaded.name.replace(\".pdf\", \"\")\n",
        "    doc_path = os.path.join(\"uploads\", uploaded.name)\n",
        "    with open(doc_path, \"wb\") as f:\n",
        "        f.write(uploaded.read())\n",
        "\n",
        "    # Run agentic pipeline\n",
        "    if st.button(\"Run Agent Pipeline\"):\n",
        "        st.subheader(\"ğŸ§  Agent Execution Log\")\n",
        "        with st.spinner(\"Running full agent workflow...\"):\n",
        "            pipeline = AgentOrchestrator(doc_id=doc_id)\n",
        "            results = pipeline.run(doc_path, filters, fields)\n",
        "            st.success(\"Pipeline execution complete âœ…\")\n",
        "            st.json(results)\n",
        "\n",
        "# --- Memory Visualizer ---\n",
        "if doc_id and os.path.exists(f\"memory/{doc_id}.json\"):\n",
        "    st.markdown(\"---\")\n",
        "    st.header(\"ğŸ—‚ï¸ Agent Memory Replay\")\n",
        "    mem = AgentMemoryBoard(doc_id)\n",
        "    mem_data = mem.get_all()\n",
        "    for k, v in mem_data.items():\n",
        "        with st.expander(f\"{k}\"):\n",
        "            if isinstance(v, str):\n",
        "                st.code(v)\n",
        "            else:\n",
        "                st.json(v)\n",
        "\n",
        "    # Confidence Chart\n",
        "    st.markdown(\"---\")\n",
        "    st.subheader(\"ğŸ“Š Confidence Score Trends\")\n",
        "    chart_path = plot_confidence_trends(doc_id)\n",
        "    if chart_path:\n",
        "        st.image(chart_path, caption=\"Field-level Confidence Scores\")\n",
        "    else:\n",
        "        st.info(\"No valid confidence scores to show.\")\n",
        "\n",
        "    # PDF Export\n",
        "    st.markdown(\"---\")\n",
        "    st.subheader(\"ğŸ“„ Download Audit Report\")\n",
        "    audit_pdf = generate_audit_pdf(doc_id)\n",
        "    if audit_pdf:\n",
        "        with open(audit_pdf, \"rb\") as f:\n",
        "            st.download_button(\"ğŸ“¥ Download Audit PDF\", f, file_name=os.path.basename(audit_pdf), mime=\"application/pdf\")\n"
      ],
      "metadata": {
        "id": "LgILgUyqjCJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# agents/chunk_retrieval_agent.py\n",
        "from retrieval_pipeline import ChunkRetriever, FieldTaskPlanner, RetrievalExecutor\n",
        "from agents.memory_board import AgentMemoryBoard\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "class ChunkRetrievalAgent:\n",
        "    def __init__(self, doc_id: str, faiss_index=\"faiss_index\"):\n",
        "        self.doc_id = doc_id\n",
        "        self.memory = AgentMemoryBoard(doc_id)\n",
        "        self.retriever = ChunkRetriever(faiss_index)\n",
        "        self.llm = OpenAI(model=\"gpt-4\")\n",
        "        self.executor = RetrievalExecutor(self.retriever, self.llm)\n",
        "\n",
        "    def run(self, fields):\n",
        "        planner = FieldTaskPlanner()\n",
        "        tasks = planner.plan(fields)\n",
        "        result = self.executor.run_all(tasks)\n",
        "        self.memory.update(\"chunk_retrieval_results\", result)\n",
        "        return result\n"
      ],
      "metadata": {
        "id": "p939aQaeoH3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# orchestrator.py with ChunkRetrievalAgent added\n",
        "from agents.planner_agent import PlannerAgent\n",
        "from agents.agent_registry import AGENT_REGISTRY\n",
        "from agents.memory_board import AgentMemoryBoard\n",
        "from agents.chunk_retrieval_agent import ChunkRetrievalAgent\n",
        "import time\n",
        "\n",
        "class AgentOrchestrator:\n",
        "    def __init__(self, doc_id: str):\n",
        "        self.doc_id = doc_id\n",
        "        self.memory = AgentMemoryBoard(doc_id)\n",
        "        self.planner = PlannerAgent()\n",
        "        self.chunk_agent = ChunkRetrievalAgent(doc_id)\n",
        "\n",
        "    def run(self, pdf_path, filters, fields):\n",
        "        plan = self.planner.generate_plan(filters, fields)\n",
        "\n",
        "        for agent_name in plan:\n",
        "            agent = AGENT_REGISTRY.get(agent_name)\n",
        "            t0 = time.time()\n",
        "\n",
        "            if agent_name == \"DocumentParserAgent\":\n",
        "                output = agent.parse(pdf_path)\n",
        "                self.memory.update(\"parsed\", output, runtime=time.time() - t0)\n",
        "\n",
        "            elif agent_name == \"PromptBuilderAgent\":\n",
        "                output = agent.run(filters, fields)\n",
        "                self.memory.update(\"prompt\", output, retries=agent.retries, runtime=time.time() - t0)\n",
        "\n",
        "            elif agent_name == \"ExtractionAgent\":\n",
        "                prompt = self.memory.get(\"prompt\")\n",
        "                text = self.memory.get(\"parsed\")[\"text\"]\n",
        "                output = agent.run(prompt, text)\n",
        "                self.memory.update(\"extracted\", output, retries=agent.retries, runtime=time.time() - t0)\n",
        "\n",
        "            elif agent_name == \"ChunkRetrievalAgent\":\n",
        "                output = self.chunk_agent.run(fields)\n",
        "                self.memory.update(\"chunk_retrieval_results\", output, runtime=time.time() - t0)\n",
        "\n",
        "            elif agent_name == \"ValidationAgent\":\n",
        "                raw = self.memory.get(\"extracted\")\n",
        "                validated, error = agent.validate(raw)\n",
        "                self.memory.update(\"validated\", validated.dict() if validated else None, runtime=time.time() - t0)\n",
        "                if error:\n",
        "                    self.memory.update(\"error\", error)\n",
        "                    return {\"error\": error, \"raw\": raw}\n",
        "                return validated.dict()\n",
        "\n",
        "        return self.memory.get_all()\n"
      ],
      "metadata": {
        "id": "KsIk0qMXoVbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# memory_flow_panel.py â€” visual chunk view + replay control\n",
        "import streamlit as st\n",
        "import os\n",
        "import json\n",
        "\n",
        "def render_memory_flow(doc_id: str):\n",
        "    mem_path = os.path.join(\"memory\", f\"{doc_id}.json\")\n",
        "    if not os.path.exists(mem_path):\n",
        "        st.warning(\"No memory found for this document.\")\n",
        "        return\n",
        "\n",
        "    with open(mem_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    st.header(\"ğŸ“ˆ Agent Execution Trace\")\n",
        "    logs = data.get(\"_meta\", {}).get(\"log\", [])\n",
        "    steps = [entry[\"step\"] for entry in logs]\n",
        "    mermaid = [\"flowchart TD\"]\n",
        "    for i in range(len(steps) - 1):\n",
        "        mermaid.append(f\"    {steps[i]} --> {steps[i+1]}\")\n",
        "    st.markdown(\"```mermaid\\n\" + \"\\n\".join(mermaid) + \"\\n```\")\n",
        "\n",
        "    st.subheader(\"ğŸ§  Agent Outputs + Replay\")\n",
        "    for entry in logs:\n",
        "        key = entry[\"step\"]\n",
        "        val = data.get(key)\n",
        "        with st.expander(f\"{key} (retried {entry['retries']}x, time: {entry['runtime']:.2f}s)\"):\n",
        "            if key == \"chunk_retrieval_results\":\n",
        "                for field, res in val.items():\n",
        "                    with st.expander(f\"ğŸ” Field: {field}\"):\n",
        "                        st.json(res)\n",
        "                        if st.button(f\"ğŸ” Retry Field: {field}\", key=f\"retry_{field}\"):\n",
        "                            st.session_state[\"replay_chunk_field\"] = field\n",
        "                            st.success(f\"Field {field} set for replay\")\n",
        "            else:\n",
        "                st.json(val)\n",
        "\n",
        "    if \"replay_chunk_field\" in st.session_state:\n",
        "        field = st.session_state[\"replay_chunk_field\"]\n",
        "        st.markdown(f\"### â© Ready to rerun chunk retrieval for: `{field}`\")\n",
        "        if st.button(\"ğŸ” Run Chunk Subtask\"):\n",
        "            st.session_state[\"resume\"] = True\n",
        "            st.session_state[\"resume_field\"] = field\n",
        "            st.success(f\"Triggering LLM + chunk rerun for {field}\")\n"
      ],
      "metadata": {
        "id": "8mnGpbXFov2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# memory_flow_panel.py â€” now with chunk highlights and metadata\n",
        "import streamlit as st\n",
        "import os\n",
        "import json\n",
        "\n",
        "\n",
        "def render_memory_flow(doc_id: str):\n",
        "    mem_path = os.path.join(\"memory\", f\"{doc_id}.json\")\n",
        "    if not os.path.exists(mem_path):\n",
        "        st.warning(\"No memory found for this document.\")\n",
        "        return\n",
        "\n",
        "    with open(mem_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    st.header(\"ğŸ“ˆ Agent Execution Trace\")\n",
        "    logs = data.get(\"_meta\", {}).get(\"log\", [])\n",
        "    steps = [entry[\"step\"] for entry in logs]\n",
        "    mermaid = [\"flowchart TD\"]\n",
        "    for i in range(len(steps) - 1):\n",
        "        mermaid.append(f\"    {steps[i]} --> {steps[i+1]}\")\n",
        "    st.markdown(\"```mermaid\\n\" + \"\\n\".join(mermaid) + \"\\n```\")\n",
        "\n",
        "    st.subheader(\"ğŸ§  Agent Outputs + Replay\")\n",
        "    for entry in logs:\n",
        "        key = entry[\"step\"]\n",
        "        val = data.get(key)\n",
        "        with st.expander(f\"{key} (retried {entry['retries']}x, time: {entry['runtime']:.2f}s)\"):\n",
        "            if key == \"chunk_retrieval_results\":\n",
        "                for field, res in val.items():\n",
        "                    with st.expander(f\"ğŸ” Field: {field}\"):\n",
        "                        st.markdown(f\"**Value:** `{res.get('value')}`\")\n",
        "                        st.markdown(f\"**Reason:** {res.get('reason')}\")\n",
        "                        st.markdown(f\"**Confidence:** `{res.get('confidence')}`\")\n",
        "\n",
        "                        # Optional alignment and metadata if present\n",
        "                        if \"_aligned_chunks\" in res:\n",
        "                            for chunk in res[\"_aligned_chunks\"]:\n",
        "                                st.markdown(f\"**Chunk ID:** `{chunk.get('chunk_id')}` | **Page:** `{chunk.get('page_num')}` | **Doc ID:** `{chunk.get('doc_id')}`\")\n",
        "                                content = chunk.get(\"text\", \"\")\n",
        "                                highlight = res.get(\"value\", \"\")\n",
        "                                if highlight and highlight in content:\n",
        "                                    content = content.replace(highlight, f\"**:green[{highlight}]**\")\n",
        "                                st.markdown(f\"> {content}\", unsafe_allow_html=True)\n",
        "\n",
        "                        if st.button(f\"ğŸ” Retry Field: {field}\", key=f\"retry_{field}\"):\n",
        "                            st.session_state[\"replay_chunk_field\"] = field\n",
        "                            st.success(f\"Field {field} set for replay\")\n",
        "            else:\n",
        "                st.json(val)\n",
        "\n",
        "    if \"replay_chunk_field\" in st.session_state:\n",
        "        field = st.session_state[\"replay_chunk_field\"]\n",
        "        st.markdown(f\"### â© Ready to rerun chunk retrieval for: `{field}`\")\n",
        "        if st.button(\"ğŸ” Run Chunk Subtask\"):\n",
        "            st.session_state[\"resume\"] = True\n",
        "            st.session_state[\"resume_field\"] = field\n",
        "            st.success(f\"Triggering LLM + chunk rerun for {field}\")\n"
      ],
      "metadata": {
        "id": "QUdCWBe9pF-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# memory_flow_panel.py â€” now with chunk highlights and metadata\n",
        "import streamlit as st\n",
        "import os\n",
        "import json\n",
        "\n",
        "\n",
        "def render_memory_flow(doc_id: str):\n",
        "    mem_path = os.path.join(\"memory\", f\"{doc_id}.json\")\n",
        "    if not os.path.exists(mem_path):\n",
        "        st.warning(\"No memory found for this document.\")\n",
        "        return\n",
        "\n",
        "    with open(mem_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    st.header(\"ğŸ“ˆ Agent Execution Trace\")\n",
        "    logs = data.get(\"_meta\", {}).get(\"log\", [])\n",
        "    steps = [entry[\"step\"] for entry in logs]\n",
        "    mermaid = [\"flowchart TD\"]\n",
        "    for i in range(len(steps) - 1):\n",
        "        mermaid.append(f\"    {steps[i]} --> {steps[i+1]}\")\n",
        "    st.markdown(\"```mermaid\\n\" + \"\\n\".join(mermaid) + \"\\n```\")\n",
        "\n",
        "    st.subheader(\"ğŸ§  Agent Outputs + Replay\")\n",
        "    for entry in logs:\n",
        "        key = entry[\"step\"]\n",
        "        val = data.get(key)\n",
        "        with st.expander(f\"{key} (retried {entry['retries']}x, time: {entry['runtime']:.2f}s)\"):\n",
        "            if key == \"chunk_retrieval_results\":\n",
        "                for field, res in val.items():\n",
        "                    with st.expander(f\"ğŸ” Field: {field}\"):\n",
        "                        st.markdown(f\"**Value:** `{res.get('value')}`\")\n",
        "                        st.markdown(f\"**Reason:** {res.get('reason')}\")\n",
        "                        st.markdown(f\"**Confidence:** `{res.get('confidence')}`\")\n",
        "\n",
        "                        # Optional alignment and metadata if present\n",
        "                        if \"_aligned_chunks\" in res:\n",
        "                            for chunk in res[\"_aligned_chunks\"]:\n",
        "                                st.markdown(f\"**Chunk ID:** `{chunk.get('chunk_id')}` | **Page:** `{chunk.get('page_num')}` | **Doc ID:** `{chunk.get('doc_id')}`\")\n",
        "                                content = chunk.get(\"text\", \"\")\n",
        "                                highlight = res.get(\"value\", \"\")\n",
        "                                if highlight and highlight in content:\n",
        "                                    content = content.replace(highlight, f\"**:green[{highlight}]**\")\n",
        "                                st.markdown(f\"> {content}\", unsafe_allow_html=True)\n",
        "\n",
        "                        if st.button(f\"ğŸ” Retry Field: {field}\", key=f\"retry_{field}\"):\n",
        "                            st.session_state[\"replay_chunk_field\"] = field\n",
        "                            st.success(f\"Field {field} set for replay\")\n",
        "            else:\n",
        "                st.json(val)\n",
        "\n",
        "    if \"replay_chunk_field\" in st.session_state:\n",
        "        field = st.session_state[\"replay_chunk_field\"]\n",
        "        st.markdown(f\"### â© Ready to rerun chunk retrieval for: `{field}`\")\n",
        "        if st.button(\"ğŸ” Run Chunk Subtask\"):\n",
        "            st.session_state[\"resume\"] = True\n",
        "            st.session_state[\"resume_field\"] = field\n",
        "            st.success(f\"Triggering LLM + chunk rerun for {field}\")\n"
      ],
      "metadata": {
        "id": "206QdQ3gov5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# insight_utils.py â€” include chunk reasoning in audit PDF\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import os\n",
        "from fpdf import FPDF\n",
        "\n",
        "\n",
        "def plot_confidence_trends(doc_id):\n",
        "    path = f\"memory/{doc_id}.json\"\n",
        "    if not os.path.exists(path):\n",
        "        return None\n",
        "    with open(path, \"r\") as f:\n",
        "        mem = json.load(f)\n",
        "\n",
        "    validations = mem.get(\"validated\") or mem.get(\"chunk_retrieval_results\", {})\n",
        "    confidences = {k: v.get(\"confidence\") for k, v in validations.items() if isinstance(v, dict)}\n",
        "    if not confidences:\n",
        "        return None\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.bar(confidences.keys(), confidences.values(), color='green')\n",
        "    plt.title(\"Confidence Scores for Extracted Fields\")\n",
        "    plt.ylabel(\"Confidence\")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.xticks(rotation=45)\n",
        "    fig_path = f\"memory/{doc_id}_confidence.png\"\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(fig_path)\n",
        "    return fig_path\n",
        "\n",
        "\n",
        "def generate_audit_pdf(doc_id):\n",
        "    mem_path = f\"memory/{doc_id}.json\"\n",
        "    img_path = f\"memory/{doc_id}_confidence.png\"\n",
        "    if not os.path.exists(mem_path):\n",
        "        return None\n",
        "\n",
        "    with open(mem_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    pdf = FPDF()\n",
        "    pdf.set_auto_page_break(auto=True, margin=15)\n",
        "    pdf.add_page()\n",
        "    pdf.set_font(\"Arial\", size=12)\n",
        "    pdf.cell(200, 10, txt=\"Agent Flow Audit Report\", ln=True, align='C')\n",
        "    pdf.ln(10)\n",
        "    pdf.cell(200, 10, txt=f\"Document ID: {doc_id}\", ln=True)\n",
        "\n",
        "    for k in data:\n",
        "        if k == \"_meta\": continue\n",
        "        pdf.set_font(\"Arial\", 'B', 12)\n",
        "        pdf.cell(200, 10, txt=f\"Step: {k}\", ln=True)\n",
        "        pdf.set_font(\"Arial\", size=10)\n",
        "\n",
        "        if k == \"chunk_retrieval_results\":\n",
        "            for field, result in data[k].items():\n",
        "                pdf.set_font(\"Arial\", 'B', 11)\n",
        "                pdf.cell(200, 10, txt=f\"Field: {field}\", ln=True)\n",
        "                pdf.set_font(\"Arial\", size=9)\n",
        "                pdf.multi_cell(0, 5, txt=json.dumps({k: v for k, v in result.items() if not k.startswith(\"_aligned\")}, indent=2))\n",
        "                if \"_aligned_chunks\" in result:\n",
        "                    for chunk in result[\"_aligned_chunks\"]:\n",
        "                        pdf.set_font(\"Arial\", 'I', 9)\n",
        "                        pdf.multi_cell(0, 5, txt=f\"Chunk (Page {chunk['page_num']}, ID {chunk['chunk_id']}):\\n{chunk['text'][:300]}...\\n\")\n",
        "        else:\n",
        "            txt = json.dumps(data[k], indent=2)\n",
        "            for line in txt.split(\"\\n\"):\n",
        "                pdf.multi_cell(0, 5, txt=line)\n",
        "        pdf.ln()\n",
        "\n",
        "    if os.path.exists(img_path):\n",
        "        pdf.add_page()\n",
        "        pdf.image(img_path, x=10, y=30, w=180)\n",
        "\n",
        "    final_path = f\"memory/{doc_id}_audit.pdf\"\n",
        "    pdf.output(final_path)\n",
        "    return final_path\n"
      ],
      "metadata": {
        "id": "V9DFvwrupl1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[\n",
        "  {\n",
        "    \"subtask\": \"Find the monthly rent amount\",\n",
        "    \"type_filter\": \"table\",\n",
        "    \"hints\": [\"monthly rent\", \"$\"]\n",
        "  },\n",
        "  {\n",
        "    \"subtask\": \"Find the total lease duration in months or years\",\n",
        "    \"type_filter\": \"narrative_text\",\n",
        "    \"hints\": [\"term\", \"commence\", \"expire\"]\n",
        "  },\n",
        "  {\n",
        "    \"subtask\": \"Multiply rent Ã— duration to calculate total\",\n",
        "    \"type_filter\": \"inferred\",\n",
        "    \"calculation_required\": true\n",
        "  }\n",
        "]\n"
      ],
      "metadata": {
        "id": "oB9OMwKuwjQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "You are a financial reasoning assistant analyzing lease contracts.\n",
        "\n",
        "Context:\n",
        "{{ merged_chunks }}\n",
        "\n",
        "Task:\n",
        "Based on the context above, answer the following multi-hop field:\n",
        "\"{{ field_name }}\"\n",
        "\n",
        "Steps:\n",
        "1. Identify related values (e.g., rent amount, lease term)\n",
        "2. Infer final value using reasoning or math\n",
        "3. Return structured JSON\n",
        "\n",
        "Respond in:\n",
        "{\n",
        "  \"value\": \"...\",\n",
        "  \"reason\": \"...\",\n",
        "  \"confidence\": 0.0â€“1.0,\n",
        "  \"evidence\": [ { \"text\": \"...\", \"chunk_id\": \"...\", \"page\": ... } ]\n",
        "}\n"
      ],
      "metadata": {
        "id": "8eCVWHZ2wjTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from agents.multi_hop_reasoning_agent import MultiHopReasoningAgent\n",
        "\n",
        "AGENT_REGISTRY = {\n",
        "  ...\n",
        "  \"MultiHopReasoningAgent\": MultiHopReasoningAgent()\n",
        "}\n",
        "if task.get(\"multi_hop\"):\n",
        "    agent = AGENT_REGISTRY[\"MultiHopReasoningAgent\"]\n",
        "    output = agent.run(task, self.memory)\n",
        "    self.memory.update(f\"multi_hop_{task['name']}\", output)\n"
      ],
      "metadata": {
        "id": "ogq3J3zZwjV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# agents/multi_hop_reasoning_agent.py\n",
        "from retrieval_pipeline import ChunkRetriever\n",
        "from langchain.llms import OpenAI\n",
        "import json\n",
        "\n",
        "class MultiHopSubtaskPlanner:\n",
        "    def plan_subtasks(self, field_task):\n",
        "        name = field_task[\"name\"]\n",
        "        description = field_task.get(\"description\", name)\n",
        "        hints = field_task.get(\"hints\", [])\n",
        "\n",
        "        if \"total rent\" in name.lower():\n",
        "            return [\n",
        "                {\"subtask\": \"Find monthly rent\", \"hints\": [\"monthly\", \"$\"], \"type_filter\": \"table\"},\n",
        "                {\"subtask\": \"Find lease term duration\", \"hints\": [\"term\", \"years\", \"months\"], \"type_filter\": \"narrative_text\"},\n",
        "                {\"subtask\": \"Calculate total rent = monthly Ã— duration\", \"inferred\": True}\n",
        "            ]\n",
        "        else:\n",
        "            return [\n",
        "                {\"subtask\": f\"Reason and extract: {description}\", \"hints\": hints}\n",
        "            ]\n",
        "\n",
        "\n",
        "class MultiHopReasoningAgent:\n",
        "    def __init__(self, index_path=\"faiss_index\"):\n",
        "        self.retriever = ChunkRetriever(index_path)\n",
        "        self.llm = OpenAI(model=\"gpt-4\")\n",
        "        self.planner = MultiHopSubtaskPlanner()\n",
        "\n",
        "    def run(self, field_task, memory):\n",
        "        subtasks = self.planner.plan_subtasks(field_task)\n",
        "        all_chunks = []\n",
        "        merged_context = []\n",
        "\n",
        "        for sub in subtasks:\n",
        "            if sub.get(\"inferred\"):\n",
        "                continue  # Handle below\n",
        "            retrieved = self.retriever.hybrid_retrieve(\n",
        "                field_name=sub[\"subtask\"],\n",
        "                description=sub[\"subtask\"],\n",
        "                type_filter=sub.get(\"type_filter\"),\n",
        "                k=5,\n",
        "                hints=sub.get(\"hints\", [])\n",
        "            )\n",
        "            for chunk in retrieved:\n",
        "                merged_context.append(chunk.page_content)\n",
        "                all_chunks.append({\n",
        "                    \"chunk_id\": chunk.metadata[\"chunk_id\"],\n",
        "                    \"page\": chunk.metadata[\"page\"],\n",
        "                    \"type\": chunk.metadata[\"type\"],\n",
        "                    \"text\": chunk.page_content\n",
        "                })\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "You are performing multi-hop reasoning to answer: {field_task['name']}\n",
        "\n",
        "Context:\n",
        "{chr(10).join(merged_context)}\n",
        "\n",
        "Follow these steps:\n",
        "- Identify partial values from above\n",
        "- Perform reasoning to calculate or infer final value\n",
        "\n",
        "Return in JSON:\n",
        "{{\"value\": ..., \"reason\": ..., \"confidence\": 0.0â€“1.0, \"evidence\": [...]}}\n",
        "\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.llm.invoke(prompt)\n",
        "            result = json.loads(response)\n",
        "            result[\"_trace\"] = subtasks\n",
        "            result[\"_evidence\"] = all_chunks\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            return {\"value\": None, \"reason\": f\"Multi-hop failed: {str(e)}\", \"confidence\": 0.0}\n"
      ],
      "metadata": {
        "id": "X8ov64S_wjYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# orchestrator.py â€” integrate MultiHopReasoningAgent and persist trace\n",
        "from agents.planner_agent import PlannerAgent\n",
        "from agents.agent_registry import AGENT_REGISTRY\n",
        "from agents.memory_board import AgentMemoryBoard\n",
        "from agents.chunk_retrieval_agent import ChunkRetrievalAgent\n",
        "from agents.multi_hop_reasoning_agent import MultiHopReasoningAgent\n",
        "import time\n",
        "\n",
        "class AgentOrchestrator:\n",
        "    def __init__(self, doc_id: str):\n",
        "        self.doc_id = doc_id\n",
        "        self.memory = AgentMemoryBoard(doc_id)\n",
        "        self.planner = PlannerAgent()\n",
        "        self.chunk_agent = ChunkRetrievalAgent(doc_id)\n",
        "        self.multi_hop_agent = MultiHopReasoningAgent()\n",
        "\n",
        "    def run(self, pdf_path, filters, fields):\n",
        "        plan = self.planner.generate_plan(filters, fields)\n",
        "\n",
        "        for agent_name in plan:\n",
        "            agent = AGENT_REGISTRY.get(agent_name)\n",
        "            t0 = time.time()\n",
        "\n",
        "            if agent_name == \"DocumentParserAgent\":\n",
        "                output = agent.parse(pdf_path)\n",
        "                self.memory.update(\"parsed\", output, runtime=time.time() - t0)\n",
        "\n",
        "            elif agent_name == \"PromptBuilderAgent\":\n",
        "                output = agent.run(filters, fields)\n",
        "                self.memory.update(\"prompt\", output, retries=agent.retries, runtime=time.time() - t0)\n",
        "\n",
        "            elif agent_name == \"ChunkRetrievalAgent\":\n",
        "                output = self.chunk_agent.run(fields)\n",
        "                self.memory.update(\"chunk_retrieval_results\", output, runtime=time.time() - t0)\n",
        "\n",
        "            elif agent_name == \"ExtractionAgent\":\n",
        "                prompt = self.memory.get(\"prompt\")\n",
        "                text = self.memory.get(\"parsed\")[\"text\"]\n",
        "                output = agent.run(prompt, text)\n",
        "                self.memory.update(\"extracted\", output, retries=agent.retries, runtime=time.time() - t0)\n",
        "\n",
        "            elif agent_name == \"ValidationAgent\":\n",
        "                raw = self.memory.get(\"extracted\")\n",
        "                validated, error = agent.validate(raw)\n",
        "                self.memory.update(\"validated\", validated.dict() if validated else None, runtime=time.time() - t0)\n",
        "                if error:\n",
        "                    self.memory.update(\"error\", error)\n",
        "                    return {\"error\": error, \"raw\": raw}\n",
        "                return validated.dict()\n",
        "\n",
        "        # Post-plan: route multi-hop fields separately\n",
        "        for field in fields:\n",
        "            if field.get(\"multi_hop\"):\n",
        "                result = self.multi_hop_agent.run(field, self.memory)\n",
        "                key = f\"multi_hop_{field['name'].replace(' ', '_')}\"\n",
        "                self.memory.update(key, result, runtime=0)\n",
        "\n",
        "        return self.memory.get_all()\n"
      ],
      "metadata": {
        "id": "tOm7t5B5xWcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# agents/multi_hop_reasoning_agent.py â€” add hop-level scoring and retry fallback\n",
        "from retrieval_pipeline import ChunkRetriever\n",
        "from langchain.llms import OpenAI\n",
        "import json\n",
        "\n",
        "class MultiHopSubtaskPlanner:\n",
        "    def plan_subtasks(self, field_task):\n",
        "        name = field_task[\"name\"]\n",
        "        description = field_task.get(\"description\", name)\n",
        "        hints = field_task.get(\"hints\", [])\n",
        "\n",
        "        if \"total rent\" in name.lower():\n",
        "            return [\n",
        "                {\"subtask\": \"Find monthly rent\", \"hints\": [\"monthly\", \"$\"], \"type_filter\": \"table\"},\n",
        "                {\"subtask\": \"Find lease term duration\", \"hints\": [\"term\", \"years\", \"months\"], \"type_filter\": \"narrative_text\"},\n",
        "                {\"subtask\": \"Calculate total rent = monthly Ã— duration\", \"inferred\": True}\n",
        "            ]\n",
        "        else:\n",
        "            return [\n",
        "                {\"subtask\": f\"Reason and extract: {description}\", \"hints\": hints}\n",
        "            ]\n",
        "\n",
        "\n",
        "class MultiHopReasoningAgent:\n",
        "    def __init__(self, index_path=\"faiss_index\"):\n",
        "        self.retriever = ChunkRetriever(index_path)\n",
        "        self.llm = OpenAI(model=\"gpt-4\")\n",
        "        self.planner = MultiHopSubtaskPlanner()\n",
        "\n",
        "    def run(self, field_task, memory):\n",
        "        subtasks = self.planner.plan_subtasks(field_task)\n",
        "        all_chunks = []\n",
        "        merged_context = []\n",
        "        hops_failed = 0\n",
        "\n",
        "        for sub in subtasks:\n",
        "            if sub.get(\"inferred\"):\n",
        "                continue\n",
        "            retrieved = self.retriever.hybrid_retrieve(\n",
        "                field_name=sub[\"subtask\"],\n",
        "                description=sub[\"subtask\"],\n",
        "                type_filter=sub.get(\"type_filter\"),\n",
        "                k=5,\n",
        "                hints=sub.get(\"hints\", [])\n",
        "            )\n",
        "            if not retrieved:\n",
        "                hops_failed += 1\n",
        "            for chunk in retrieved:\n",
        "                merged_context.append(chunk.page_content)\n",
        "                all_chunks.append({\n",
        "                    \"chunk_id\": chunk.metadata[\"chunk_id\"],\n",
        "                    \"page\": chunk.metadata[\"page\"],\n",
        "                    \"type\": chunk.metadata[\"type\"],\n",
        "                    \"text\": chunk.page_content\n",
        "                })\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "You are performing multi-hop reasoning to answer: {field_task['name']}\n",
        "\n",
        "Context:\n",
        "{chr(10).join(merged_context)}\n",
        "\n",
        "Follow these steps:\n",
        "- Identify partial values from above\n",
        "- Perform reasoning to calculate or infer final value\n",
        "\n",
        "Return in JSON:\n",
        "{{\"value\": ..., \"reason\": ..., \"confidence\": 0.0â€“1.0, \"evidence\": [...]}}\n",
        "\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.llm.invoke(prompt)\n",
        "            result = json.loads(response)\n",
        "            result[\"_trace\"] = subtasks\n",
        "            result[\"_evidence\"] = all_chunks\n",
        "            result[\"_hops_failed\"] = hops_failed\n",
        "            if hops_failed >= len(subtasks) - 1:\n",
        "                result[\"confidence\"] = 0.0\n",
        "                result[\"reason\"] = result.get(\"reason\", \"Too many subtasks failed to support reasoning\")\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"value\": None,\n",
        "                \"reason\": f\"Multi-hop failed: {str(e)}\",\n",
        "                \"confidence\": 0.0,\n",
        "                \"_trace\": subtasks,\n",
        "                \"_evidence\": all_chunks,\n",
        "                \"_hops_failed\": hops_failed\n",
        "            }\n"
      ],
      "metadata": {
        "id": "_kp5NFm5xmSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# agents/multi_hop_reasoning_agent.py â€” fallback to direct extraction on total hop failure\n",
        "from retrieval_pipeline import ChunkRetriever, RetrievalExecutor\n",
        "from langchain.llms import OpenAI\n",
        "import json\n",
        "\n",
        "class MultiHopSubtaskPlanner:\n",
        "    def plan_subtasks(self, field_task):\n",
        "        name = field_task[\"name\"]\n",
        "        description = field_task.get(\"description\", name)\n",
        "        hints = field_task.get(\"hints\", [])\n",
        "\n",
        "        if \"total rent\" in name.lower():\n",
        "            return [\n",
        "                {\"subtask\": \"Find monthly rent\", \"hints\": [\"monthly\", \"$\"], \"type_filter\": \"table\"},\n",
        "                {\"subtask\": \"Find lease term duration\", \"hints\": [\"term\", \"years\", \"months\"], \"type_filter\": \"narrative_text\"},\n",
        "                {\"subtask\": \"Calculate total rent = monthly Ã— duration\", \"inferred\": True}\n",
        "            ]\n",
        "        else:\n",
        "            return [\n",
        "                {\"subtask\": f\"Reason and extract: {description}\", \"hints\": hints}\n",
        "            ]\n",
        "\n",
        "\n",
        "class MultiHopReasoningAgent:\n",
        "    def __init__(self, index_path=\"faiss_index\"):\n",
        "        self.retriever = ChunkRetriever(index_path)\n",
        "        self.llm = OpenAI(model=\"gpt-4\")\n",
        "        self.executor = RetrievalExecutor(self.retriever, self.llm)\n",
        "        self.planner = MultiHopSubtaskPlanner()\n",
        "\n",
        "    def run(self, field_task, memory):\n",
        "        subtasks = self.planner.plan_subtasks(field_task)\n",
        "        all_chunks = []\n",
        "        merged_context = []\n",
        "        hops_failed = 0\n",
        "\n",
        "        for sub in subtasks:\n",
        "            if sub.get(\"inferred\"):\n",
        "                continue\n",
        "            retrieved = self.retriever.hybrid_retrieve(\n",
        "                field_name=sub[\"subtask\"],\n",
        "                description=sub[\"subtask\"],\n",
        "                type_filter=sub.get(\"type_filter\"),\n",
        "                k=5,\n",
        "                hints=sub.get(\"hints\", [])\n",
        "            )\n",
        "            if not retrieved:\n",
        "                hops_failed += 1\n",
        "            for chunk in retrieved:\n",
        "                merged_context.append(chunk.page_content)\n",
        "                all_chunks.append({\n",
        "                    \"chunk_id\": chunk.metadata[\"chunk_id\"],\n",
        "                    \"page\": chunk.metadata[\"page\"],\n",
        "                    \"type\": chunk.metadata[\"type\"],\n",
        "                    \"text\": chunk.page_content\n",
        "                })\n",
        "\n",
        "        # fallback to single-hop if all non-inferred hops fail\n",
        "        if hops_failed >= len([s for s in subtasks if not s.get(\"inferred\")]):\n",
        "            fallback = self.executor.run_task(field_task)\n",
        "            fallback[\"_trace\"] = [\"multi-hop fallback: single-hop extraction used\"]\n",
        "            fallback[\"_hops_failed\"] = hops_failed\n",
        "            fallback[\"_fallback_used\"] = True\n",
        "            return fallback\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "You are performing multi-hop reasoning to answer: {field_task['name']}\n",
        "\n",
        "Context:\n",
        "{chr(10).join(merged_context)}\n",
        "\n",
        "Follow these steps:\n",
        "- Identify partial values from above\n",
        "- Perform reasoning to calculate or infer final value\n",
        "\n",
        "Return in JSON:\n",
        "{{\"value\": ..., \"reason\": ..., \"confidence\": 0.0â€“1.0, \"evidence\": [...]}}\n",
        "\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.llm.invoke(prompt)\n",
        "            result = json.loads(response)\n",
        "            result[\"_trace\"] = subtasks\n",
        "            result[\"_evidence\"] = all_chunks\n",
        "            result[\"_hops_failed\"] = hops_failed\n",
        "            result[\"_fallback_used\"] = False\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"value\": None,\n",
        "                \"reason\": f\"Multi-hop failed: {str(e)}\",\n",
        "                \"confidence\": 0.0,\n",
        "                \"_trace\": subtasks,\n",
        "                \"_evidence\": all_chunks,\n",
        "                \"_hops_failed\": hops_failed,\n",
        "                \"_fallback_used\": False\n",
        "            }\n"
      ],
      "metadata": {
        "id": "KVx-UYXsx0e8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# memory_flow_panel.py â€” display multi-hop fallback, hops failed, and trace\n",
        "import streamlit as st\n",
        "import os\n",
        "import json\n",
        "\n",
        "def render_memory_flow(doc_id: str):\n",
        "    mem_path = os.path.join(\"memory\", f\"{doc_id}.json\")\n",
        "    if not os.path.exists(mem_path):\n",
        "        st.warning(\"No memory found for this document.\")\n",
        "        return\n",
        "\n",
        "    with open(mem_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    st.header(\"ğŸ“ˆ Agent Execution Trace\")\n",
        "    logs = data.get(\"_meta\", {}).get(\"log\", [])\n",
        "    steps = [entry[\"step\"] for entry in logs]\n",
        "    mermaid = [\"flowchart TD\"]\n",
        "    for i in range(len(steps) - 1):\n",
        "        mermaid.append(f\"    {steps[i]} --> {steps[i+1]}\")\n",
        "    st.markdown(\"```mermaid\\n\" + \"\\n\".join(mermaid) + \"\\n```\")\n",
        "\n",
        "    st.subheader(\"ğŸ§  Agent Outputs + Replay\")\n",
        "    for key, val in data.items():\n",
        "        if key.startswith(\"multi_hop_\"):\n",
        "            with st.expander(f\"ğŸ§  {key} (Multi-Hop Field Result)\"):\n",
        "                st.markdown(f\"**Value:** `{val.get('value')}`\")\n",
        "                st.markdown(f\"**Reason:** {val.get('reason')}\")\n",
        "                st.markdown(f\"**Confidence:** `{val.get('confidence')}`\")\n",
        "                st.markdown(f\"**Fallback Used:** `{val.get('_fallback_used', False)}`\")\n",
        "                st.markdown(f\"**Hops Failed:** `{val.get('_hops_failed', 0)}`\")\n",
        "\n",
        "                if \"_trace\" in val:\n",
        "                    st.markdown(\"**Reasoning Trace:**\")\n",
        "                    for step in val[\"_trace\"]:\n",
        "                        st.code(step if isinstance(step, str) else json.dumps(step, indent=2))\n",
        "\n",
        "                if \"_evidence\" in val:\n",
        "                    st.markdown(\"**Supporting Evidence Chunks:**\")\n",
        "                    for chunk in val[\"_evidence\"]:\n",
        "                        with st.expander(f\"Chunk {chunk['chunk_id']} (Page {chunk['page']}, Type {chunk['type']})\"):\n",
        "                            st.code(chunk[\"text\"][:800])\n",
        "\n",
        "    for entry in logs:\n",
        "        key = entry[\"step\"]\n",
        "        val = data.get(key)\n",
        "        if key.startswith(\"multi_hop_\"):\n",
        "            continue  # already shown\n",
        "        with st.expander(f\"{key} (retried {entry['retries']}x, time: {entry['runtime']:.2f}s)\"):\n",
        "            st.json(val)\n"
      ],
      "metadata": {
        "id": "czSTTWlAyDO1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}