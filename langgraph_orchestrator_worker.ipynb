{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph Orchestrator-Worker Framework\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "This notebook implements a **LangGraph-driven orchestrator** that coordinates multiple **worker agents deployed as independent microservices**. The architecture follows an orchestrator-worker pattern where:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                        ORCHESTRATOR (LangGraph)                     │\n",
    "│                                                                     │\n",
    "│  ┌──────────┐   ┌───────────┐   ┌───────────┐   ┌──────────────┐  │\n",
    "│  │ Planning  │──▶│ Discovery │──▶│ Execution │──▶│  Response    │  │\n",
    "│  │  Agent    │   │  Service  │   │  Router   │   │  Synthesis   │  │\n",
    "│  └──────────┘   └───────────┘   └───────────┘   └──────────────┘  │\n",
    "│                       │               │                             │\n",
    "│                       ▼               ▼                             │\n",
    "│              ┌─────────────┐  ┌─────────────┐                      │\n",
    "│              │  Agent      │  │  Streaming   │                      │\n",
    "│              │  Registry   │  │  Multiplexer │                      │\n",
    "│              └─────────────┘  └─────────────┘                      │\n",
    "└───────────────────────┬───────────────┬─────────────────────────────┘\n",
    "                        │               │\n",
    "          ┌─────────────┼───────────────┼─────────────┐\n",
    "          ▼             ▼               ▼             ▼\n",
    "   ┌────────────┐ ┌────────────┐ ┌────────────┐ ┌────────────┐\n",
    "   │  Worker A  │ │  Worker B  │ │  Worker C  │ │  Worker N  │\n",
    "   │ (Dynamic   │ │ (Static    │ │ (Dynamic   │ │ (Static    │\n",
    "   │  Streaming)│ │  Response) │ │  Streaming)│ │  Response) │\n",
    "   │  :8001     │ │  :8002     │ │  :8003     │ │  :800N     │\n",
    "   └────────────┘ └────────────┘ └────────────┘ └────────────┘\n",
    "```\n",
    "\n",
    "### Core Components\n",
    "\n",
    "1. **Planning Agent** — Decomposes user queries into executable sub-tasks\n",
    "2. **Discovery Service** — Microservice that provides agent metadata (endpoints, payloads, capabilities, streaming mode)\n",
    "3. **Execution Router** — Dispatches sub-tasks to appropriate worker agents via HTTP\n",
    "4. **Response Synthesis** — Aggregates results from multiple workers into a coherent response\n",
    "5. **Streaming Multiplexer** — Handles both dynamic (LLM-streamed) and static (hardcoded) responses\n",
    "\n",
    "### Streaming Modes\n",
    "\n",
    "| Mode | Description | Use Case |\n",
    "|------|-------------|----------|\n",
    "| **Dynamic (event-level)** | SSE stream of token-by-token LLM output | Real-time generative agents |\n",
    "| **Dynamic (chat-level)** | SSE stream of complete chat messages | Conversational agents |\n",
    "| **Static** | Pre-defined response returned immediately | Deterministic/rule-based agents |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dependencies & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import logging\n",
    "from enum import Enum\n",
    "from typing import (\n",
    "    Any,\n",
    "    AsyncGenerator,\n",
    "    Dict,\n",
    "    List,\n",
    "    Literal,\n",
    "    Optional,\n",
    "    Sequence,\n",
    "    TypedDict,\n",
    "    Union,\n",
    "    Annotated,\n",
    ")\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "\n",
    "import httpx\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\")\n",
    "logger = logging.getLogger(\"orchestrator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Configuration — swap these for your deployment\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"sk-...\")\n",
    "DISCOVERY_SERVICE_URL = os.getenv(\"DISCOVERY_SERVICE_URL\", \"http://localhost:9000\")\n",
    "\n",
    "ORCHESTRATOR_LLM_MODEL = \"gpt-4o\"\n",
    "ORCHESTRATOR_LLM_TEMPERATURE = 0.2\n",
    "\n",
    "WORKER_TIMEOUT_SECONDS = 120\n",
    "WORKER_MAX_RETRIES = 2\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=ORCHESTRATOR_LLM_MODEL,\n",
    "    temperature=ORCHESTRATOR_LLM_TEMPERATURE,\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    streaming=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Models\n",
    "\n",
    "Pydantic models define the contract between the orchestrator, discovery service, and worker agents. Every worker is described by an `AgentDescriptor` that the discovery service returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingMode(str, Enum):\n",
    "    \"\"\"How a worker agent delivers its output back to the orchestrator.\"\"\"\n",
    "    DYNAMIC_EVENTS = \"dynamic_events\"    # SSE token-level stream\n",
    "    DYNAMIC_MESSAGES = \"dynamic_messages\"  # SSE chat-message-level stream\n",
    "    STATIC = \"static\"                     # Pre-canned / hardcoded response\n",
    "\n",
    "\n",
    "class AgentCapability(str, Enum):\n",
    "    \"\"\"Broad capability tags so the planner can match tasks to agents.\"\"\"\n",
    "    TEXT_GENERATION = \"text_generation\"\n",
    "    CODE_GENERATION = \"code_generation\"\n",
    "    DATA_ANALYSIS = \"data_analysis\"\n",
    "    SUMMARIZATION = \"summarization\"\n",
    "    TRANSLATION = \"translation\"\n",
    "    SEARCH = \"search\"\n",
    "    CUSTOM = \"custom\"\n",
    "\n",
    "\n",
    "class PayloadSchema(BaseModel):\n",
    "    \"\"\"Describes the JSON payload a worker agent expects.\"\"\"\n",
    "    content_type: str = \"application/json\"\n",
    "    required_fields: Dict[str, str] = Field(\n",
    "        default_factory=dict,\n",
    "        description=\"Map of field name to expected type, e.g. {'query': 'string'}\",\n",
    "    )\n",
    "    optional_fields: Dict[str, str] = Field(default_factory=dict)\n",
    "    example: Dict[str, Any] = Field(default_factory=dict)\n",
    "\n",
    "\n",
    "class AgentDescriptor(BaseModel):\n",
    "    \"\"\"\n",
    "    Full metadata for a single worker agent.\n",
    "    The discovery service returns a list of these.\n",
    "    \"\"\"\n",
    "    agent_id: str = Field(..., description=\"Unique identifier\")\n",
    "    name: str\n",
    "    description: str\n",
    "    endpoint: str = Field(..., description=\"Base URL, e.g. http://worker-a:8001\")\n",
    "    health_endpoint: str = Field(default=\"/health\")\n",
    "    invoke_endpoint: str = Field(default=\"/invoke\")\n",
    "    stream_endpoint: str = Field(default=\"/stream\")\n",
    "    capabilities: List[AgentCapability] = Field(default_factory=list)\n",
    "    streaming_mode: StreamingMode = StreamingMode.STATIC\n",
    "    payload_schema: PayloadSchema = Field(default_factory=PayloadSchema)\n",
    "    static_response: Optional[str] = Field(\n",
    "        None,\n",
    "        description=\"If streaming_mode is STATIC, this is the hardcoded response.\",\n",
    "    )\n",
    "    priority: int = Field(default=0, description=\"Higher = preferred when multiple agents match\")\n",
    "    max_tokens: Optional[int] = None\n",
    "    timeout_seconds: int = 60\n",
    "    metadata: Dict[str, Any] = Field(default_factory=dict)\n",
    "\n",
    "\n",
    "class SubTask(BaseModel):\n",
    "    \"\"\"A single unit of work the planner produces.\"\"\"\n",
    "    task_id: str\n",
    "    description: str\n",
    "    required_capability: AgentCapability\n",
    "    input_data: Dict[str, Any] = Field(default_factory=dict)\n",
    "    depends_on: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"task_ids this sub-task depends on (for DAG execution)\",\n",
    "    )\n",
    "\n",
    "\n",
    "class ExecutionPlan(BaseModel):\n",
    "    \"\"\"Output of the planning agent.\"\"\"\n",
    "    plan_id: str\n",
    "    original_query: str\n",
    "    sub_tasks: List[SubTask]\n",
    "    reasoning: str = \"\"\n",
    "\n",
    "\n",
    "class WorkerResult(BaseModel):\n",
    "    \"\"\"Result returned by a single worker invocation.\"\"\"\n",
    "    agent_id: str\n",
    "    task_id: str\n",
    "    success: bool\n",
    "    response: str = \"\"\n",
    "    streaming_mode: StreamingMode = StreamingMode.STATIC\n",
    "    events: List[Dict[str, Any]] = Field(default_factory=list)\n",
    "    error: Optional[str] = None\n",
    "    latency_ms: float = 0.0\n",
    "\n",
    "\n",
    "class StreamEvent(BaseModel):\n",
    "    \"\"\"\n",
    "    A single event yielded during streaming.\n",
    "    Carries enough metadata so the consumer knows which agent it came from.\n",
    "    \"\"\"\n",
    "    agent_id: str\n",
    "    task_id: str\n",
    "    event_type: Literal[\"token\", \"message\", \"status\", \"error\", \"done\"]\n",
    "    data: str\n",
    "    timestamp: str = Field(default_factory=lambda: datetime.utcnow().isoformat())\n",
    "\n",
    "\n",
    "print(\"Data models loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Discovery Service Client\n",
    "\n",
    "The Discovery Service is itself a microservice. The orchestrator calls it at startup (and optionally on a refresh interval) to learn which worker agents are available, what payloads they expect, and whether they stream dynamically or return static responses.\n",
    "\n",
    "> **Recommendation:** In production, add a caching layer (TTL-based) so you don't call discovery on every request. Use a background refresh task instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscoveryServiceClient:\n",
    "    \"\"\"\n",
    "    Client for the Discovery microservice.\n",
    "\n",
    "    Responsibilities:\n",
    "      - Fetch the full agent registry\n",
    "      - Filter agents by capability\n",
    "      - Health-check individual agents\n",
    "      - Cache results to reduce network chatter\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_url: str, cache_ttl_seconds: int = 300):\n",
    "        self._base_url = base_url.rstrip(\"/\")\n",
    "        self._cache_ttl = cache_ttl_seconds\n",
    "        self._cache: Optional[List[AgentDescriptor]] = None\n",
    "        self._cache_ts: Optional[datetime] = None\n",
    "        self._client = httpx.AsyncClient(timeout=30)\n",
    "\n",
    "    def _cache_is_valid(self) -> bool:\n",
    "        if self._cache is None or self._cache_ts is None:\n",
    "            return False\n",
    "        elapsed = (datetime.utcnow() - self._cache_ts).total_seconds()\n",
    "        return elapsed < self._cache_ttl\n",
    "\n",
    "    async def discover_agents(self, force_refresh: bool = False) -> List[AgentDescriptor]:\n",
    "        \"\"\"Fetch all registered agents from the discovery service.\"\"\"\n",
    "        if self._cache_is_valid() and not force_refresh:\n",
    "            logger.info(\"Returning %d agents from cache\", len(self._cache))\n",
    "            return self._cache\n",
    "\n",
    "        url = f\"{self._base_url}/agents\"\n",
    "        logger.info(\"Fetching agent registry from %s\", url)\n",
    "        try:\n",
    "            resp = await self._client.get(url)\n",
    "            resp.raise_for_status()\n",
    "            raw = resp.json()\n",
    "            agents = [AgentDescriptor(**a) for a in raw.get(\"agents\", raw)]\n",
    "            self._cache = agents\n",
    "            self._cache_ts = datetime.utcnow()\n",
    "            logger.info(\"Discovered %d agents\", len(agents))\n",
    "            return agents\n",
    "        except httpx.HTTPError as exc:\n",
    "            logger.error(\"Discovery service error: %s\", exc)\n",
    "            if self._cache is not None:\n",
    "                logger.warning(\"Falling back to stale cache (%d agents)\", len(self._cache))\n",
    "                return self._cache\n",
    "            raise\n",
    "\n",
    "    async def get_agents_by_capability(\n",
    "        self, capability: AgentCapability, force_refresh: bool = False\n",
    "    ) -> List[AgentDescriptor]:\n",
    "        \"\"\"Return agents that advertise a specific capability, sorted by priority.\"\"\"\n",
    "        agents = await self.discover_agents(force_refresh=force_refresh)\n",
    "        matched = [a for a in agents if capability in a.capabilities]\n",
    "        return sorted(matched, key=lambda a: a.priority, reverse=True)\n",
    "\n",
    "    async def health_check(self, agent: AgentDescriptor) -> bool:\n",
    "        \"\"\"Ping a single agent's health endpoint.\"\"\"\n",
    "        url = f\"{agent.endpoint.rstrip('/')}{agent.health_endpoint}\"\n",
    "        try:\n",
    "            resp = await self._client.get(url, timeout=5)\n",
    "            return resp.status_code == 200\n",
    "        except httpx.HTTPError:\n",
    "            return False\n",
    "\n",
    "    async def close(self):\n",
    "        await self._client.aclose()\n",
    "\n",
    "\n",
    "discovery_client = DiscoveryServiceClient(DISCOVERY_SERVICE_URL)\n",
    "print(\"Discovery service client ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Streaming Infrastructure\n",
    "\n",
    "The streaming multiplexer handles the two fundamentally different output modes:\n",
    "\n",
    "- **Dynamic streaming** — The worker returns an SSE stream. We consume it token-by-token (event-level) or message-by-message (chat-level) and re-yield `StreamEvent` objects so the orchestrator can forward them to the caller in real time.\n",
    "- **Static responses** — The worker's response is hardcoded in the agent descriptor (or returned as a plain JSON body). We wrap it into a single `StreamEvent` for uniform handling.\n",
    "\n",
    "> **Recommendation:** Use `httpx` with `aiter_lines()` for SSE parsing rather than pulling in a dedicated SSE library — it keeps dependencies lean and gives you full control over reconnection logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingMultiplexer:\n",
    "    \"\"\"\n",
    "    Unified streaming layer that abstracts over dynamic SSE streams and static\n",
    "    responses, exposing a single async-generator interface to the orchestrator.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._client = httpx.AsyncClient(timeout=WORKER_TIMEOUT_SECONDS)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Dynamic event-level streaming  (token by token from LLM workers)\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    async def stream_events(\n",
    "        self, agent: AgentDescriptor, task_id: str, payload: Dict[str, Any]\n",
    "    ) -> AsyncGenerator[StreamEvent, None]:\n",
    "        \"\"\"\n",
    "        Connect to a worker's SSE /stream endpoint and yield StreamEvent\n",
    "        objects as tokens arrive.\n",
    "\n",
    "        Expected SSE format from the worker:\n",
    "            data: {\"token\": \"Hello\", \"type\": \"token\"}\n",
    "            data: {\"token\": \" world\", \"type\": \"token\"}\n",
    "            data: {\"type\": \"done\"}\n",
    "        \"\"\"\n",
    "        url = f\"{agent.endpoint.rstrip('/')}{agent.stream_endpoint}\"\n",
    "        logger.info(\"[stream_events] POST %s  task=%s\", url, task_id)\n",
    "\n",
    "        yield StreamEvent(\n",
    "            agent_id=agent.agent_id,\n",
    "            task_id=task_id,\n",
    "            event_type=\"status\",\n",
    "            data=f\"Connecting to {agent.name}...\",\n",
    "        )\n",
    "\n",
    "        async with self._client.stream(\"POST\", url, json=payload) as resp:\n",
    "            resp.raise_for_status()\n",
    "            async for line in resp.aiter_lines():\n",
    "                if not line.startswith(\"data:\"):\n",
    "                    continue\n",
    "                raw = line[len(\"data:\"):].strip()\n",
    "                if not raw:\n",
    "                    continue\n",
    "                try:\n",
    "                    chunk = json.loads(raw)\n",
    "                except json.JSONDecodeError:\n",
    "                    yield StreamEvent(\n",
    "                        agent_id=agent.agent_id,\n",
    "                        task_id=task_id,\n",
    "                        event_type=\"token\",\n",
    "                        data=raw,\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                event_type = chunk.get(\"type\", \"token\")\n",
    "                if event_type == \"done\":\n",
    "                    yield StreamEvent(\n",
    "                        agent_id=agent.agent_id,\n",
    "                        task_id=task_id,\n",
    "                        event_type=\"done\",\n",
    "                        data=\"\",\n",
    "                    )\n",
    "                    return\n",
    "\n",
    "                yield StreamEvent(\n",
    "                    agent_id=agent.agent_id,\n",
    "                    task_id=task_id,\n",
    "                    event_type=event_type,\n",
    "                    data=chunk.get(\"token\", chunk.get(\"content\", \"\")),\n",
    "                )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Dynamic message-level streaming  (complete messages from LLM workers)\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    async def stream_messages(\n",
    "        self, agent: AgentDescriptor, task_id: str, payload: Dict[str, Any]\n",
    "    ) -> AsyncGenerator[StreamEvent, None]:\n",
    "        \"\"\"\n",
    "        Like stream_events but each SSE frame is a complete chat message.\n",
    "\n",
    "        Expected SSE format:\n",
    "            data: {\"role\": \"assistant\", \"content\": \"Full sentence one.\"}\n",
    "            data: {\"role\": \"assistant\", \"content\": \"Full sentence two.\"}\n",
    "            data: {\"type\": \"done\"}\n",
    "        \"\"\"\n",
    "        url = f\"{agent.endpoint.rstrip('/')}{agent.stream_endpoint}\"\n",
    "        logger.info(\"[stream_messages] POST %s  task=%s\", url, task_id)\n",
    "\n",
    "        yield StreamEvent(\n",
    "            agent_id=agent.agent_id,\n",
    "            task_id=task_id,\n",
    "            event_type=\"status\",\n",
    "            data=f\"Streaming messages from {agent.name}...\",\n",
    "        )\n",
    "\n",
    "        async with self._client.stream(\"POST\", url, json=payload) as resp:\n",
    "            resp.raise_for_status()\n",
    "            async for line in resp.aiter_lines():\n",
    "                if not line.startswith(\"data:\"):\n",
    "                    continue\n",
    "                raw = line[len(\"data:\"):].strip()\n",
    "                if not raw:\n",
    "                    continue\n",
    "                try:\n",
    "                    chunk = json.loads(raw)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "\n",
    "                if chunk.get(\"type\") == \"done\":\n",
    "                    yield StreamEvent(\n",
    "                        agent_id=agent.agent_id,\n",
    "                        task_id=task_id,\n",
    "                        event_type=\"done\",\n",
    "                        data=\"\",\n",
    "                    )\n",
    "                    return\n",
    "\n",
    "                yield StreamEvent(\n",
    "                    agent_id=agent.agent_id,\n",
    "                    task_id=task_id,\n",
    "                    event_type=\"message\",\n",
    "                    data=chunk.get(\"content\", json.dumps(chunk)),\n",
    "                )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Static (hardcoded) responses\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    async def emit_static(\n",
    "        self, agent: AgentDescriptor, task_id: str, payload: Dict[str, Any]\n",
    "    ) -> AsyncGenerator[StreamEvent, None]:\n",
    "        \"\"\"\n",
    "        For agents with StreamingMode.STATIC: either return the hardcoded\n",
    "        static_response from the descriptor, or POST to /invoke and wrap\n",
    "        the JSON body as a single message event.\n",
    "        \"\"\"\n",
    "        if agent.static_response:\n",
    "            logger.info(\"[static] Using hardcoded response for %s\", agent.agent_id)\n",
    "            yield StreamEvent(\n",
    "                agent_id=agent.agent_id,\n",
    "                task_id=task_id,\n",
    "                event_type=\"message\",\n",
    "                data=agent.static_response,\n",
    "            )\n",
    "        else:\n",
    "            url = f\"{agent.endpoint.rstrip('/')}{agent.invoke_endpoint}\"\n",
    "            logger.info(\"[static] POST %s  task=%s\", url, task_id)\n",
    "            resp = await self._client.post(url, json=payload)\n",
    "            resp.raise_for_status()\n",
    "            body = resp.json()\n",
    "            yield StreamEvent(\n",
    "                agent_id=agent.agent_id,\n",
    "                task_id=task_id,\n",
    "                event_type=\"message\",\n",
    "                data=body.get(\"response\", json.dumps(body)),\n",
    "            )\n",
    "\n",
    "        yield StreamEvent(\n",
    "            agent_id=agent.agent_id,\n",
    "            task_id=task_id,\n",
    "            event_type=\"done\",\n",
    "            data=\"\",\n",
    "        )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Unified dispatcher — picks the right method based on streaming mode\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    async def stream(\n",
    "        self, agent: AgentDescriptor, task_id: str, payload: Dict[str, Any]\n",
    "    ) -> AsyncGenerator[StreamEvent, None]:\n",
    "        \"\"\"\n",
    "        Single entry-point: route to the correct streaming strategy based\n",
    "        on the agent's declared streaming_mode.\n",
    "        \"\"\"\n",
    "        if agent.streaming_mode == StreamingMode.DYNAMIC_EVENTS:\n",
    "            async for event in self.stream_events(agent, task_id, payload):\n",
    "                yield event\n",
    "        elif agent.streaming_mode == StreamingMode.DYNAMIC_MESSAGES:\n",
    "            async for event in self.stream_messages(agent, task_id, payload):\n",
    "                yield event\n",
    "        else:\n",
    "            async for event in self.emit_static(agent, task_id, payload):\n",
    "                yield event\n",
    "\n",
    "    async def close(self):\n",
    "        await self._client.aclose()\n",
    "\n",
    "\n",
    "streaming_mux = StreamingMultiplexer()\n",
    "print(\"Streaming multiplexer ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LangGraph State Definition\n",
    "\n",
    "The orchestrator's shared state flows through every node in the graph. We use `TypedDict` with `Annotated` fields so LangGraph can merge message lists automatically across nodes.\n",
    "\n",
    "> **Recommendation:** Keep the state flat rather than deeply nested. LangGraph checkpoints serialize the full state — flat structures are cheaper to persist and easier to debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrchestratorState(TypedDict):\n",
    "    \"\"\"\n",
    "    Shared state that travels through the LangGraph orchestrator.\n",
    "    Every node reads from and writes to this dict.\n",
    "    \"\"\"\n",
    "\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    user_query: str\n",
    "    execution_plan: Optional[ExecutionPlan]\n",
    "    available_agents: List[AgentDescriptor]\n",
    "    task_agent_map: Dict[str, AgentDescriptor]\n",
    "    worker_results: List[WorkerResult]\n",
    "    stream_buffer: List[StreamEvent]\n",
    "    final_response: str\n",
    "    current_phase: str\n",
    "    error: Optional[str]\n",
    "\n",
    "\n",
    "def make_initial_state(query: str) -> OrchestratorState:\n",
    "    \"\"\"Factory for a clean initial state.\"\"\"\n",
    "    return OrchestratorState(\n",
    "        messages=[HumanMessage(content=query)],\n",
    "        user_query=query,\n",
    "        execution_plan=None,\n",
    "        available_agents=[],\n",
    "        task_agent_map={},\n",
    "        worker_results=[],\n",
    "        stream_buffer=[],\n",
    "        final_response=\"\",\n",
    "        current_phase=\"planning\",\n",
    "        error=None,\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"State schema defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Orchestrator Nodes\n",
    "\n",
    "Each function below is a **LangGraph node**. The graph wires them in sequence:\n",
    "\n",
    "```\n",
    "planning -> discovery -> execution -> response_synthesis\n",
    "```\n",
    "\n",
    "Conditional edges handle error recovery and re-planning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6a. Planning Agent Node\n",
    "\n",
    "The planning agent uses the orchestrator's own LLM to decompose the user query into discrete sub-tasks, each tagged with a required capability. This allows the discovery node to match tasks to the best available worker.\n",
    "\n",
    "> **Recommendation:** Use structured output (function calling / tool use) to get the LLM to return a typed `ExecutionPlan` rather than parsing free-form text. This eliminates brittle regex parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLANNING_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are the Planning Agent inside an orchestration framework.\n",
    "\n",
    "Your job:\n",
    "1. Analyse the user's query.\n",
    "2. Decompose it into 1-N independent sub-tasks.\n",
    "3. For each sub-task, assign one of these capabilities:\n",
    "   text_generation, code_generation, data_analysis,\n",
    "   summarization, translation, search, custom.\n",
    "4. Identify dependencies between sub-tasks (if any).\n",
    "\n",
    "Return ONLY valid JSON matching this schema (no markdown fences):\n",
    "{\n",
    "  \"plan_id\": \"<uuid-like string>\",\n",
    "  \"original_query\": \"<the user query>\",\n",
    "  \"reasoning\": \"<brief explanation of the decomposition>\",\n",
    "  \"sub_tasks\": [\n",
    "    {\n",
    "      \"task_id\": \"t1\",\n",
    "      \"description\": \"...\",\n",
    "      \"required_capability\": \"text_generation\",\n",
    "      \"input_data\": {\"query\": \"...\"},\n",
    "      \"depends_on\": []\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "async def planning_node(state: OrchestratorState) -> dict:\n",
    "    \"\"\"\n",
    "    LangGraph node: invoke the LLM to produce an ExecutionPlan.\n",
    "    \"\"\"\n",
    "    logger.info(\"=== PLANNING NODE ===\")\n",
    "    user_query = state[\"user_query\"]\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(content=PLANNING_SYSTEM_PROMPT),\n",
    "        HumanMessage(content=f\"User query:\\n{user_query}\"),\n",
    "    ]\n",
    "\n",
    "    resp = await llm.ainvoke(messages)\n",
    "    raw_text = resp.content.strip()\n",
    "\n",
    "    # Strip markdown code fences if the LLM wraps its JSON\n",
    "    if raw_text.startswith(\"```\"):\n",
    "        raw_text = \"\\n\".join(raw_text.split(\"\\n\")[1:])\n",
    "    if raw_text.endswith(\"```\"):\n",
    "        raw_text = \"\\n\".join(raw_text.split(\"\\n\")[:-1])\n",
    "\n",
    "    try:\n",
    "        plan_data = json.loads(raw_text)\n",
    "        plan = ExecutionPlan(**plan_data)\n",
    "    except (json.JSONDecodeError, Exception) as exc:\n",
    "        logger.error(\"Planning failed to parse: %s\", exc)\n",
    "        return {\n",
    "            \"error\": f\"Planning parse error: {exc}\",\n",
    "            \"current_phase\": \"error\",\n",
    "            \"messages\": [AIMessage(content=f\"Planning failed: {exc}\")],\n",
    "        }\n",
    "\n",
    "    logger.info(\"Plan: %d sub-tasks, reasoning=%s\", len(plan.sub_tasks), plan.reasoning[:80])\n",
    "\n",
    "    return {\n",
    "        \"execution_plan\": plan,\n",
    "        \"current_phase\": \"discovery\",\n",
    "        \"messages\": [\n",
    "            AIMessage(content=f\"Plan created with {len(plan.sub_tasks)} sub-task(s): {plan.reasoning}\")\n",
    "        ],\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Planning node defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6b. Discovery Node\n",
    "\n",
    "This node calls the Discovery Service microservice to fetch the live agent registry, then maps each sub-task from the plan to the best-matching agent based on capability and priority.\n",
    "\n",
    "> **Recommendation:** Implement a fallback strategy — if the primary agent is unhealthy, automatically pick the next-highest-priority agent with the same capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def discovery_node(state: OrchestratorState) -> dict:\n",
    "    \"\"\"\n",
    "    LangGraph node: call the Discovery Service to resolve agents for every\n",
    "    sub-task in the execution plan.\n",
    "    \"\"\"\n",
    "    logger.info(\"=== DISCOVERY NODE ===\")\n",
    "    plan: ExecutionPlan = state[\"execution_plan\"]\n",
    "\n",
    "    if plan is None:\n",
    "        return {\n",
    "            \"error\": \"No execution plan available\",\n",
    "            \"current_phase\": \"error\",\n",
    "            \"messages\": [AIMessage(content=\"Discovery skipped — no plan.\")],\n",
    "        }\n",
    "\n",
    "    agents = await discovery_client.discover_agents()\n",
    "    task_agent_map: Dict[str, AgentDescriptor] = {}\n",
    "    unmatched_tasks: List[str] = []\n",
    "\n",
    "    for task in plan.sub_tasks:\n",
    "        candidates = [\n",
    "            a for a in agents if task.required_capability in a.capabilities\n",
    "        ]\n",
    "        candidates.sort(key=lambda a: a.priority, reverse=True)\n",
    "\n",
    "        if not candidates:\n",
    "            logger.warning(\"No agent for capability=%s (task=%s)\", task.required_capability, task.task_id)\n",
    "            unmatched_tasks.append(task.task_id)\n",
    "            continue\n",
    "\n",
    "        # Health-check the top candidate; fall back to next if unhealthy\n",
    "        assigned = False\n",
    "        for candidate in candidates:\n",
    "            healthy = await discovery_client.health_check(candidate)\n",
    "            if healthy:\n",
    "                task_agent_map[task.task_id] = candidate\n",
    "                logger.info(\"Task %s -> %s (%s)\", task.task_id, candidate.name, candidate.streaming_mode)\n",
    "                assigned = True\n",
    "                break\n",
    "            else:\n",
    "                logger.warning(\"Agent %s unhealthy, trying next\", candidate.agent_id)\n",
    "\n",
    "        if not assigned:\n",
    "            unmatched_tasks.append(task.task_id)\n",
    "\n",
    "    if unmatched_tasks:\n",
    "        logger.warning(\"Unmatched tasks: %s\", unmatched_tasks)\n",
    "\n",
    "    return {\n",
    "        \"available_agents\": agents,\n",
    "        \"task_agent_map\": task_agent_map,\n",
    "        \"current_phase\": \"execution\",\n",
    "        \"messages\": [\n",
    "            AIMessage(\n",
    "                content=(\n",
    "                    f\"Discovery complete: {len(task_agent_map)} tasks matched, \"\n",
    "                    f\"{len(unmatched_tasks)} unmatched.\"\n",
    "                )\n",
    "            )\n",
    "        ],\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Discovery node defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6c. Execution Node\n",
    "\n",
    "The execution node dispatches sub-tasks to worker agents in dependency order. For independent tasks it fans out concurrently using `asyncio.gather`. The streaming multiplexer handles the actual HTTP/SSE communication and normalizes all output into `StreamEvent` objects.\n",
    "\n",
    "> **Recommendation:** For production workloads, replace the simple dependency-order loop with a proper DAG executor (topological sort + parallel dispatch per level). This notebook uses a simplified two-pass approach for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def _execute_single_task(\n",
    "    task: SubTask,\n",
    "    agent: AgentDescriptor,\n",
    "    prior_results: Dict[str, WorkerResult],\n",
    ") -> WorkerResult:\n",
    "    \"\"\"\n",
    "    Execute one sub-task against its assigned worker agent.\n",
    "    Collects all stream events and assembles them into a WorkerResult.\n",
    "    \"\"\"\n",
    "    import time\n",
    "\n",
    "    payload = {\n",
    "        **task.input_data,\n",
    "        \"task_id\": task.task_id,\n",
    "        \"description\": task.description,\n",
    "    }\n",
    "\n",
    "    # Inject outputs from upstream tasks this task depends on\n",
    "    if task.depends_on:\n",
    "        payload[\"context\"] = {\n",
    "            dep_id: prior_results[dep_id].response\n",
    "            for dep_id in task.depends_on\n",
    "            if dep_id in prior_results\n",
    "        }\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    collected_events: List[Dict[str, Any]] = []\n",
    "    full_response_parts: List[str] = []\n",
    "\n",
    "    try:\n",
    "        async for event in streaming_mux.stream(agent, task.task_id, payload):\n",
    "            collected_events.append(event.model_dump())\n",
    "            if event.event_type in (\"token\", \"message\"):\n",
    "                full_response_parts.append(event.data)\n",
    "\n",
    "        elapsed = (time.perf_counter() - t0) * 1000\n",
    "        return WorkerResult(\n",
    "            agent_id=agent.agent_id,\n",
    "            task_id=task.task_id,\n",
    "            success=True,\n",
    "            response=\"\".join(full_response_parts),\n",
    "            streaming_mode=agent.streaming_mode,\n",
    "            events=collected_events,\n",
    "            latency_ms=round(elapsed, 1),\n",
    "        )\n",
    "\n",
    "    except Exception as exc:\n",
    "        elapsed = (time.perf_counter() - t0) * 1000\n",
    "        logger.error(\"Task %s failed: %s\", task.task_id, exc)\n",
    "        return WorkerResult(\n",
    "            agent_id=agent.agent_id,\n",
    "            task_id=task.task_id,\n",
    "            success=False,\n",
    "            error=str(exc),\n",
    "            latency_ms=round(elapsed, 1),\n",
    "        )\n",
    "\n",
    "\n",
    "async def execution_node(state: OrchestratorState) -> dict:\n",
    "    \"\"\"\n",
    "    LangGraph node: execute all sub-tasks via worker agents.\n",
    "\n",
    "    Execution strategy:\n",
    "      Pass 1 — tasks with no dependencies (fan-out with asyncio.gather)\n",
    "      Pass 2 — tasks that depend on Pass-1 results (sequential for simplicity)\n",
    "    \"\"\"\n",
    "    logger.info(\"=== EXECUTION NODE ===\")\n",
    "    plan: ExecutionPlan = state[\"execution_plan\"]\n",
    "    task_agent_map: Dict[str, AgentDescriptor] = state[\"task_agent_map\"]\n",
    "\n",
    "    if not plan or not task_agent_map:\n",
    "        return {\n",
    "            \"error\": \"Nothing to execute\",\n",
    "            \"current_phase\": \"error\",\n",
    "            \"messages\": [AIMessage(content=\"Execution skipped — no plan or agents.\")],\n",
    "        }\n",
    "\n",
    "    results_by_id: Dict[str, WorkerResult] = {}\n",
    "    all_results: List[WorkerResult] = []\n",
    "    all_events: List[StreamEvent] = []\n",
    "\n",
    "    independent = [t for t in plan.sub_tasks if not t.depends_on and t.task_id in task_agent_map]\n",
    "    dependent = [t for t in plan.sub_tasks if t.depends_on and t.task_id in task_agent_map]\n",
    "\n",
    "    # --- Pass 1: fan-out independent tasks concurrently ---\n",
    "    if independent:\n",
    "        logger.info(\"Pass 1: executing %d independent tasks concurrently\", len(independent))\n",
    "        coros = [\n",
    "            _execute_single_task(task, task_agent_map[task.task_id], results_by_id)\n",
    "            for task in independent\n",
    "        ]\n",
    "        pass1_results = await asyncio.gather(*coros)\n",
    "        for r in pass1_results:\n",
    "            results_by_id[r.task_id] = r\n",
    "            all_results.append(r)\n",
    "\n",
    "    # --- Pass 2: dependent tasks sequentially ---\n",
    "    for task in dependent:\n",
    "        logger.info(\"Pass 2: executing dependent task %s\", task.task_id)\n",
    "        r = await _execute_single_task(task, task_agent_map[task.task_id], results_by_id)\n",
    "        results_by_id[r.task_id] = r\n",
    "        all_results.append(r)\n",
    "\n",
    "    for r in all_results:\n",
    "        for evt_dict in r.events:\n",
    "            all_events.append(StreamEvent(**evt_dict))\n",
    "\n",
    "    successes = sum(1 for r in all_results if r.success)\n",
    "    failures = len(all_results) - successes\n",
    "    logger.info(\"Execution done: %d succeeded, %d failed\", successes, failures)\n",
    "\n",
    "    return {\n",
    "        \"worker_results\": all_results,\n",
    "        \"stream_buffer\": [e.model_dump() for e in all_events],\n",
    "        \"current_phase\": \"synthesis\",\n",
    "        \"messages\": [\n",
    "            AIMessage(\n",
    "                content=f\"Execution complete: {successes} succeeded, {failures} failed.\"\n",
    "            )\n",
    "        ],\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Execution node defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6d. Response Synthesis Node\n",
    "\n",
    "After all workers have returned their results, the synthesis node uses the orchestrator's LLM to weave the individual outputs into a single, coherent response.\n",
    "\n",
    "> **Recommendation:** For latency-sensitive applications, consider streaming the synthesis LLM call itself so the user sees tokens arriving while synthesis is in progress — a \"stream of streams\" pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYNTHESIS_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are the Response Synthesis Agent.\n",
    "\n",
    "You receive the outputs of multiple worker agents that each handled a sub-task\n",
    "of the user's original query. Your job:\n",
    "1. Merge the outputs into a single coherent response.\n",
    "2. Resolve any contradictions by preferring higher-confidence answers.\n",
    "3. Maintain the user's original intent and tone.\n",
    "4. If any sub-task failed, acknowledge it gracefully and provide\n",
    "   the best partial answer you can.\n",
    "\n",
    "Do NOT mention internal task IDs, agent names, or infrastructure details.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "async def synthesis_node(state: OrchestratorState) -> dict:\n",
    "    \"\"\"\n",
    "    LangGraph node: synthesize all worker results into one final answer.\n",
    "    \"\"\"\n",
    "    logger.info(\"=== SYNTHESIS NODE ===\")\n",
    "    results: List[WorkerResult] = state[\"worker_results\"]\n",
    "    user_query = state[\"user_query\"]\n",
    "\n",
    "    if not results:\n",
    "        return {\n",
    "            \"final_response\": \"No results to synthesize.\",\n",
    "            \"current_phase\": \"done\",\n",
    "            \"messages\": [AIMessage(content=\"No worker results available.\")],\n",
    "        }\n",
    "\n",
    "    result_sections = []\n",
    "    for r in results:\n",
    "        status = \"SUCCESS\" if r.success else f\"FAILED ({r.error})\"\n",
    "        section = (\n",
    "            f\"--- Sub-task {r.task_id} [{status}] \"\n",
    "            f\"(agent={r.agent_id}, mode={r.streaming_mode.value}, \"\n",
    "            f\"latency={r.latency_ms:.0f}ms) ---\\n\"\n",
    "            f\"{r.response if r.success else '[no output]'}\\n\"\n",
    "        )\n",
    "        result_sections.append(section)\n",
    "\n",
    "    context = \"\\n\".join(result_sections)\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(content=SYNTHESIS_SYSTEM_PROMPT),\n",
    "        HumanMessage(\n",
    "            content=(\n",
    "                f\"Original user query:\\n{user_query}\\n\\n\"\n",
    "                f\"Worker results:\\n{context}\\n\\n\"\n",
    "                \"Synthesize these into a single, helpful response.\"\n",
    "            )\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    resp = await llm.ainvoke(messages)\n",
    "    final = resp.content.strip()\n",
    "    logger.info(\"Synthesis complete (%d chars)\", len(final))\n",
    "\n",
    "    return {\n",
    "        \"final_response\": final,\n",
    "        \"current_phase\": \"done\",\n",
    "        \"messages\": [AIMessage(content=final)],\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Synthesis node defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Handling Node\n",
    "\n",
    "A dedicated error-handling node catches failures at any stage and decides whether to retry, re-plan, or gracefully terminate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_RETRIES = 2\n",
    "\n",
    "\n",
    "async def error_handler_node(state: OrchestratorState) -> dict:\n",
    "    \"\"\"\n",
    "    LangGraph node: handle errors from any upstream node.\n",
    "    Simple strategy: log, attach an error message, and move to done.\n",
    "    \"\"\"\n",
    "    logger.error(\"=== ERROR HANDLER === error: %s\", state.get(\"error\"))\n",
    "    error_msg = state.get(\"error\", \"Unknown error\")\n",
    "\n",
    "    return {\n",
    "        \"final_response\": f\"The orchestrator encountered an error: {error_msg}\",\n",
    "        \"current_phase\": \"done\",\n",
    "        \"messages\": [\n",
    "            AIMessage(content=f\"I encountered an issue while processing your request: {error_msg}\")\n",
    "        ],\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Error handler node defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. LangGraph — Wiring the Orchestrator Graph\n",
    "\n",
    "Here we assemble the full graph. Conditional edges route to the error handler when any node sets `current_phase = \"error\"`.\n",
    "\n",
    "```\n",
    "            +----------+\n",
    "            | planning |\n",
    "            +----+-----+\n",
    "                 |\n",
    "          +------v------+\n",
    "          |  discovery  |\n",
    "          +------+------+\n",
    "                 |\n",
    "          +------v------+\n",
    "          |  execution  |\n",
    "          +------+------+\n",
    "                 |\n",
    "          +------v------+\n",
    "          |  synthesis  |\n",
    "          +------+------+\n",
    "                 |\n",
    "               (END)\n",
    "\n",
    "     Any node -> error_handler -> (END)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_after_planning(state: OrchestratorState) -> str:\n",
    "    if state.get(\"error\"):\n",
    "        return \"error_handler\"\n",
    "    return \"discovery\"\n",
    "\n",
    "\n",
    "def route_after_discovery(state: OrchestratorState) -> str:\n",
    "    if state.get(\"error\"):\n",
    "        return \"error_handler\"\n",
    "    return \"execution\"\n",
    "\n",
    "\n",
    "def route_after_execution(state: OrchestratorState) -> str:\n",
    "    if state.get(\"error\"):\n",
    "        return \"error_handler\"\n",
    "    return \"synthesis\"\n",
    "\n",
    "\n",
    "def route_after_synthesis(state: OrchestratorState) -> str:\n",
    "    return END\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Build the graph\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "workflow = StateGraph(OrchestratorState)\n",
    "\n",
    "workflow.add_node(\"planning\", planning_node)\n",
    "workflow.add_node(\"discovery\", discovery_node)\n",
    "workflow.add_node(\"execution\", execution_node)\n",
    "workflow.add_node(\"synthesis\", synthesis_node)\n",
    "workflow.add_node(\"error_handler\", error_handler_node)\n",
    "\n",
    "workflow.set_entry_point(\"planning\")\n",
    "\n",
    "workflow.add_conditional_edges(\"planning\", route_after_planning, {\n",
    "    \"discovery\": \"discovery\",\n",
    "    \"error_handler\": \"error_handler\",\n",
    "})\n",
    "\n",
    "workflow.add_conditional_edges(\"discovery\", route_after_discovery, {\n",
    "    \"execution\": \"execution\",\n",
    "    \"error_handler\": \"error_handler\",\n",
    "})\n",
    "\n",
    "workflow.add_conditional_edges(\"execution\", route_after_execution, {\n",
    "    \"synthesis\": \"synthesis\",\n",
    "    \"error_handler\": \"error_handler\",\n",
    "})\n",
    "\n",
    "workflow.add_conditional_edges(\"synthesis\", route_after_synthesis, {\n",
    "    END: END,\n",
    "})\n",
    "\n",
    "workflow.add_edge(\"error_handler\", END)\n",
    "\n",
    "orchestrator_graph = workflow.compile()\n",
    "\n",
    "print(\"LangGraph orchestrator compiled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Streaming Pass-Through Wrapper\n",
    "\n",
    "This is the top-level API the caller (e.g. a FastAPI endpoint) uses. It runs the LangGraph orchestrator and **streams events back in real time** — forwarding dynamic worker streams token-by-token and wrapping static responses into the same event protocol.\n",
    "\n",
    "> **Recommendation:** In a real deployment, expose this as an SSE endpoint via FastAPI's `StreamingResponse`. The caller receives a uniform stream of `StreamEvent` JSON lines regardless of which workers are dynamic vs. static."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_orchestrator(query: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Non-streaming entry point — runs the full graph to completion\n",
    "    and returns the final state.\n",
    "    \"\"\"\n",
    "    initial = make_initial_state(query)\n",
    "    final_state = await orchestrator_graph.ainvoke(initial)\n",
    "    return final_state\n",
    "\n",
    "\n",
    "async def run_orchestrator_streaming(query: str) -> AsyncGenerator[StreamEvent, None]:\n",
    "    \"\"\"\n",
    "    Streaming entry point — yields StreamEvent objects as the orchestrator\n",
    "    progresses through planning -> discovery -> execution -> synthesis.\n",
    "\n",
    "    Uses LangGraph's astream_events to intercept node transitions\n",
    "    and forward worker stream events in real time.\n",
    "    \"\"\"\n",
    "    initial = make_initial_state(query)\n",
    "\n",
    "    yield StreamEvent(\n",
    "        agent_id=\"orchestrator\",\n",
    "        task_id=\"root\",\n",
    "        event_type=\"status\",\n",
    "        data=\"Starting orchestration...\",\n",
    "    )\n",
    "\n",
    "    async for graph_event in orchestrator_graph.astream_events(initial, version=\"v2\"):\n",
    "        event_kind = graph_event.get(\"event\", \"\")\n",
    "        event_name = graph_event.get(\"name\", \"\")\n",
    "\n",
    "        if event_kind == \"on_chain_start\" and event_name in (\n",
    "            \"planning\", \"discovery\", \"execution\", \"synthesis\", \"error_handler\"\n",
    "        ):\n",
    "            yield StreamEvent(\n",
    "                agent_id=\"orchestrator\",\n",
    "                task_id=\"root\",\n",
    "                event_type=\"status\",\n",
    "                data=f\"Entering {event_name} phase...\",\n",
    "            )\n",
    "\n",
    "        # When the execution node completes, forward all buffered stream events\n",
    "        if event_kind == \"on_chain_end\" and event_name == \"execution\":\n",
    "            output = graph_event.get(\"data\", {}).get(\"output\", {})\n",
    "            buffered = output.get(\"stream_buffer\", [])\n",
    "            for evt_data in buffered:\n",
    "                if isinstance(evt_data, dict):\n",
    "                    yield StreamEvent(**evt_data)\n",
    "\n",
    "        # When synthesis completes, yield the final answer\n",
    "        if event_kind == \"on_chain_end\" and event_name == \"synthesis\":\n",
    "            output = graph_event.get(\"data\", {}).get(\"output\", {})\n",
    "            final = output.get(\"final_response\", \"\")\n",
    "            if final:\n",
    "                yield StreamEvent(\n",
    "                    agent_id=\"orchestrator\",\n",
    "                    task_id=\"root\",\n",
    "                    event_type=\"message\",\n",
    "                    data=final,\n",
    "                )\n",
    "\n",
    "    yield StreamEvent(\n",
    "        agent_id=\"orchestrator\",\n",
    "        task_id=\"root\",\n",
    "        event_type=\"done\",\n",
    "        data=\"\",\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Orchestrator wrappers defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Mock Discovery Service & Workers (for local testing)\n",
    "\n",
    "To run this notebook end-to-end without deploying real microservices, we stand up lightweight mock servers using `asyncio` and a simple dict-based registry.\n",
    "\n",
    "These mocks simulate:\n",
    "- A **discovery service** that returns agent descriptors\n",
    "- **Dynamic-streaming workers** that emit SSE token streams\n",
    "- **Static workers** that return hardcoded JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOCK_AGENTS = [\n",
    "    AgentDescriptor(\n",
    "        agent_id=\"writer-01\",\n",
    "        name=\"Creative Writer\",\n",
    "        description=\"Generates creative text, stories, and marketing copy\",\n",
    "        endpoint=\"http://localhost:8001\",\n",
    "        capabilities=[AgentCapability.TEXT_GENERATION],\n",
    "        streaming_mode=StreamingMode.DYNAMIC_EVENTS,\n",
    "        payload_schema=PayloadSchema(\n",
    "            required_fields={\"query\": \"string\"},\n",
    "            example={\"query\": \"Write a haiku about AI\"},\n",
    "        ),\n",
    "        priority=10,\n",
    "        timeout_seconds=60,\n",
    "    ),\n",
    "    AgentDescriptor(\n",
    "        agent_id=\"coder-01\",\n",
    "        name=\"Code Assistant\",\n",
    "        description=\"Generates and explains code\",\n",
    "        endpoint=\"http://localhost:8002\",\n",
    "        capabilities=[AgentCapability.CODE_GENERATION],\n",
    "        streaming_mode=StreamingMode.DYNAMIC_MESSAGES,\n",
    "        payload_schema=PayloadSchema(\n",
    "            required_fields={\"query\": \"string\", \"language\": \"string\"},\n",
    "            example={\"query\": \"Fibonacci function\", \"language\": \"python\"},\n",
    "        ),\n",
    "        priority=10,\n",
    "    ),\n",
    "    AgentDescriptor(\n",
    "        agent_id=\"lookup-01\",\n",
    "        name=\"Knowledge Lookup\",\n",
    "        description=\"Returns pre-indexed factual answers\",\n",
    "        endpoint=\"http://localhost:8003\",\n",
    "        capabilities=[AgentCapability.SEARCH],\n",
    "        streaming_mode=StreamingMode.STATIC,\n",
    "        static_response=\"Paris is the capital of France. Population: ~2.1 million (city), ~12 million (metro).\",\n",
    "        priority=5,\n",
    "    ),\n",
    "    AgentDescriptor(\n",
    "        agent_id=\"summarizer-01\",\n",
    "        name=\"Summarizer\",\n",
    "        description=\"Summarizes long text into concise bullet points\",\n",
    "        endpoint=\"http://localhost:8004\",\n",
    "        capabilities=[AgentCapability.SUMMARIZATION],\n",
    "        streaming_mode=StreamingMode.STATIC,\n",
    "        static_response=\"Point 1: Key insight extracted.\\nPoint 2: Supporting detail.\\nPoint 3: Conclusion.\",\n",
    "        priority=8,\n",
    "    ),\n",
    "    AgentDescriptor(\n",
    "        agent_id=\"translator-01\",\n",
    "        name=\"Translator\",\n",
    "        description=\"Translates text between languages using an LLM\",\n",
    "        endpoint=\"http://localhost:8005\",\n",
    "        capabilities=[AgentCapability.TRANSLATION],\n",
    "        streaming_mode=StreamingMode.DYNAMIC_EVENTS,\n",
    "        payload_schema=PayloadSchema(\n",
    "            required_fields={\"text\": \"string\", \"target_language\": \"string\"},\n",
    "            example={\"text\": \"Hello world\", \"target_language\": \"Spanish\"},\n",
    "        ),\n",
    "        priority=7,\n",
    "    ),\n",
    "    AgentDescriptor(\n",
    "        agent_id=\"analyst-01\",\n",
    "        name=\"Data Analyst\",\n",
    "        description=\"Performs data analysis and generates insights\",\n",
    "        endpoint=\"http://localhost:8006\",\n",
    "        capabilities=[AgentCapability.DATA_ANALYSIS],\n",
    "        streaming_mode=StreamingMode.DYNAMIC_MESSAGES,\n",
    "        payload_schema=PayloadSchema(\n",
    "            required_fields={\"query\": \"string\"},\n",
    "            example={\"query\": \"Analyze sales trends\"},\n",
    "        ),\n",
    "        priority=9,\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"Mock registry loaded with {len(MOCK_AGENTS)} agents:\")\n",
    "for a in MOCK_AGENTS:\n",
    "    print(f\"  - {a.name:20s}  mode={a.streaming_mode.value:20s}  capabilities={[c.value for c in a.capabilities]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mock HTTP Servers\n",
    "\n",
    "We use `aiohttp` to spin up lightweight servers that simulate the discovery service and individual workers. In a real deployment these would be separate Docker containers / Kubernetes pods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiohttp import web\n",
    "\n",
    "\n",
    "async def handle_discovery(request: web.Request) -> web.Response:\n",
    "    \"\"\"GET /agents -> return all registered mock agents.\"\"\"\n",
    "    agents_data = [a.model_dump() for a in MOCK_AGENTS]\n",
    "    return web.json_response({\"agents\": agents_data})\n",
    "\n",
    "\n",
    "async def handle_agent_health(request: web.Request) -> web.Response:\n",
    "    \"\"\"GET /health -> always healthy in the mock.\"\"\"\n",
    "    return web.json_response({\"status\": \"ok\"})\n",
    "\n",
    "\n",
    "async def handle_dynamic_event_stream(request: web.Request) -> web.StreamResponse:\n",
    "    \"\"\"\n",
    "    POST /stream -> SSE response that emits tokens one by one.\n",
    "    Simulates a generative LLM worker.\n",
    "    \"\"\"\n",
    "    body = await request.json()\n",
    "    query = body.get(\"query\", body.get(\"description\", \"default task\"))\n",
    "\n",
    "    simulated_tokens = f\"Here is a generated response for: '{query}'. \".split()\n",
    "    simulated_tokens += \"This output is streamed token-by-token from the worker microservice.\".split()\n",
    "\n",
    "    resp = web.StreamResponse(\n",
    "        status=200,\n",
    "        headers={\"Content-Type\": \"text/event-stream\", \"Cache-Control\": \"no-cache\"},\n",
    "    )\n",
    "    await resp.prepare(request)\n",
    "\n",
    "    for token in simulated_tokens:\n",
    "        chunk = json.dumps({\"token\": token + \" \", \"type\": \"token\"})\n",
    "        await resp.write(f\"data: {chunk}\\n\\n\".encode())\n",
    "        await asyncio.sleep(0.05)\n",
    "\n",
    "    await resp.write(b'data: {\"type\": \"done\"}\\n\\n')\n",
    "    return resp\n",
    "\n",
    "\n",
    "async def handle_dynamic_message_stream(request: web.Request) -> web.StreamResponse:\n",
    "    \"\"\"\n",
    "    POST /stream -> SSE response that emits complete chat messages.\n",
    "    \"\"\"\n",
    "    body = await request.json()\n",
    "    query = body.get(\"query\", body.get(\"description\", \"default task\"))\n",
    "\n",
    "    messages_to_send = [\n",
    "        f\"Analyzing your request: '{query}'\",\n",
    "        \"Processing with the language model...\",\n",
    "        f\"Result: Here is the complete response for '{query}'. This was streamed as full messages.\",\n",
    "    ]\n",
    "\n",
    "    resp = web.StreamResponse(\n",
    "        status=200,\n",
    "        headers={\"Content-Type\": \"text/event-stream\", \"Cache-Control\": \"no-cache\"},\n",
    "    )\n",
    "    await resp.prepare(request)\n",
    "\n",
    "    for msg in messages_to_send:\n",
    "        chunk = json.dumps({\"role\": \"assistant\", \"content\": msg})\n",
    "        await resp.write(f\"data: {chunk}\\n\\n\".encode())\n",
    "        await asyncio.sleep(0.2)\n",
    "\n",
    "    await resp.write(b'data: {\"type\": \"done\"}\\n\\n')\n",
    "    return resp\n",
    "\n",
    "\n",
    "async def handle_static_invoke(request: web.Request) -> web.Response:\n",
    "    \"\"\"POST /invoke -> plain JSON response (no streaming).\"\"\"\n",
    "    body = await request.json()\n",
    "    return web.json_response({\n",
    "        \"response\": f\"Static result for task '{body.get('task_id', '?')}': \"\n",
    "                    \"This is a deterministic, hardcoded answer.\",\n",
    "    })\n",
    "\n",
    "\n",
    "_mock_servers = []\n",
    "\n",
    "\n",
    "async def start_mock_discovery(port: int = 9000):\n",
    "    app = web.Application()\n",
    "    app.router.add_get(\"/agents\", handle_discovery)\n",
    "    app.router.add_get(\"/health\", handle_agent_health)\n",
    "    runner = web.AppRunner(app)\n",
    "    await runner.setup()\n",
    "    site = web.TCPSite(runner, \"localhost\", port)\n",
    "    await site.start()\n",
    "    _mock_servers.append(runner)\n",
    "    logger.info(\"Mock Discovery Service running on :%d\", port)\n",
    "\n",
    "\n",
    "async def start_mock_worker(port: int, streaming_mode: StreamingMode):\n",
    "    app = web.Application()\n",
    "    app.router.add_get(\"/health\", handle_agent_health)\n",
    "\n",
    "    if streaming_mode == StreamingMode.DYNAMIC_EVENTS:\n",
    "        app.router.add_post(\"/stream\", handle_dynamic_event_stream)\n",
    "    elif streaming_mode == StreamingMode.DYNAMIC_MESSAGES:\n",
    "        app.router.add_post(\"/stream\", handle_dynamic_message_stream)\n",
    "    else:\n",
    "        app.router.add_post(\"/invoke\", handle_static_invoke)\n",
    "\n",
    "    runner = web.AppRunner(app)\n",
    "    await runner.setup()\n",
    "    site = web.TCPSite(runner, \"localhost\", port)\n",
    "    await site.start()\n",
    "    _mock_servers.append(runner)\n",
    "    logger.info(\"Mock Worker running on :%d  mode=%s\", port, streaming_mode.value)\n",
    "\n",
    "\n",
    "async def start_all_mocks():\n",
    "    \"\"\"Spin up the discovery service and one worker per mock agent.\"\"\"\n",
    "    await start_mock_discovery(9000)\n",
    "    for agent in MOCK_AGENTS:\n",
    "        port = int(agent.endpoint.split(\":\")[-1])\n",
    "        await start_mock_worker(port, agent.streaming_mode)\n",
    "    print(f\"All mock servers running ({1 + len(MOCK_AGENTS)} total)\")\n",
    "\n",
    "\n",
    "async def stop_all_mocks():\n",
    "    for runner in _mock_servers:\n",
    "        await runner.cleanup()\n",
    "    _mock_servers.clear()\n",
    "    print(\"All mock servers stopped\")\n",
    "\n",
    "\n",
    "print(\"Mock server helpers defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. End-to-End Demo\n",
    "\n",
    "Start the mock servers, run the orchestrator with a sample query, and observe the full pipeline in action: planning -> discovery -> execution (with streaming) -> synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start mock infrastructure\n",
    "await start_all_mocks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Demo 1: Non-streaming run (full result at the end)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "demo_query = \"Write a short poem about artificial intelligence and also look up the capital of France.\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"QUERY: {demo_query}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "result = await run_orchestrator(demo_query)\n",
    "\n",
    "print(\"\\n--- Final Response ---\")\n",
    "print(result.get(\"final_response\", \"(no response)\"))\n",
    "print(\"\\n--- Execution Plan ---\")\n",
    "plan = result.get(\"execution_plan\")\n",
    "if plan:\n",
    "    print(f\"  Plan ID: {plan.plan_id}\")\n",
    "    print(f\"  Reasoning: {plan.reasoning}\")\n",
    "    for t in plan.sub_tasks:\n",
    "        print(f\"  - {t.task_id}: {t.description} [{t.required_capability.value}]\")\n",
    "\n",
    "print(\"\\n--- Worker Results ---\")\n",
    "for wr in result.get(\"worker_results\", []):\n",
    "    status = \"OK\" if wr.success else \"FAIL\"\n",
    "    print(f\"  [{status}] {wr.task_id} (agent={wr.agent_id}, mode={wr.streaming_mode.value}, {wr.latency_ms:.0f}ms)\")\n",
    "    preview = wr.response[:120] + ('...' if len(wr.response) > 120 else '')\n",
    "    print(f\"    Response: {preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Demo 2: Streaming run (events arrive in real time)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "demo_query_2 = \"Generate a Python function for binary search and summarize its time complexity.\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"STREAMING QUERY: {demo_query_2}\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "async for event in run_orchestrator_streaming(demo_query_2):\n",
    "    prefix = f\"[{event.agent_id}/{event.task_id}]\"\n",
    "    if event.event_type == \"token\":\n",
    "        print(event.data, end=\"\", flush=True)\n",
    "    elif event.event_type == \"message\":\n",
    "        print(f\"\\n{prefix} MSG: {event.data[:200]}\")\n",
    "    elif event.event_type == \"status\":\n",
    "        print(f\"\\n{prefix} STATUS: {event.data}\")\n",
    "    elif event.event_type == \"error\":\n",
    "        print(f\"\\n{prefix} ERROR: {event.data}\")\n",
    "    elif event.event_type == \"done\":\n",
    "        print(f\"\\n{prefix} DONE\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup mock servers\n",
    "await stop_all_mocks()\n",
    "await discovery_client.close()\n",
    "await streaming_mux.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. FastAPI Integration Example\n",
    "\n",
    "Below is a reference implementation showing how to expose the orchestrator as a production-ready HTTP API with SSE streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FASTAPI_EXAMPLE = '''\n",
    "from fastapi import FastAPI, Request\n",
    "from fastapi.responses import StreamingResponse\n",
    "import json\n",
    "\n",
    "app = FastAPI(title=\"LangGraph Orchestrator API\")\n",
    "\n",
    "\n",
    "@app.post(\"/orchestrate\")\n",
    "async def orchestrate_non_streaming(request: Request):\n",
    "    \"\"\"Non-streaming: returns the full synthesized response.\"\"\"\n",
    "    body = await request.json()\n",
    "    query = body[\"query\"]\n",
    "    result = await run_orchestrator(query)\n",
    "    return {\n",
    "        \"response\": result[\"final_response\"],\n",
    "        \"plan\": result[\"execution_plan\"].model_dump() if result.get(\"execution_plan\") else None,\n",
    "        \"worker_results\": [r.model_dump() for r in result.get(\"worker_results\", [])],\n",
    "    }\n",
    "\n",
    "\n",
    "@app.post(\"/orchestrate/stream\")\n",
    "async def orchestrate_streaming(request: Request):\n",
    "    \"\"\"\n",
    "    Streaming: returns SSE events as the orchestrator executes.\n",
    "\n",
    "    Each line is:  data: {\"agent_id\": \"...\", \"event_type\": \"token|message|status|done\", \"data\": \"...\"}\n",
    "\n",
    "    The client reads this with EventSource or any SSE library.\n",
    "    \"\"\"\n",
    "    body = await request.json()\n",
    "    query = body[\"query\"]\n",
    "\n",
    "    async def event_generator():\n",
    "        async for event in run_orchestrator_streaming(query):\n",
    "            yield f\"data: {event.model_dump_json()}\\\\n\\\\n\"\n",
    "\n",
    "    return StreamingResponse(\n",
    "        event_generator(),\n",
    "        media_type=\"text/event-stream\",\n",
    "        headers={\"Cache-Control\": \"no-cache\", \"X-Accel-Buffering\": \"no\"},\n",
    "    )\n",
    "'''\n",
    "\n",
    "print(\"FastAPI integration example (reference only — not executed in notebook):\")\n",
    "print(FASTAPI_EXAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Recommendations & Design Notes\n",
    "\n",
    "### Architecture Recommendations\n",
    "\n",
    "| Area | Recommendation | Why |\n",
    "|------|----------------|-----|\n",
    "| **Discovery caching** | Cache agent descriptors with a 5-min TTL and background refresh | Avoids hammering the discovery service on every request; stale cache is acceptable for short windows |\n",
    "| **Circuit breakers** | Wrap worker calls in a circuit breaker (e.g. `aiobreaker`) | Prevents cascading failures when a worker microservice goes down |\n",
    "| **Retry with backoff** | Use exponential backoff (2x delay, max 3 retries) for transient HTTP errors | Handles network blips without overwhelming workers |\n",
    "| **Structured LLM output** | Use OpenAI function-calling / tool-use for the planner | Eliminates brittle JSON parsing from free-form text |\n",
    "| **Observability** | Add OpenTelemetry spans per node and per worker call | Critical for debugging latency in a distributed system |\n",
    "| **DAG execution** | Replace the 2-pass executor with a topological-sort DAG scheduler | Enables maximum parallelism when tasks have complex dependency trees |\n",
    "| **Streaming protocol** | Standardize on NDJSON over SSE with a `StreamEvent` schema | Uniform protocol across all consumers (web, mobile, CLI) |\n",
    "| **Idempotency** | Give each orchestration run a unique `run_id`; workers should be idempotent | Safe retries and deduplication |\n",
    "\n",
    "### Streaming Design Notes\n",
    "\n",
    "1. **Dynamic vs. Static is per-agent, not per-request.** The discovery service declares the mode. The orchestrator doesn't guess — it reads the agent descriptor.\n",
    "\n",
    "2. **Event-level streaming** (token-by-token) is best for:\n",
    "   - Real-time UX where users see text appearing\n",
    "   - Long-running generative tasks\n",
    "\n",
    "3. **Message-level streaming** (full messages) is best for:\n",
    "   - Multi-turn conversational agents\n",
    "   - Agents that produce structured intermediate steps\n",
    "\n",
    "4. **Static responses** are best for:\n",
    "   - Lookup / retrieval agents\n",
    "   - Rule-based / deterministic agents\n",
    "   - Agents that call external APIs and return a fixed-format result\n",
    "\n",
    "5. **Back-pressure:** If the client can't consume events fast enough, consider adding a bounded async queue between the streaming multiplexer and the SSE endpoint.\n",
    "\n",
    "### Security Considerations\n",
    "\n",
    "- **mTLS between orchestrator and workers** — microservices should authenticate each other\n",
    "- **API key rotation** via the discovery service metadata\n",
    "- **Rate limiting** at the orchestrator level to prevent abuse\n",
    "- **Input validation** — the planner's output should be schema-validated before dispatching to workers\n",
    "\n",
    "### Scalability Path\n",
    "\n",
    "```\n",
    "Phase 1: Single orchestrator process, mock workers (this notebook)\n",
    "Phase 2: Docker Compose with real workers, shared Redis for state\n",
    "Phase 3: Kubernetes with auto-scaling workers, LangGraph Cloud for orchestration\n",
    "Phase 4: Multi-region with geo-routed discovery service\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Dependencies\n",
    "\n",
    "Install the required packages:\n",
    "\n",
    "```bash\n",
    "pip install langchain-openai langgraph langchain-core pydantic httpx aiohttp\n",
    "```\n",
    "\n",
    "### Version Matrix (tested)\n",
    "\n",
    "| Package | Version |\n",
    "|---------|---------|\n",
    "| langchain-openai | >= 0.1.0 |\n",
    "| langgraph | >= 0.2.0 |\n",
    "| langchain-core | >= 0.2.0 |\n",
    "| pydantic | >= 2.0 |\n",
    "| httpx | >= 0.27 |\n",
    "| aiohttp | >= 3.9 |\n",
    "\n",
    "---\n",
    "\n",
    "*End of notebook.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
