{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IozQ4kTuRwe_",
        "outputId": "635fb021-a627-42d9-da28-80c9a416225a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.52)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.23 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.23)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.31)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.19.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.23->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.23->langchain-community) (2.11.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain-community) (2.33.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.21-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.21 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.9.1 python-dotenv-1.1.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from jinja2 import Template\n",
        "from typing import List, Dict\n",
        "import json\n",
        "from pathlib import Path\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "sPLoVXn1SLVQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PromptBuilderAgent: Modular Prompt Generator\n",
        "\n",
        "from typing import List, Dict\n",
        "from jinja2 import Template\n",
        "\n",
        "# ----------------------------\n",
        "# AGENT CLASS\n",
        "# ----------------------------\n",
        "class PromptBuilderAgent:\n",
        "    def __init__(self):\n",
        "        self.context_template = \"\"\"### DOCUMENT CONTEXT\n",
        "The content below was extracted from a PDF. It may be machine-readable or OCR-processed.\n",
        "If OCR was used and accuracy is low, prefer fallback strategies like synonym search, pattern anchoring, or structured table recovery.\n",
        "\n",
        "[START OF DOCUMENT CONTENT]\n",
        "{{ document_content }}\n",
        "[END OF DOCUMENT CONTENT]\n",
        "\"\"\"\n",
        "\n",
        "        self.few_shot_examples = \"\"\"### FEW-SHOT EXAMPLES\n",
        "\n",
        "#### Input Fields\n",
        "- Commencement Date: The date when the lease becomes active\n",
        "- Owner Name: The legal name of the landlord\n",
        "\n",
        "#### Ideal Output\n",
        "```json\n",
        "{\n",
        "  \"Commencement Date\": {\n",
        "    \"value\": \"July 1, 2020\",\n",
        "    \"reasoning\": \"Located under section 'Lease Commencement'; clearly a valid date.\",\n",
        "    \"confidence\": 0.95\n",
        "  },\n",
        "  \"Owner Name\": {\n",
        "    \"value\": null,\n",
        "    \"reasoning\": \"The term 'Owner' was not found. Related terms like 'Lessor' were present but ambiguous.\",\n",
        "    \"confidence\": 0.4\n",
        "  }\n",
        "}\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "        self.output_constraints = \"\"\"### OUTPUT FORMAT & GUARDRAILS\n",
        "\n",
        "- Output must be a valid **JSON** object.\n",
        "- Each key must match a field name exactly.\n",
        "- Each value must be a nested object with:\n",
        "  - \"value\": string or null\n",
        "  - \"reasoning\": string explanation of the extraction or failure\n",
        "  - \"confidence\": a float between 0 and 1\n",
        "- Do not hallucinate or fabricate values.\n",
        "- Only extract information grounded in the document context.\n",
        "- If OCR issues occur, state so explicitly in the reasoning.\n",
        "\"\"\"\n",
        "\n",
        "    def generate_system_role(self, filters: Dict[str, str]) -> str:\n",
        "        return f\"\"\"\n",
        "You are a domain-specific document intelligence system specialized in analyzing legal documents in the **{filters['Module Name']}** module.\n",
        "You operate primarily in the **{filters['Territory']}** territory, focusing on **{filters['Contract Type']}** documents such as **{filters['Document Type']}**s.\n",
        "You are assigned to review documents for the owner **{filters['Owner']}**, within the **{filters['Market']}** market, specifically the **{filters['SubMarket']}** submarket in **{filters['Local Market']}**.\n",
        "These documents pertain to **{filters['Facility Type']}** facilities and can be machine-readable or OCR-scanned.\n",
        "\"\"\"\n",
        "\n",
        "    def generate_objective(self, fields: List[Dict[str, str]]) -> str:\n",
        "        objective = [\"For each of the following fields, your job is to:\",\n",
        "                    \"1. Check whether the field is mentioned in the document.\",\n",
        "                    \"2. If found, extract the value exactly as written.\",\n",
        "                    \"3. If not found, return null and explain why (e.g., not mentioned, ambiguous, OCR failure).\",\n",
        "                    \"4. Return reasoning and confidence score per field.\",\n",
        "                    \"\"]\n",
        "        for idx, field in enumerate(fields, 1):\n",
        "            desc = field.get(\"description\", \"\")\n",
        "            line = f\"{idx}. Field: '{field['name']}' - {desc}\"\n",
        "            objective.append(line)\n",
        "        return \"\\n\".join(objective)\n",
        "\n",
        "    def build_prompt(self, filters: Dict[str, str], fields: List[Dict[str, str]]) -> str:\n",
        "        role = self.generate_system_role(filters)\n",
        "        objective = self.generate_objective(fields)\n",
        "        return f\"\"\"\n",
        "//‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï SYSTEM ‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï\n",
        "{role}\n",
        "\n",
        "//‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï OBJECTIVE ‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï\n",
        "{objective}\n",
        "\n",
        "{self.context_template}\n",
        "\n",
        "{self.few_shot_examples}\n",
        "\n",
        "{self.output_constraints}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "nLI8Fuy8SSIb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Example Usage\n",
        "\n",
        "filters_example = {\n",
        "    \"Module Name\": \"Real estate\",\n",
        "    \"Territory\": \"South\",\n",
        "    \"Owner\": \"Crown\",\n",
        "    \"Market\": \"South East\",\n",
        "    \"Contract Type\": \"Lease\",\n",
        "    \"SubMarket\": \"Florida\",\n",
        "    \"Document Type\": \"Lease\",\n",
        "    \"Local Market\": \"Tampa\",\n",
        "    \"Facility Type\": \"Easement\"\n",
        "}\n",
        "\n",
        "fields_example = [\n",
        "    {\"name\": \"Commencement Date\", \"description\": \"The start date of the lease term.\"},\n",
        "    {\"name\": \"Site ID\", \"description\": \"The internal or regulatory identifier for the facility.\"},\n",
        "    {\"name\": \"Owner Name\", \"description\": \"Legal name of the property owner or landlord.\"}\n",
        "]\n",
        "\n",
        "agent = PromptBuilderAgent()\n",
        "prompt_text = agent.build_prompt(filters_example, fields_example)\n",
        "print(prompt_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VAtzUCuU8Vf",
        "outputId": "8cd173de-8395-4d7c-eed3-b2d6d541ef67"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "//‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï SYSTEM ‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï\n",
            "\n",
            "You are a domain-specific document intelligence system specialized in analyzing legal documents in the **Real estate** module.\n",
            "You operate primarily in the **South** territory, focusing on **Lease** documents such as **Lease**s.\n",
            "You are assigned to review documents for the owner **Crown**, within the **South East** market, specifically the **Florida** submarket in **Tampa**.\n",
            "These documents pertain to **Easement** facilities and can be machine-readable or OCR-scanned.\n",
            "\n",
            "\n",
            "//‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï OBJECTIVE ‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï\n",
            "For each of the following fields, your job is to:\n",
            "1. Check whether the field is mentioned in the document.\n",
            "2. If found, extract the value exactly as written.\n",
            "3. If not found, return null and explain why (e.g., not mentioned, ambiguous, OCR failure).\n",
            "4. Return reasoning and confidence score per field.\n",
            "\n",
            "1. Field: 'Commencement Date' - The start date of the lease term.\n",
            "2. Field: 'Site ID' - The internal or regulatory identifier for the facility.\n",
            "3. Field: 'Owner Name' - Legal name of the property owner or landlord.\n",
            "\n",
            "### DOCUMENT CONTEXT\n",
            "The content below was extracted from a PDF. It may be machine-readable or OCR-processed.\n",
            "If OCR was used and accuracy is low, prefer fallback strategies like synonym search, pattern anchoring, or structured table recovery.\n",
            "\n",
            "[START OF DOCUMENT CONTENT]\n",
            "{{ document_content }}\n",
            "[END OF DOCUMENT CONTENT]\n",
            "\n",
            "\n",
            "### FEW-SHOT EXAMPLES\n",
            "\n",
            "#### Input Fields\n",
            "- Commencement Date: The date when the lease becomes active\n",
            "- Owner Name: The legal name of the landlord\n",
            "\n",
            "#### Ideal Output\n",
            "```json\n",
            "{\n",
            "  \"Commencement Date\": {\n",
            "    \"value\": \"July 1, 2020\",\n",
            "    \"reasoning\": \"Located under section 'Lease Commencement'; clearly a valid date.\",\n",
            "    \"confidence\": 0.95\n",
            "  },\n",
            "  \"Owner Name\": {\n",
            "    \"value\": null,\n",
            "    \"reasoning\": \"The term 'Owner' was not found. Related terms like 'Lessor' were present but ambiguous.\",\n",
            "    \"confidence\": 0.4\n",
            "  }\n",
            "}\n",
            "```\n",
            "\n",
            "\n",
            "### OUTPUT FORMAT & GUARDRAILS\n",
            "\n",
            "- Output must be a valid **JSON** object.\n",
            "- Each key must match a field name exactly.\n",
            "- Each value must be a nested object with:\n",
            "  - \"value\": string or null\n",
            "  - \"reasoning\": string explanation of the extraction or failure\n",
            "  - \"confidence\": a float between 0 and 1\n",
            "- Do not hallucinate or fabricate values.\n",
            "- Only extract information grounded in the document context.\n",
            "- If OCR issues occur, state so explicitly in the reasoning.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " A[PDF Upload] --> B[OCR Module]\n",
        "    B --> C[Cleaned Text]\n",
        "    C --> D[PromptBuilderAgent (LLM)]\n",
        "    D --> E[Generated Prompt (Validated)]\n",
        "    E --> F[LLM Extractor Agent (e.g., GPT-4)]\n",
        "    F --> G[Structured JSON Output with Reasoning & Confidence]"
      ],
      "metadata": {
        "id": "use6eoVjxxj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Full Document Intelligence Pipeline with OCR Fallback, PDF Support, Audit Logging, and Pydantic Validation\n",
        "\n",
        "from typing import List, Dict, Optional\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "import pytesseract\n",
        "from pdf2image import convert_from_path\n",
        "import fitz  # PyMuPDF\n",
        "from PIL import Image\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "from pydantic import BaseModel, ValidationError, Field\n",
        "from datetime import datetime\n",
        "\n",
        "# ----------------------------\n",
        "# PromptBuilderAgent (LLM-generated prompt)\n",
        "# ----------------------------\n",
        "class PromptBuilderAgent:\n",
        "    def __init__(self, model_name: str = \"gpt-4\", temperature: float = 0.3):\n",
        "        self.llm = OpenAI(model=model_name, temperature=temperature)\n",
        "        self.prompt_template = PromptTemplate.from_template(\"\"\"\n",
        "You are a prompt generation agent for legal document analysis.\n",
        "\n",
        "Given the metadata:\n",
        "- Module: {module_name}\n",
        "- Document Type: {document_type}\n",
        "- Facility Type: {facility_type}\n",
        "- Market/Submarket: {market} / {submarket}\n",
        "- Local Market: {local_market}\n",
        "- Owner: {owner}\n",
        "- OCR Mode: {has_ocr}\n",
        "\n",
        "And Fields:\n",
        "{field_string}\n",
        "\n",
        "Generate a production-grade prompt that includes:\n",
        "1. Role declaration\n",
        "2. Step-by-step objective section (per field)\n",
        "3. Placeholder for document content\n",
        "4. At least one realistic few-shot example\n",
        "5. Output JSON schema with value, reasoning, confidence\n",
        "6. Guardrails to prevent hallucination or guessing\n",
        "7. OCR handling instructions if text is noisy\n",
        "\"\"\")\n",
        "        self.chain = LLMChain(prompt=self.prompt_template, llm=self.llm)\n",
        "\n",
        "    def build_prompt(self, filters: Dict[str, str], fields: List[Dict[str, str]], has_ocr: bool = True) -> str:\n",
        "        field_str = \"\\n\".join([f\"- {f['name']}: {f.get('description', '')}\" for f in fields])\n",
        "        return self.chain.run(\n",
        "            module_name=filters.get(\"Module Name\", \"\"),\n",
        "            document_type=filters.get(\"Document Type\", \"\"),\n",
        "            facility_type=filters.get(\"Facility Type\", \"\"),\n",
        "            market=filters.get(\"Market\", \"\"),\n",
        "            submarket=filters.get(\"SubMarket\", \"\"),\n",
        "            local_market=filters.get(\"Local Market\", \"\"),\n",
        "            owner=filters.get(\"Owner\", \"\"),\n",
        "            has_ocr=\"Yes\" if has_ocr else \"No\",\n",
        "            field_string=field_str\n",
        "        )\n",
        "\n",
        "# ----------------------------\n",
        "# PromptValidator\n",
        "# ----------------------------\n",
        "def validate_prompt_structure(prompt: str) -> bool:\n",
        "    required_sections = [\"SYSTEM\", \"OBJECTIVE\", \"DOCUMENT CONTEXT\", \"FEW-SHOT\", \"OUTPUT FORMAT\"]\n",
        "    return all(section in prompt for section in required_sections)\n",
        "\n",
        "# ----------------------------\n",
        "# Pydantic Schema for LLM Output\n",
        "# ----------------------------\n",
        "class FieldExtraction(BaseModel):\n",
        "    value: Optional[str]\n",
        "    reasoning: str\n",
        "    confidence: float = Field(..., ge=0, le=1)\n",
        "\n",
        "class ExtractionSchema(BaseModel):\n",
        "    __root__: Dict[str, FieldExtraction]\n",
        "\n",
        "# ----------------------------\n",
        "# DocumentProcessor\n",
        "# ----------------------------\n",
        "class DocumentProcessor:\n",
        "    def __init__(self, prompt_agent: PromptBuilderAgent, extractor_model: str = \"gpt-4\"):\n",
        "        self.prompt_agent = prompt_agent\n",
        "        self.extractor_llm = OpenAI(model=extractor_model, temperature=0.2)\n",
        "\n",
        "    def extract_text_from_pdf(self, pdf_path: str) -> Dict[str, str]:\n",
        "        try:\n",
        "            doc = fitz.open(pdf_path)\n",
        "            text = \"\\n\\n\".join([page.get_text() for page in doc])\n",
        "            return {\"text\": text, \"is_ocr\": False}\n",
        "        except Exception:\n",
        "            images = convert_from_path(pdf_path)\n",
        "            full_text = []\n",
        "            for img in images:\n",
        "                ocr_result = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT)\n",
        "                for i in range(len(ocr_result['text'])):\n",
        "                    if int(ocr_result['conf'][i]) > 60:\n",
        "                        full_text.append(ocr_result['text'][i])\n",
        "            return {\"text\": \" \".join(full_text), \"is_ocr\": True}\n",
        "\n",
        "    def run_pipeline(self, pdf_path: str, filters: Dict[str, str], fields: List[Dict[str, str]]) -> Dict:\n",
        "        extracted = self.extract_text_from_pdf(pdf_path)\n",
        "        document_text = extracted[\"text\"]\n",
        "        is_ocr = extracted[\"is_ocr\"]\n",
        "\n",
        "        prompt = self.prompt_agent.build_prompt(filters, fields, has_ocr=is_ocr)\n",
        "\n",
        "        if not validate_prompt_structure(prompt):\n",
        "            raise ValueError(\"Generated prompt does not meet required structure standards.\")\n",
        "\n",
        "        final_input = f\"\"\"{prompt}\\n\\n### DOCUMENT CONTENT\\n{document_text}\\n\"\"\"\n",
        "\n",
        "        raw_output = self.extractor_llm.invoke(final_input)\n",
        "        self.log_prompt(prompt, raw_output)\n",
        "\n",
        "        try:\n",
        "            parsed_output = json.loads(raw_output)\n",
        "            validated = ExtractionSchema.parse_obj(parsed_output)\n",
        "            return validated.dict()\n",
        "        except (json.JSONDecodeError, ValidationError):\n",
        "            # Retry logic or graceful fallback\n",
        "            return {\"error\": \"Failed to validate LLM output\", \"raw\": raw_output}\n",
        "\n",
        "    def log_prompt(self, prompt: str, response: str):\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        os.makedirs(\"audit_logs\", exist_ok=True)\n",
        "        with open(f\"audit_logs/prompt_{timestamp}.txt\", \"w\") as f:\n",
        "            f.write(prompt)\n",
        "        with open(f\"audit_logs/response_{timestamp}.json\", \"w\") as f:\n",
        "            f.write(response)\n",
        "\n",
        "\n",
        "# ‚úÖ Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    filters_example = {\n",
        "        \"Module Name\": \"Real Estate\",\n",
        "        \"Territory\": \"South\",\n",
        "        \"Owner\": \"Crown\",\n",
        "        \"Market\": \"South East\",\n",
        "        \"Contract Type\": \"Lease\",\n",
        "        \"SubMarket\": \"Florida\",\n",
        "        \"Document Type\": \"Lease\",\n",
        "        \"Local Market\": \"Tampa\",\n",
        "        \"Facility Type\": \"Easement\"\n",
        "    }\n",
        "\n",
        "    fields_example = [\n",
        "        {\"name\": \"Commencement Date\", \"description\": \"The start date of the lease term.\"},\n",
        "        {\"name\": \"Site ID\", \"description\": \"Internal or regulatory identifier for the facility.\"},\n",
        "        {\"name\": \"Owner Name\", \"description\": \"Legal name of the property owner or landlord.\"}\n",
        "    ]\n",
        "\n",
        "    agent = PromptBuilderAgent()\n",
        "    processor = DocumentProcessor(prompt_agent=agent)\n",
        "    output = processor.run_pipeline(\"example_lease.pdf\", filters_example, fields_example)\n",
        "    print(json.dumps(output, indent=2))\n"
      ],
      "metadata": {
        "id": "2SBxs2hGVVbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Streamlit UI for Document Intelligence with Retry Logic\n",
        "\n",
        "import streamlit as st\n",
        "from typing import List, Dict\n",
        "from dynamic_prompt_generator import PromptBuilderAgent, DocumentProcessor\n",
        "from pydantic import ValidationError\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Set page config\n",
        "st.set_page_config(page_title=\"Lease Document Analyzer\", layout=\"wide\")\n",
        "\n",
        "# Title\n",
        "st.title(\"üìÑ Lease Document Intelligence System\")\n",
        "\n",
        "# Sidebar Inputs\n",
        "st.sidebar.header(\"Document Metadata Filters\")\n",
        "filters = {\n",
        "    \"Module Name\": st.sidebar.text_input(\"Module Name\", value=\"Real Estate\"),\n",
        "    \"Territory\": st.sidebar.text_input(\"Territory\", value=\"South\"),\n",
        "    \"Owner\": st.sidebar.text_input(\"Owner\", value=\"Crown\"),\n",
        "    \"Market\": st.sidebar.text_input(\"Market\", value=\"South East\"),\n",
        "    \"Contract Type\": st.sidebar.text_input(\"Contract Type\", value=\"Lease\"),\n",
        "    \"SubMarket\": st.sidebar.text_input(\"SubMarket\", value=\"Florida\"),\n",
        "    \"Document Type\": st.sidebar.text_input(\"Document Type\", value=\"Lease\"),\n",
        "    \"Local Market\": st.sidebar.text_input(\"Local Market\", value=\"Tampa\"),\n",
        "    \"Facility Type\": st.sidebar.text_input(\"Facility Type\", value=\"Easement\")\n",
        "}\n",
        "\n",
        "# Field Input\n",
        "st.sidebar.header(\"Fields to Extract\")\n",
        "num_fields = st.sidebar.number_input(\"Number of fields\", min_value=1, max_value=10, value=3)\n",
        "fields_input = []\n",
        "for i in range(num_fields):\n",
        "    with st.sidebar.expander(f\"Field {i+1}\"):\n",
        "        name = st.text_input(f\"Field Name {i+1}\", key=f\"fname_{i}\")\n",
        "        desc = st.text_input(f\"Description {i+1}\", key=f\"fdesc_{i}\")\n",
        "        if name:\n",
        "            fields_input.append({\"name\": name, \"description\": desc})\n",
        "\n",
        "# File Upload\n",
        "st.header(\"Upload Lease Document (PDF)\")\n",
        "uploaded_file = st.file_uploader(\"Choose a PDF file\", type=\"pdf\")\n",
        "\n",
        "# Process Button\n",
        "if uploaded_file and fields_input:\n",
        "    if st.button(\"Run Document Analysis\"):\n",
        "        with st.spinner(\"üîç Processing document and generating prompt...\"):\n",
        "            # Save to temp\n",
        "            pdf_path = os.path.join(\"temp\", uploaded_file.name)\n",
        "            os.makedirs(\"temp\", exist_ok=True)\n",
        "            with open(pdf_path, \"wb\") as f:\n",
        "                f.write(uploaded_file.read())\n",
        "\n",
        "            # Initialize engine\n",
        "            agent = PromptBuilderAgent()\n",
        "            processor = DocumentProcessor(prompt_agent=agent)\n",
        "\n",
        "            # Retry logic\n",
        "            max_attempts = 2\n",
        "            attempt = 0\n",
        "            output = {}\n",
        "            while attempt < max_attempts:\n",
        "                output = processor.run_pipeline(pdf_path, filters, fields_input)\n",
        "                if \"error\" not in output:\n",
        "                    break\n",
        "                attempt += 1\n",
        "\n",
        "            if \"error\" in output:\n",
        "                st.error(\"‚ùå Failed to extract structured data. See below.\")\n",
        "                st.code(json.dumps(output, indent=2))\n",
        "            else:\n",
        "                st.success(\"‚úÖ Extraction Complete\")\n",
        "                st.json(output)\n",
        "                st.download_button(\"Download JSON\", data=json.dumps(output, indent=2), file_name=\"output.json\")\n"
      ],
      "metadata": {
        "id": "H3-vVNmiV-7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Full Document Intelligence Pipeline with Enhanced Retry, Logging, and Robustness\n",
        "\n",
        "from typing import List, Dict, Optional\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "import pytesseract\n",
        "from pdf2image import convert_from_path\n",
        "import fitz  # PyMuPDF\n",
        "from PIL import Image\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "from pydantic import BaseModel, ValidationError, Field\n",
        "from datetime import datetime\n",
        "\n",
        "# ----------------------------\n",
        "# PromptBuilderAgent (LLM-generated prompt)\n",
        "# ----------------------------\n",
        "class PromptBuilderAgent:\n",
        "    def __init__(self, model_name: str = \"gpt-4\", temperature: float = 0.3):\n",
        "        self.llm = OpenAI(model=model_name, temperature=temperature)\n",
        "        self.prompt_template = PromptTemplate.from_template(\"\"\"\n",
        "You are a prompt generation agent for legal document analysis.\n",
        "\n",
        "Given the metadata:\n",
        "- Module: {module_name}\n",
        "- Document Type: {document_type}\n",
        "- Facility Type: {facility_type}\n",
        "- Market/Submarket: {market} / {submarket}\n",
        "- Local Market: {local_market}\n",
        "- Owner: {owner}\n",
        "- OCR Mode: {has_ocr}\n",
        "\n",
        "And Fields:\n",
        "{field_string}\n",
        "\n",
        "Generate a production-grade prompt that includes:\n",
        "1. Role declaration\n",
        "2. Step-by-step objective section (per field)\n",
        "3. Placeholder for document content\n",
        "4. At least one realistic few-shot example\n",
        "5. Output JSON schema with value, reasoning, confidence\n",
        "6. Guardrails to prevent hallucination or guessing\n",
        "7. OCR handling instructions if text is noisy\n",
        "\"\"\")\n",
        "        self.chain = LLMChain(prompt=self.prompt_template, llm=self.llm)\n",
        "\n",
        "    def build_prompt(self, filters: Dict[str, str], fields: List[Dict[str, str]], has_ocr: bool = True) -> str:\n",
        "        field_str = \"\\n\".join([f\"- {f['name']}: {f.get('description', '')}\" for f in fields])\n",
        "        return self.chain.run(\n",
        "            module_name=filters.get(\"Module Name\", \"\"),\n",
        "            document_type=filters.get(\"Document Type\", \"\"),\n",
        "            facility_type=filters.get(\"Facility Type\", \"\"),\n",
        "            market=filters.get(\"Market\", \"\"),\n",
        "            submarket=filters.get(\"SubMarket\", \"\"),\n",
        "            local_market=filters.get(\"Local Market\", \"\"),\n",
        "            owner=filters.get(\"Owner\", \"\"),\n",
        "            has_ocr=\"Yes\" if has_ocr else \"No\",\n",
        "            field_string=field_str\n",
        "        )\n",
        "\n",
        "# ----------------------------\n",
        "# PromptValidator\n",
        "# ----------------------------\n",
        "def validate_prompt_structure(prompt: str) -> bool:\n",
        "    required_sections = [\"SYSTEM\", \"OBJECTIVE\", \"DOCUMENT CONTEXT\", \"FEW-SHOT\", \"OUTPUT FORMAT\"]\n",
        "    return all(section in prompt for section in required_sections)\n",
        "\n",
        "# ----------------------------\n",
        "# Pydantic Schema for LLM Output\n",
        "# ----------------------------\n",
        "class FieldExtraction(BaseModel):\n",
        "    value: Optional[str]\n",
        "    reasoning: str\n",
        "    confidence: float = Field(..., ge=0, le=1)\n",
        "\n",
        "class ExtractionSchema(BaseModel):\n",
        "    __root__: Dict[str, FieldExtraction]\n",
        "\n",
        "# ----------------------------\n",
        "# DocumentProcessor with Enhanced Retry + Logging\n",
        "# ----------------------------\n",
        "class DocumentProcessor:\n",
        "    def __init__(self, prompt_agent: PromptBuilderAgent, extractor_model: str = \"gpt-4\"):\n",
        "        self.prompt_agent = prompt_agent\n",
        "        self.extractor_llm = OpenAI(model=extractor_model, temperature=0.2)\n",
        "\n",
        "    def extract_text_from_pdf(self, pdf_path: str) -> Dict[str, str]:\n",
        "        try:\n",
        "            doc = fitz.open(pdf_path)\n",
        "            text = \"\\n\\n\".join([page.get_text() for page in doc])\n",
        "            return {\"text\": text, \"is_ocr\": False}\n",
        "        except Exception:\n",
        "            images = convert_from_path(pdf_path)\n",
        "            full_text = []\n",
        "            for img in images:\n",
        "                ocr_result = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT)\n",
        "                for i in range(len(ocr_result['text'])):\n",
        "                    if int(ocr_result['conf'][i]) > 60:\n",
        "                        full_text.append(ocr_result['text'][i])\n",
        "            return {\"text\": \" \".join(full_text), \"is_ocr\": True}\n",
        "\n",
        "    def run_pipeline(self, pdf_path: str, filters: Dict[str, str], fields: List[Dict[str, str]]) -> Dict:\n",
        "        extracted = self.extract_text_from_pdf(pdf_path)\n",
        "        document_text = extracted[\"text\"]\n",
        "        is_ocr = extracted[\"is_ocr\"]\n",
        "\n",
        "        max_attempts = 3\n",
        "        retry_logs = []\n",
        "\n",
        "        for attempt in range(max_attempts):\n",
        "            try:\n",
        "                prompt = self.prompt_agent.build_prompt(filters, fields, has_ocr=is_ocr)\n",
        "                if not validate_prompt_structure(prompt):\n",
        "                    raise ValueError(\"Invalid prompt structure\")\n",
        "\n",
        "                final_input = f\"\"\"{prompt}\\n\\n### DOCUMENT CONTENT\\n{document_text}\\n\"\"\"\n",
        "                raw_output = self.extractor_llm.invoke(final_input)\n",
        "                self.log_attempt(prompt, raw_output, attempt)\n",
        "\n",
        "                parsed_output = json.loads(raw_output)\n",
        "                validated = ExtractionSchema.parse_obj(parsed_output)\n",
        "                return validated.dict()\n",
        "\n",
        "            except Exception as e:\n",
        "                retry_logs.append({\"attempt\": attempt + 1, \"error\": str(e)})\n",
        "                continue\n",
        "\n",
        "        return {\n",
        "            \"error\": \"Failed to validate LLM output after multiple retries\",\n",
        "            \"retries\": retry_logs,\n",
        "            \"raw_output\": raw_output if 'raw_output' in locals() else None\n",
        "        }\n",
        "\n",
        "    def log_attempt(self, prompt: str, response: str, attempt: int):\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        os.makedirs(\"audit_logs\", exist_ok=True)\n",
        "        with open(f\"audit_logs/prompt_attempt{attempt+1}_{timestamp}.txt\", \"w\") as f:\n",
        "            f.write(prompt)\n",
        "        with open(f\"audit_logs/response_attempt{attempt+1}_{timestamp}.json\", \"w\") as f:\n",
        "            f.write(response)\n",
        "\n",
        "\n",
        "# ‚úÖ Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    filters_example = {\n",
        "        \"Module Name\": \"Real Estate\",\n",
        "        \"Territory\": \"South\",\n",
        "        \"Owner\": \"Crown\",\n",
        "        \"Market\": \"South East\",\n",
        "        \"Contract Type\": \"Lease\",\n",
        "        \"SubMarket\": \"Florida\",\n",
        "        \"Document Type\": \"Lease\",\n",
        "        \"Local Market\": \"Tampa\",\n",
        "        \"Facility Type\": \"Easement\"\n",
        "    }\n",
        "\n",
        "    fields_example = [\n",
        "        {\"name\": \"Commencement Date\", \"description\": \"The start date of the lease term.\"},\n",
        "        {\"name\": \"Site ID\", \"description\": \"Internal or regulatory identifier for the facility.\"},\n",
        "        {\"name\": \"Owner Name\", \"description\": \"Legal name of the property owner or landlord.\"}\n",
        "    ]\n",
        "\n",
        "    agent = PromptBuilderAgent()\n",
        "    processor = DocumentProcessor(prompt_agent=agent)\n",
        "    output = processor.run_pipeline(\"example_lease.pdf\", filters_example, fields_example)\n",
        "    print(json.dumps(output, indent=2))"
      ],
      "metadata": {
        "id": "w5vRQnCAWH2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Few-Shot Bootstrap Utilities and Streamlit Toggle Integration\n",
        "\n",
        "import os\n",
        "import json\n",
        "import streamlit as st\n",
        "from typing import List, Dict\n",
        "from langchain.schema import Document\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# ----------------------------\n",
        "# Sample Few-Shot Bootstrap Examples\n",
        "# ----------------------------\n",
        "def bootstrap_fewshot_examples(output_dir=\"fewshot_examples\"):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    examples = {\n",
        "        \"lease_commencement_example.json\": {\n",
        "            \"Commencement Date\": {\n",
        "                \"value\": \"July 1, 2020\",\n",
        "                \"reasoning\": \"Found under section 'Lease Commencement Date'; clearly formatted.\",\n",
        "                \"confidence\": 0.96\n",
        "            }\n",
        "        },\n",
        "        \"site_id_extraction_success.json\": {\n",
        "            \"Site ID\": {\n",
        "                \"value\": \"FL-TMP-8234\",\n",
        "                \"reasoning\": \"Located next to label 'Site Identifier' in tabular header.\",\n",
        "                \"confidence\": 0.92\n",
        "            }\n",
        "        },\n",
        "        \"owner_name_missing_example.json\": {\n",
        "            \"Owner Name\": {\n",
        "                \"value\": None,\n",
        "                \"reasoning\": \"No match for 'Owner'; other legal parties listed but unrelated.\",\n",
        "                \"confidence\": 0.43\n",
        "            }\n",
        "        },\n",
        "        \"monthly_fee_success.json\": {\n",
        "            \"Monthly Fee\": {\n",
        "                \"value\": \"$1,500\",\n",
        "                \"reasoning\": \"Matched to line item labeled 'Monthly Payment Obligation'.\",\n",
        "                \"confidence\": 0.97\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    for fname, content in examples.items():\n",
        "        with open(os.path.join(output_dir, fname), \"w\") as f:\n",
        "            json.dump(content, f, indent=2)\n",
        "\n",
        "# ----------------------------\n",
        "# Auto-Update FAISS Index with New Examples\n",
        "# ----------------------------\n",
        "def refresh_faiss_index(example_dir=\"fewshot_examples\"):\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "    documents = []\n",
        "    for fname in os.listdir(example_dir):\n",
        "        if fname.endswith(\".json\"):\n",
        "            with open(os.path.join(example_dir, fname), \"r\") as f:\n",
        "                data = f.read()\n",
        "                documents.append(Document(page_content=data, metadata={\"filename\": fname}))\n",
        "    return FAISS.from_documents(documents, embeddings)\n",
        "\n",
        "# ----------------------------\n",
        "# Streamlit Toggle to Preview Injected Few-Shots\n",
        "# ----------------------------\n",
        "def preview_few_shot_examples(query: str, k: int = 2):\n",
        "    index = refresh_faiss_index()\n",
        "    results = index.similarity_search(query, k=k)\n",
        "    st.subheader(\"üîç Injected Few-Shot Examples\")\n",
        "    for i, r in enumerate(results):\n",
        "        st.markdown(f\"**Example {i+1} ‚Äî {r.metadata['filename']}**\")\n",
        "        st.code(r.page_content, language=\"json\")\n",
        "\n",
        "# ‚úÖ Example Usage: Bootstrapping and Streamlit Preview\n",
        "if __name__ == \"__main__\":\n",
        "    bootstrap_fewshot_examples()\n",
        "    query_string = \"Prompt for: Lease fields: Commencement Date, Site ID, Owner Name\"\n",
        "    st.title(\"üîß Few-Shot Preview Utility\")\n",
        "    preview_few_shot_examples(query_string)\n"
      ],
      "metadata": {
        "id": "BQtW9KPcWMKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "streamlit run dynamic_prompt_generator.py\n",
        "# Streamlit Interface for Few-Shot Editing and Retrieval Logging\n",
        "\n",
        "import os\n",
        "import json\n",
        "import streamlit as st\n",
        "from datetime import datetime\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.schema import Document\n",
        "\n",
        "EXAMPLES_DIR = \"fewshot_examples\"\n",
        "LOG_FILE = \"retrieval_log.json\"\n",
        "\n",
        "# ----------------------------\n",
        "# UI Toggle: Preview Few-Shot Examples\n",
        "# ----------------------------\n",
        "def preview_few_shot_examples(query: str, k: int = 2):\n",
        "    st.subheader(\"üìå Injected Few-Shot Examples\")\n",
        "    index = refresh_faiss_index()\n",
        "    results = index.similarity_search(query, k=k)\n",
        "    retrieval_log(query, results)\n",
        "    for i, doc in enumerate(results):\n",
        "        st.markdown(f\"**Example {i+1} ‚Äî {doc.metadata['filename']}**\")\n",
        "        st.code(doc.page_content, language=\"json\")\n",
        "\n",
        "# ----------------------------\n",
        "# Few-Shot Editor Mode\n",
        "# ----------------------------\n",
        "def few_shot_editor():\n",
        "    st.header(\"üõ†Ô∏è Few-Shot Editor\")\n",
        "    files = [f for f in os.listdir(EXAMPLES_DIR) if f.endswith(\".json\")]\n",
        "    selected_file = st.selectbox(\"Choose a sample to edit\", files)\n",
        "    filepath = os.path.join(EXAMPLES_DIR, selected_file)\n",
        "\n",
        "    if selected_file:\n",
        "        with open(filepath, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "        edited = st_ace_editor(json.dumps(data, indent=2), language=\"json\")\n",
        "\n",
        "        if st.button(\"üíæ Save Changes\"):\n",
        "            try:\n",
        "                parsed = json.loads(edited)\n",
        "                with open(filepath, \"w\") as f:\n",
        "                    json.dump(parsed, f, indent=2)\n",
        "                st.success(f\"Saved {selected_file}\")\n",
        "            except json.JSONDecodeError:\n",
        "                st.error(\"Invalid JSON ‚Äî please fix and try again\")\n",
        "\n",
        "# ----------------------------\n",
        "# Editor Widget (ACE)\n",
        "# ----------------------------\n",
        "from streamlit_ace import st_ace as st_ace_editor\n",
        "\n",
        "# ----------------------------\n",
        "# FAISS Index Refresh\n",
        "# ----------------------------\n",
        "def refresh_faiss_index():\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "    docs = []\n",
        "    for fname in os.listdir(EXAMPLES_DIR):\n",
        "        if fname.endswith(\".json\"):\n",
        "            with open(os.path.join(EXAMPLES_DIR, fname), \"r\") as f:\n",
        "                content = f.read()\n",
        "                docs.append(Document(page_content=content, metadata={\"filename\": fname}))\n",
        "    return FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "# ----------------------------\n",
        "# Retrieval Logging\n",
        "# ----------------------------\n",
        "def retrieval_log(query: str, results):\n",
        "    log_entry = {\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"query\": query,\n",
        "        \"retrieved\": [r.metadata[\"filename\"] for r in results]\n",
        "    }\n",
        "    os.makedirs(\"logs\", exist_ok=True)\n",
        "    path = os.path.join(\"logs\", LOG_FILE)\n",
        "    existing = []\n",
        "    if os.path.exists(path):\n",
        "        with open(path, \"r\") as f:\n",
        "            try:\n",
        "                existing = json.load(f)\n",
        "            except:\n",
        "                existing = []\n",
        "    existing.append(log_entry)\n",
        "    with open(path, \"w\") as f:\n",
        "        json.dump(existing, f, indent=2)\n",
        "\n",
        "# ----------------------------\n",
        "# Streamlit Launcher\n",
        "# ----------------------------\n",
        "def main():\n",
        "    st.set_page_config(layout=\"wide\")\n",
        "    st.title(\"üîé Few-Shot Intelligence Tools\")\n",
        "\n",
        "    tab1, tab2 = st.tabs([\"Preview\", \"Editor\"])\n",
        "\n",
        "    with tab1:\n",
        "        q = st.text_input(\"Query (ex: Lease fields: Site ID, Commencement Date)\")\n",
        "        k = st.slider(\"# of Examples\", 1, 5, 2)\n",
        "        if st.button(\"üîç Retrieve\") and q:\n",
        "            preview_few_shot_examples(f\"Prompt for: {q}\", k=k)\n",
        "\n",
        "    with tab2:\n",
        "        few_shot_editor()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    os.makedirs(EXAMPLES_DIR, exist_ok=True)\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "hbEojwkfWSSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieval_pipeline.py\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.schema import Document\n",
        "from typing import List, Dict, Any\n",
        "import faiss\n",
        "import os\n",
        "import json\n",
        "\n",
        "class ChunkIndexer:\n",
        "    def __init__(self, index_path=\"faiss_index\"):\n",
        "        self.index_path = index_path\n",
        "        self.embeddings = OpenAIEmbeddings()\n",
        "\n",
        "    def chunk_document(self, doc_id: str, pages: List[str]) -> List[Document]:\n",
        "        chunks = []\n",
        "        for i, page in enumerate(pages):\n",
        "            content = page.strip()\n",
        "            if content:\n",
        "                chunks.append(Document(\n",
        "                    page_content=content,\n",
        "                    metadata={\n",
        "                        \"doc_id\": doc_id,\n",
        "                        \"chunk_id\": f\"{doc_id}_{i+1}\",\n",
        "                        \"page_num\": i + 1,\n",
        "                        \"source\": \"ocr\" if \"\\n\" in content else \"digital\"\n",
        "                    }\n",
        "                ))\n",
        "        return chunks\n",
        "\n",
        "    def build_or_update_index(self, doc_id: str, pages: List[str]):\n",
        "        chunks = self.chunk_document(doc_id, pages)\n",
        "        if not os.path.exists(self.index_path):\n",
        "            vectorstore = FAISS.from_documents(chunks, self.embeddings)\n",
        "            vectorstore.save_local(self.index_path)\n",
        "        else:\n",
        "            vectorstore = FAISS.load_local(self.index_path, self.embeddings)\n",
        "            vectorstore.add_documents(chunks)\n",
        "            vectorstore.save_local(self.index_path)\n",
        "\n",
        "\n",
        "class ChunkRetriever:\n",
        "    def __init__(self, index_path=\"faiss_index\"):\n",
        "        self.embeddings = OpenAIEmbeddings()\n",
        "        self.index = FAISS.load_local(index_path, self.embeddings)\n",
        "\n",
        "    def semantic_retrieve(self, query: str, k: int = 5) -> List[Document]:\n",
        "        return self.index.similarity_search(query, k=k)\n",
        "\n",
        "    def keyword_retrieve(self, keyword: str, k: int = 5) -> List[Document]:\n",
        "        all_docs = self.index.docstore._dict.values()\n",
        "        filtered = [doc for doc in all_docs if keyword.lower() in doc.page_content.lower()]\n",
        "        return filtered[:k]\n",
        "\n",
        "    def hybrid_retrieve(self, field_name: str, description: str, k: int = 5) -> List[Document]:\n",
        "        results = self.semantic_retrieve(description, k=k)\n",
        "        fallback = self.keyword_retrieve(field_name, k=k)\n",
        "        ids = {r.metadata['chunk_id'] for r in results}\n",
        "        results.extend([doc for doc in fallback if doc.metadata['chunk_id'] not in ids])\n",
        "        return results[:k]\n",
        "\n",
        "\n",
        "class FieldTaskPlanner:\n",
        "    def plan(self, fields: List[Dict[str, str]]) -> List[Dict[str, Any]]:\n",
        "        tasks = []\n",
        "        for f in fields:\n",
        "            tasks.append({\n",
        "                \"name\": f[\"name\"],\n",
        "                \"description\": f.get(\"description\", f[\"name\"]),\n",
        "                \"strategy\": \"hybrid\",\n",
        "                \"top_k\": 5\n",
        "            })\n",
        "        return tasks\n",
        "\n",
        "\n",
        "class RetrievalExecutor:\n",
        "    def __init__(self, retriever: ChunkRetriever, llm):\n",
        "        self.retriever = retriever\n",
        "        self.llm = llm\n",
        "\n",
        "    def run_task(self, task: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        chunks = self.retriever.hybrid_retrieve(task[\"name\"], task[\"description\"], task.get(\"top_k\", 5))\n",
        "        context = \"\\n\\n\".join([doc.page_content for doc in chunks])\n",
        "        prompt = f\"\"\"\n",
        "You are an expert lease document reviewer. Extract the field: '{task['name']}' from the content below.\n",
        "If present, return the exact value with confidence.\n",
        "If not found, return null and a reason.\n",
        "\n",
        "---\n",
        "{context}\n",
        "---\n",
        "Respond in JSON: {{ \"value\": ..., \"reason\": ..., \"confidence\": 0.0‚Äì1.0 }}\n",
        "\"\"\"\n",
        "        response = self.llm.invoke(prompt)\n",
        "        try:\n",
        "            return json.loads(response)\n",
        "        except:\n",
        "            return {\"value\": None, \"reason\": \"Invalid LLM output\", \"confidence\": 0.0}\n",
        "\n",
        "    def run_all(self, field_tasks: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "        results = {}\n",
        "        for task in field_tasks:\n",
        "            results[task[\"name\"]] = self.run_task(task)\n",
        "        return results\n"
      ],
      "metadata": {
        "id": "2ul0Dt0KWUwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# structured_chunker.py\n",
        "import fitz  # PyMuPDF\n",
        "import pdfplumber\n",
        "import pytesseract\n",
        "from pdf2image import convert_from_path\n",
        "from typing import List, Dict, Union, Tuple\n",
        "from dataclasses import dataclass, asdict\n",
        "import os\n",
        "import uuid\n",
        "\n",
        "@dataclass\n",
        "class TypedChunk:\n",
        "    type: str  # \"narrative_text\", \"table\", \"image\"\n",
        "    page: int\n",
        "    bbox: Tuple[float, float, float, float]\n",
        "    content: Union[str, List]\n",
        "    metadata: Dict\n",
        "\n",
        "\n",
        "class StructuredChunker:\n",
        "    def __init__(self, pdf_path: str, dpi: int = 300):\n",
        "        self.pdf_path = pdf_path\n",
        "        self.doc = fitz.open(pdf_path)\n",
        "        self.dpi = dpi\n",
        "        self.chunk_id_prefix = os.path.basename(pdf_path).replace(\".pdf\", \"\")\n",
        "\n",
        "    def extract_text_blocks(self) -> List[TypedChunk]:\n",
        "        chunks = []\n",
        "        for i, page in enumerate(self.doc):\n",
        "            blocks = page.get_text(\"blocks\")  # (x0, y0, x1, y1, text, block_no)\n",
        "            for b in blocks:\n",
        "                x0, y0, x1, y1, text, *_ = b\n",
        "                if text.strip():\n",
        "                    chunk = TypedChunk(\n",
        "                        type=\"narrative_text\",\n",
        "                        page=i + 1,\n",
        "                        bbox=(x0, y0, x1, y1),\n",
        "                        content=text.strip(),\n",
        "                        metadata={\"chunk_id\": f\"{self.chunk_id_prefix}_text_{uuid.uuid4().hex[:6]}\"}\n",
        "                    )\n",
        "                    chunks.append(chunk)\n",
        "        return chunks\n",
        "\n",
        "    def extract_tables(self) -> List[TypedChunk]:\n",
        "        tables = []\n",
        "        with pdfplumber.open(self.pdf_path) as pdf:\n",
        "            for i, page in enumerate(pdf.pages):\n",
        "                try:\n",
        "                    for table in page.extract_tables():\n",
        "                        if table and len(table) > 1:\n",
        "                            chunk = TypedChunk(\n",
        "                                type=\"table\",\n",
        "                                page=i + 1,\n",
        "                                bbox=page.bbox,\n",
        "                                content=table,\n",
        "                                metadata={\"chunk_id\": f\"{self.chunk_id_prefix}_table_{uuid.uuid4().hex[:6]}\"}\n",
        "                            )\n",
        "                            tables.append(chunk)\n",
        "                except Exception:\n",
        "                    continue\n",
        "        return tables\n",
        "\n",
        "    def extract_images_with_ocr(self) -> List[TypedChunk]:\n",
        "        chunks = []\n",
        "        images = convert_from_path(self.pdf_path, dpi=self.dpi)\n",
        "        for i, img in enumerate(images):\n",
        "            ocr_text = pytesseract.image_to_string(img)\n",
        "            if ocr_text.strip():\n",
        "                chunk = TypedChunk(\n",
        "                    type=\"image\",\n",
        "                    page=i + 1,\n",
        "                    bbox=(0, 0, img.size[0], img.size[1]),\n",
        "                    content=ocr_text.strip(),\n",
        "                    metadata={\"chunk_id\": f\"{self.chunk_id_prefix}_img_{uuid.uuid4().hex[:6]}\"}\n",
        "                )\n",
        "                chunks.append(chunk)\n",
        "        return chunks\n",
        "\n",
        "    def get_all_chunks(self) -> List[Dict]:\n",
        "        text_chunks = self.extract_text_blocks()\n",
        "        table_chunks = self.extract_tables()\n",
        "        image_chunks = self.extract_images_with_ocr()\n",
        "        all_chunks = text_chunks + table_chunks + image_chunks\n",
        "\n",
        "        # Sort by page + vertical position (y0)\n",
        "        all_chunks.sort(key=lambda c: (c.page, c.bbox[1]))\n",
        "        return [asdict(chunk) for chunk in all_chunks]\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import json\n",
        "    import sys\n",
        "\n",
        "    pdf_file = sys.argv[1] if len(sys.argv) > 1 else \"example.pdf\"\n",
        "    chunker = StructuredChunker(pdf_file)\n",
        "    output = chunker.get_all_chunks()\n",
        "    json_path = pdf_file.replace(\".pdf\", \"_structured.json\")\n",
        "\n",
        "    with open(json_path, \"w\") as f:\n",
        "        json.dump(output, f, indent=2)\n",
        "\n",
        "    print(f\"‚úÖ Structured chunks saved to {json_path}\")\n"
      ],
      "metadata": {
        "id": "bs38PYcxWbJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieval_pipeline.py with structured chunk ingestion + type metadata\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.schema import Document\n",
        "from typing import List, Dict, Any\n",
        "import faiss\n",
        "import os\n",
        "import json\n",
        "\n",
        "class ChunkIndexer:\n",
        "    def __init__(self, index_path=\"faiss_index\"):\n",
        "        self.index_path = index_path\n",
        "        self.embeddings = OpenAIEmbeddings()\n",
        "\n",
        "    def load_structured_chunks(self, chunk_file: str) -> List[Document]:\n",
        "        with open(chunk_file, \"r\") as f:\n",
        "            chunks = json.load(f)\n",
        "\n",
        "        docs = []\n",
        "        for c in chunks:\n",
        "            text = \"\"\n",
        "            if c[\"type\"] == \"table\":\n",
        "                text = \"\\n\".join([\" | \".join(row) for row in c[\"content\"]])\n",
        "            elif c[\"type\"] == \"image\":\n",
        "                text = c[\"content\"]\n",
        "            elif c[\"type\"] == \"narrative_text\":\n",
        "                text = c[\"content\"]\n",
        "\n",
        "            doc = Document(\n",
        "                page_content=text,\n",
        "                metadata={\n",
        "                    \"chunk_id\": c[\"metadata\"].get(\"chunk_id\"),\n",
        "                    \"type\": c[\"type\"],\n",
        "                    \"page\": c[\"page\"],\n",
        "                    **c.get(\"metadata\", {})\n",
        "                }\n",
        "            )\n",
        "            docs.append(doc)\n",
        "        return docs\n",
        "\n",
        "    def build_index_from_chunks(self, chunk_json_file: str):\n",
        "        chunks = self.load_structured_chunks(chunk_json_file)\n",
        "        if not os.path.exists(self.index_path):\n",
        "            db = FAISS.from_documents(chunks, self.embeddings)\n",
        "            db.save_local(self.index_path)\n",
        "        else:\n",
        "            db = FAISS.load_local(self.index_path, self.embeddings)\n",
        "            db.add_documents(chunks)\n",
        "            db.save_local(self.index_path)\n",
        "\n",
        "\n",
        "class ChunkRetriever:\n",
        "    def __init__(self, index_path=\"faiss_index\"):\n",
        "        self.embeddings = OpenAIEmbeddings()\n",
        "        self.index = FAISS.load_local(index_path, self.embeddings)\n",
        "\n",
        "    def semantic_retrieve(self, query: str, k: int = 5) -> List[Document]:\n",
        "        return self.index.similarity_search(query, k=k)\n",
        "\n",
        "    def keyword_retrieve(self, keyword: str, k: int = 5) -> List[Document]:\n",
        "        all_docs = self.index.docstore._dict.values()\n",
        "        return [doc for doc in all_docs if keyword.lower() in doc.page_content.lower()][:k]\n",
        "\n",
        "    def hybrid_retrieve(self, field_name: str, description: str, k: int = 5) -> List[Document]:\n",
        "        semantic = self.semantic_retrieve(description, k=k)\n",
        "        keyword = self.keyword_retrieve(field_name, k=k)\n",
        "        existing_ids = {d.metadata['chunk_id'] for d in semantic}\n",
        "        combined = semantic + [d for d in keyword if d.metadata['chunk_id'] not in existing_ids]\n",
        "        return combined[:k]\n",
        "\n",
        "\n",
        "class FieldTaskPlanner:\n",
        "    def plan(self, fields: List[Dict[str, str]]) -> List[Dict[str, Any]]:\n",
        "        return [{\n",
        "            \"name\": f[\"name\"],\n",
        "            \"description\": f.get(\"description\", f[\"name\"]),\n",
        "            \"strategy\": \"hybrid\",\n",
        "            \"top_k\": 5\n",
        "        } for f in fields]\n",
        "\n",
        "\n",
        "class RetrievalExecutor:\n",
        "    def __init__(self, retriever: ChunkRetriever, llm):\n",
        "        self.retriever = retriever\n",
        "        self.llm = llm\n",
        "\n",
        "    def run_task(self, task: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        chunks = self.retriever.hybrid_retrieve(task[\"name\"], task[\"description\"], task.get(\"top_k\", 5))\n",
        "        context = \"\\n\\n\".join([doc.page_content for doc in chunks])\n",
        "        prompt = f\"\"\"\n",
        "You are an expert lease analyst. Extract the field: '{task['name']}' from the context below.\n",
        "Use chunk metadata (type, page, source) to improve reasoning. If not found, say why.\n",
        "\n",
        "---\\n{context}\\n---\n",
        "\n",
        "Respond in JSON:\n",
        "{{\"value\":..., \"reason\":..., \"confidence\": 0.0-1.0}}\n",
        "\"\"\"\n",
        "        try:\n",
        "            response = self.llm.invoke(prompt)\n",
        "            result = json.loads(response)\n",
        "            result[\"_aligned_chunks\"] = [\n",
        "                {\n",
        "                    \"chunk_id\": d.metadata[\"chunk_id\"],\n",
        "                    \"page_num\": d.metadata[\"page\"],\n",
        "                    \"doc_id\": d.metadata.get(\"doc_id\"),\n",
        "                    \"type\": d.metadata[\"type\"],\n",
        "                    \"text\": d.page_content\n",
        "                } for d in chunks\n",
        "            ]\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            return {\"value\": None, \"reason\": str(e), \"confidence\": 0.0}\n"
      ],
      "metadata": {
        "id": "T4yQMwdoWdjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieval_pipeline.py ‚Äî with type filtering + confidence scoring hint\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.schema import Document\n",
        "from typing import List, Dict, Any\n",
        "import faiss\n",
        "import os\n",
        "import json\n",
        "\n",
        "class ChunkIndexer:\n",
        "    def __init__(self, index_path=\"faiss_index\"):\n",
        "        self.index_path = index_path\n",
        "        self.embeddings = OpenAIEmbeddings()\n",
        "\n",
        "    def load_structured_chunks(self, chunk_file: str) -> List[Document]:\n",
        "        with open(chunk_file, \"r\") as f:\n",
        "            chunks = json.load(f)\n",
        "\n",
        "        docs = []\n",
        "        for c in chunks:\n",
        "            text = \"\"\n",
        "            if c[\"type\"] == \"table\":\n",
        "                text = \"\\n\".join([\" | \".join(row) for row in c[\"content\"]])\n",
        "            elif c[\"type\"] == \"image\":\n",
        "                text = c[\"content\"]\n",
        "            elif c[\"type\"] == \"narrative_text\":\n",
        "                text = c[\"content\"]\n",
        "\n",
        "            doc = Document(\n",
        "                page_content=text,\n",
        "                metadata={\n",
        "                    \"chunk_id\": c[\"metadata\"].get(\"chunk_id\"),\n",
        "                    \"type\": c[\"type\"],\n",
        "                    \"page\": c[\"page\"],\n",
        "                    **c.get(\"metadata\", {})\n",
        "                }\n",
        "            )\n",
        "            docs.append(doc)\n",
        "        return docs\n",
        "\n",
        "    def build_index_from_chunks(self, chunk_json_file: str):\n",
        "        chunks = self.load_structured_chunks(chunk_json_file)\n",
        "        if not os.path.exists(self.index_path):\n",
        "            db = FAISS.from_documents(chunks, self.embeddings)\n",
        "            db.save_local(self.index_path)\n",
        "        else:\n",
        "            db = FAISS.load_local(self.index_path, self.embeddings)\n",
        "            db.add_documents(chunks)\n",
        "            db.save_local(self.index_path)\n",
        "\n",
        "\n",
        "class ChunkRetriever:\n",
        "    def __init__(self, index_path=\"faiss_index\"):\n",
        "        self.embeddings = OpenAIEmbeddings()\n",
        "        self.index = FAISS.load_local(index_path, self.embeddings)\n",
        "\n",
        "    def semantic_retrieve(self, query: str, k: int = 5, type_filter: str = None) -> List[Document]:\n",
        "        docs = self.index.similarity_search(query, k=k)\n",
        "        if type_filter:\n",
        "            docs = [d for d in docs if d.metadata.get(\"type\") == type_filter]\n",
        "        return docs\n",
        "\n",
        "    def keyword_retrieve(self, keyword: str, k: int = 5, type_filter: str = None) -> List[Document]:\n",
        "        all_docs = self.index.docstore._dict.values()\n",
        "        docs = [doc for doc in all_docs if keyword.lower() in doc.page_content.lower()]\n",
        "        if type_filter:\n",
        "            docs = [d for d in docs if d.metadata.get(\"type\") == type_filter]\n",
        "        return docs[:k]\n",
        "\n",
        "    def hybrid_retrieve(self, field_name: str, description: str, k: int = 5, type_filter: str = None) -> List[Document]:\n",
        "        semantic = self.semantic_retrieve(description, k=k, type_filter=type_filter)\n",
        "        keyword = self.keyword_retrieve(field_name, k=k, type_filter=type_filter)\n",
        "        existing_ids = {d.metadata['chunk_id'] for d in semantic}\n",
        "        combined = semantic + [d for d in keyword if d.metadata['chunk_id'] not in existing_ids]\n",
        "        return combined[:k]\n",
        "\n",
        "\n",
        "class FieldTaskPlanner:\n",
        "    def plan(self, fields: List[Dict[str, str]]) -> List[Dict[str, Any]]:\n",
        "        return [{\n",
        "            \"name\": f[\"name\"],\n",
        "            \"description\": f.get(\"description\", f[\"name\"]),\n",
        "            \"strategy\": \"hybrid\",\n",
        "            \"top_k\": 5,\n",
        "            \"type_filter\": f.get(\"preferred_type\")  # optional: limit to \"table\" | \"narrative_text\" | etc\n",
        "        } for f in fields]\n",
        "\n",
        "\n",
        "class RetrievalExecutor:\n",
        "    def __init__(self, retriever: ChunkRetriever, llm):\n",
        "        self.retriever = retriever\n",
        "        self.llm = llm\n",
        "\n",
        "    def run_task(self, task: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        chunks = self.retriever.hybrid_retrieve(\n",
        "            field_name=task[\"name\"],\n",
        "            description=task[\"description\"],\n",
        "            k=task.get(\"top_k\", 5),\n",
        "            type_filter=task.get(\"type_filter\")\n",
        "        )\n",
        "\n",
        "        context = \"\\n\\n\".join([doc.page_content for doc in chunks])\n",
        "        prompt = f\"\"\"\n",
        "You are a lease document reviewer. Extract the field: '{task['name']}' from the provided document content.\n",
        "\n",
        "You are analyzing chunks of type: {[d.metadata['type'] for d in chunks]}.\n",
        "\n",
        "If you find a valid value, return it with a high confidence (close to 1.0).\n",
        "If it's ambiguous or fuzzy, lower the confidence.\n",
        "If missing, return value=null, reason=\"...\", confidence=0.0\n",
        "\n",
        "---\\n{context}\\n---\n",
        "Respond strictly in JSON:\n",
        "{{\"value\": ..., \"reason\": ..., \"confidence\": 0.0-1.0}}\n",
        "\"\"\"\n",
        "        try:\n",
        "            response = self.llm.invoke(prompt)\n",
        "            result = json.loads(response)\n",
        "            result[\"_aligned_chunks\"] = [\n",
        "                {\n",
        "                    \"chunk_id\": d.metadata[\"chunk_id\"],\n",
        "                    \"page_num\": d.metadata[\"page\"],\n",
        "                    \"doc_id\": d.metadata.get(\"doc_id\"),\n",
        "                    \"type\": d.metadata[\"type\"],\n",
        "                    \"text\": d.page_content\n",
        "                } for d in chunks\n",
        "            ]\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            return {\"value\": None, \"reason\": str(e), \"confidence\": 0.0}\n"
      ],
      "metadata": {
        "id": "v44ZTaCZWixC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieval_pipeline.py ‚Äî with field normalization + semantic prompt injection\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.schema import Document\n",
        "from typing import List, Dict, Any\n",
        "import faiss\n",
        "import os\n",
        "import json\n",
        "\n",
        "class ChunkIndexer:\n",
        "    def __init__(self, index_path=\"faiss_index\"):\n",
        "        self.index_path = index_path\n",
        "        self.embeddings = OpenAIEmbeddings()\n",
        "\n",
        "    def load_structured_chunks(self, chunk_file: str) -> List[Document]:\n",
        "        with open(chunk_file, \"r\") as f:\n",
        "            chunks = json.load(f)\n",
        "\n",
        "        docs = []\n",
        "        for c in chunks:\n",
        "            text = \"\"\n",
        "            if c[\"type\"] == \"table\":\n",
        "                text = \"\\n\".join([\" | \".join(row) for row in c[\"content\"]])\n",
        "            elif c[\"type\"] == \"image\":\n",
        "                text = c[\"content\"]\n",
        "            elif c[\"type\"] == \"narrative_text\":\n",
        "                text = c[\"content\"]\n",
        "\n",
        "            doc = Document(\n",
        "                page_content=text,\n",
        "                metadata={\n",
        "                    \"chunk_id\": c[\"metadata\"].get(\"chunk_id\"),\n",
        "                    \"type\": c[\"type\"],\n",
        "                    \"page\": c[\"page\"],\n",
        "                    **c.get(\"metadata\", {})\n",
        "                }\n",
        "            )\n",
        "            docs.append(doc)\n",
        "        return docs\n",
        "\n",
        "    def build_index_from_chunks(self, chunk_json_file: str):\n",
        "        chunks = self.load_structured_chunks(chunk_json_file)\n",
        "        if not os.path.exists(self.index_path):\n",
        "            db = FAISS.from_documents(chunks, self.embeddings)\n",
        "            db.save_local(self.index_path)\n",
        "        else:\n",
        "            db = FAISS.load_local(self.index_path, self.embeddings)\n",
        "            db.add_documents(chunks)\n",
        "            db.save_local(self.index_path)\n",
        "\n",
        "\n",
        "class ChunkRetriever:\n",
        "    def __init__(self, index_path=\"faiss_index\"):\n",
        "        self.embeddings = OpenAIEmbeddings()\n",
        "        self.index = FAISS.load_local(index_path, self.embeddings)\n",
        "\n",
        "    def semantic_retrieve(self, query: str, k: int = 5, type_filter: str = None) -> List[Document]:\n",
        "        docs = self.index.similarity_search(query, k=k)\n",
        "        if type_filter:\n",
        "            docs = [d for d in docs if d.metadata.get(\"type\") == type_filter]\n",
        "        return docs\n",
        "\n",
        "    def keyword_retrieve(self, keyword: str, k: int = 5, type_filter: str = None) -> List[Document]:\n",
        "        all_docs = self.index.docstore._dict.values()\n",
        "        docs = [doc for doc in all_docs if keyword.lower() in doc.page_content.lower()]\n",
        "        if type_filter:\n",
        "            docs = [d for d in docs if d.metadata.get(\"type\") == type_filter]\n",
        "        return docs[:k]\n",
        "\n",
        "    def hybrid_retrieve(self, field_name: str, description: str, k: int = 5, type_filter: str = None, hints: List[str] = []) -> List[Document]:\n",
        "        compound_query = description + \" \" + \" \".join(hints)\n",
        "        semantic = self.semantic_retrieve(compound_query, k=k, type_filter=type_filter)\n",
        "        keyword = self.keyword_retrieve(field_name, k=k, type_filter=type_filter)\n",
        "        existing_ids = {d.metadata['chunk_id'] for d in semantic}\n",
        "        combined = semantic + [d for d in keyword if d.metadata['chunk_id'] not in existing_ids]\n",
        "        return combined[:k]\n",
        "\n",
        "\n",
        "class FieldTaskPlanner:\n",
        "    def plan(self, fields: List[Dict[str, str]]) -> List[Dict[str, Any]]:\n",
        "        tasks = []\n",
        "        for f in fields:\n",
        "            name = f[\"name\"]\n",
        "            desc = f.get(\"description\", name)\n",
        "            synonyms = f.get(\"synonyms\", [])\n",
        "            hint_keywords = f.get(\"hint_keywords\", [])\n",
        "            hints = synonyms + hint_keywords\n",
        "            tasks.append({\n",
        "                \"name\": name,\n",
        "                \"description\": desc,\n",
        "                \"strategy\": \"hybrid\",\n",
        "                \"top_k\": f.get(\"top_k\", 5),\n",
        "                \"type_filter\": f.get(\"preferred_type\"),\n",
        "                \"importance\": f.get(\"importance\", \"medium\"),\n",
        "                \"temporal_scope\": f.get(\"temporal_scope\"),\n",
        "                \"must_have_units\": f.get(\"must_have_units\", []),\n",
        "                \"hints\": hints\n",
        "            })\n",
        "        return tasks\n",
        "\n",
        "\n",
        "class RetrievalExecutor:\n",
        "    def __init__(self, retriever: ChunkRetriever, llm):\n",
        "        self.retriever = retriever\n",
        "        self.llm = llm\n",
        "\n",
        "    def run_task(self, task: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        chunks = self.retriever.hybrid_retrieve(\n",
        "            field_name=task[\"name\"],\n",
        "            description=task[\"description\"],\n",
        "            k=task.get(\"top_k\", 5),\n",
        "            type_filter=task.get(\"type_filter\"),\n",
        "            hints=task.get(\"hints\", [])\n",
        "        )\n",
        "\n",
        "        chunk_types = set([d.metadata[\"type\"] for d in chunks])\n",
        "        unit_hint = \", must include units: \" + \", \".join(task.get(\"must_have_units\", [])) if task.get(\"must_have_units\") else \"\"\n",
        "        scope_hint = f\" (temporal scope: {task.get('temporal_scope')})\" if task.get(\"temporal_scope\") else \"\"\n",
        "\n",
        "        context = \"\\n\\n\".join([doc.page_content for doc in chunks])\n",
        "        prompt = f\"\"\"\n",
        "You are analyzing a lease document. Your goal is to extract the field: '{task['name']}'.\n",
        "\n",
        "Field Description: {task['description']}{scope_hint}\n",
        "Chunk types in use: {', '.join(chunk_types)}{unit_hint}\n",
        "Hints: {', '.join(task.get('hints', []))}\n",
        "\n",
        "---\\n{context}\\n---\n",
        "Respond strictly in JSON format:\n",
        "{{\"value\": ..., \"reason\": ..., \"confidence\": 0.0-1.0}}\n",
        "\"\"\"\n",
        "        try:\n",
        "            response = self.llm.invoke(prompt)\n",
        "            result = json.loads(response)\n",
        "            result[\"_aligned_chunks\"] = [\n",
        "                {\n",
        "                    \"chunk_id\": d.metadata[\"chunk_id\"],\n",
        "                    \"page_num\": d.metadata[\"page\"],\n",
        "                    \"doc_id\": d.metadata.get(\"doc_id\"),\n",
        "                    \"type\": d.metadata[\"type\"],\n",
        "                    \"text\": d.page_content\n",
        "                } for d in chunks\n",
        "            ]\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            return {\"value\": None, \"reason\": str(e), \"confidence\": 0.0}\n"
      ],
      "metadata": {
        "id": "s3wgQaliWlOf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}